#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æœ€ç»ˆåˆ†ææŠ¥å‘Šï¼šæœå›­æ•°æ®å›ç¯æ£€æµ‹ç»“æœ
"""

import pickle
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

def generate_final_report():
    """ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š"""
    
    print("="*80)
    print("æœå›­å·¡æ£€æ•°æ®å›ç¯æ£€æµ‹æœ€ç»ˆåˆ†ææŠ¥å‘Š")
    print("="*80)
    
    # åŠ è½½ç»“æœ
    results_file = Path("results/simple_test_results.pkl")
    if not results_file.exists():
        print("æœªæ‰¾åˆ°æµ‹è¯•ç»“æœæ–‡ä»¶")
        return
    
    with open(results_file, 'rb') as f:
        results = pickle.load(f)
    
    predictions = np.array(results['predictions'])
    confidences = np.array(results['confidences'])
    file_indices = np.array(results['file_indices'])
    file_names = results['file_names']
    
    print(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯")
    print(f"{'='*50}")
    print(f"æ•°æ®æ¥æº: æœå›­å·¡æ£€rosbag (2025-07-03-16-28-57.bag)")
    print(f"æ€»ç‚¹äº‘æ–‡ä»¶æ•°: 1769ä¸ª")
    print(f"é‡‡æ ·å¤„ç†æ–‡ä»¶æ•°: {len(file_names)}ä¸ª (1:20é‡‡æ ·)")
    print(f"æœ‰æ•ˆé¢„æµ‹æ•°: {len(predictions[predictions >= 0])}ä¸ª")
    print(f"é¢„æµ‹æˆåŠŸç‡: {len(predictions[predictions >= 0])/len(predictions)*100:.1f}%")
    
    valid_predictions = predictions[predictions >= 0]
    valid_confidences = confidences[predictions >= 0]
    valid_indices = file_indices[predictions >= 0]
    
    print(f"\nğŸ¯ é¢„æµ‹ç»“æœåˆ†æ")
    print(f"{'='*50}")
    print(f"é¢„æµ‹ç±»åˆ«èŒƒå›´: {np.min(valid_predictions)} - {np.max(valid_predictions)}")
    print(f"é¢„æµ‹ç±»åˆ«æ•°é‡: {len(np.unique(valid_predictions))}ä¸ª")
    print(f"å¹³å‡ç½®ä¿¡åº¦: {np.mean(valid_confidences):.4f}")
    print(f"ç½®ä¿¡åº¦æ ‡å‡†å·®: {np.std(valid_confidences):.4f}")
    print(f"æœ€é«˜ç½®ä¿¡åº¦: {np.max(valid_confidences):.4f}")
    print(f"æœ€ä½ç½®ä¿¡åº¦: {np.min(valid_confidences):.4f}")
    
    # é¢„æµ‹åˆ†å¸ƒåˆ†æ
    print(f"\nğŸ“ˆ é¢„æµ‹ç±»åˆ«åˆ†å¸ƒ")
    print(f"{'='*50}")
    unique, counts = np.unique(valid_predictions, return_counts=True)
    
    # æŒ‰å‡ºç°æ¬¡æ•°æ’åº
    sorted_indices = np.argsort(counts)[::-1]
    
    print("ç±»åˆ«  | å‡ºç°æ¬¡æ•° | å æ¯”   | å¹³å‡ç½®ä¿¡åº¦")
    print("-" * 40)
    for i in sorted_indices:
        cls = unique[i]
        count = counts[i]
        percentage = count / len(valid_predictions) * 100
        
        # è®¡ç®—è¯¥ç±»åˆ«çš„å¹³å‡ç½®ä¿¡åº¦
        cls_mask = valid_predictions == cls
        avg_conf = np.mean(valid_confidences[cls_mask])
        
        print(f"{cls:4d}  | {count:8d} | {percentage:5.1f}% | {avg_conf:.4f}")
    
    # æ—¶åºåˆ†æ
    print(f"\nâ° æ—¶åºåˆ†å¸ƒåˆ†æ")
    print(f"{'='*50}")
    
    # åˆ†æé¢„æµ‹åœ¨æ—¶é—´è½´ä¸Šçš„åˆ†å¸ƒ
    time_segments = {
        "å‰æ®µ (0-400)": (0, 400),
        "ä¸­æ®µ (400-800)": (400, 800), 
        "ä¸­åæ®µ (800-1200)": (800, 1200),
        "åæ®µ (1200-1600)": (1200, 1600),
        "æœ«æ®µ (1600+)": (1600, 2000)
    }
    
    for segment_name, (start, end) in time_segments.items():
        mask = (valid_indices >= start) & (valid_indices < end)
        if np.sum(mask) > 0:
            segment_predictions = valid_predictions[mask]
            segment_confidences = valid_confidences[mask]
            
            print(f"{segment_name}:")
            print(f"  æ ·æœ¬æ•°: {len(segment_predictions)}")
            print(f"  ä¸»è¦ç±»åˆ«: {np.bincount(segment_predictions).argmax()}")
            print(f"  å¹³å‡ç½®ä¿¡åº¦: {np.mean(segment_confidences):.4f}")
    
    # å›ç¯æ£€æµ‹åˆ†æ
    print(f"\nğŸ”„ å›ç¯æ£€æµ‹åˆ†æ")
    print(f"{'='*50}")
    
    # å¯»æ‰¾å¯èƒ½çš„å›ç¯æ¨¡å¼
    class_positions = {}
    for i, (pred, idx) in enumerate(zip(valid_predictions, valid_indices)):
        if pred not in class_positions:
            class_positions[pred] = []
        class_positions[pred].append((idx, i))
    
    potential_loops = []
    for cls, positions in class_positions.items():
        if len(positions) > 1:
            # è®¡ç®—ä½ç½®é—´éš”
            pos_indices = [pos[0] for pos in positions]
            gaps = []
            for i in range(1, len(pos_indices)):
                gap = pos_indices[i] - pos_indices[i-1]
                gaps.append(gap)
            
            # å¦‚æœé—´éš”è¾ƒå¤§ï¼Œå¯èƒ½æ˜¯çœŸæ­£çš„å›ç¯
            if any(gap > 200 for gap in gaps):
                potential_loops.append((cls, positions, gaps))
    
    if potential_loops:
        print("å‘ç°æ½œåœ¨å›ç¯æ¨¡å¼:")
        for cls, positions, gaps in potential_loops:
            pos_indices = [pos[0] for pos in positions]
            print(f"  ç±»åˆ« {cls}: å‡ºç°åœ¨æ–‡ä»¶ç´¢å¼• {pos_indices}")
            print(f"    é—´éš”: {gaps} (æ–‡ä»¶æ•°)")
            
            # è®¡ç®—è¯¥ç±»åˆ«çš„ç½®ä¿¡åº¦
            cls_confidences = [valid_confidences[pos[1]] for pos in positions]
            print(f"    ç½®ä¿¡åº¦: {[f'{conf:.3f}' for conf in cls_confidences]}")
    else:
        print("æœªå‘ç°æ˜æ˜¾çš„å›ç¯æ¨¡å¼")
        print("å¯èƒ½åŸå› :")
        print("  1. æœå›­ç¯å¢ƒä¸è®­ç»ƒæ•°æ®(å†œç”°)å·®å¼‚è¾ƒå¤§")
        print("  2. æ¨¡å‹è®­ç»ƒä¸å……åˆ†(ä»…20ä¸ªepoch)")
        print("  3. æœå›­è½¨è¿¹å¯èƒ½æ²¡æœ‰æ˜æ˜¾çš„é‡å¤è®¿é—®æ¨¡å¼")
    
    # æ¨¡å‹æ€§èƒ½è¯„ä¼°
    print(f"\nğŸ¤– æ¨¡å‹æ€§èƒ½è¯„ä¼°")
    print(f"{'='*50}")
    
    # ç½®ä¿¡åº¦åˆ†å¸ƒåˆ†æ
    high_conf_count = np.sum(valid_confidences > 0.2)
    medium_conf_count = np.sum((valid_confidences > 0.1) & (valid_confidences <= 0.2))
    low_conf_count = np.sum(valid_confidences <= 0.1)
    
    print(f"ç½®ä¿¡åº¦åˆ†å¸ƒ:")
    print(f"  é«˜ç½®ä¿¡åº¦ (>0.2): {high_conf_count} ä¸ª ({high_conf_count/len(valid_confidences)*100:.1f}%)")
    print(f"  ä¸­ç½®ä¿¡åº¦ (0.1-0.2): {medium_conf_count} ä¸ª ({medium_conf_count/len(valid_confidences)*100:.1f}%)")
    print(f"  ä½ç½®ä¿¡åº¦ (â‰¤0.1): {low_conf_count} ä¸ª ({low_conf_count/len(valid_confidences)*100:.1f}%)")
    
    # é¢„æµ‹å¤šæ ·æ€§åˆ†æ
    entropy = -np.sum((counts/len(valid_predictions)) * np.log2(counts/len(valid_predictions)))
    max_entropy = np.log2(len(unique))
    normalized_entropy = entropy / max_entropy
    
    print(f"\né¢„æµ‹å¤šæ ·æ€§:")
    print(f"  é¢„æµ‹ç†µ: {entropy:.3f}")
    print(f"  å½’ä¸€åŒ–ç†µ: {normalized_entropy:.3f}")
    print(f"  å¤šæ ·æ€§è¯„ä»·: {'é«˜' if normalized_entropy > 0.8 else 'ä¸­' if normalized_entropy > 0.5 else 'ä½'}")
    
    # ç»“è®ºå’Œå»ºè®®
    print(f"\nğŸ“ ç»“è®ºå’Œå»ºè®®")
    print(f"{'='*50}")
    
    print("ä¸»è¦å‘ç°:")
    print("1. æ¨¡å‹èƒ½å¤Ÿå¯¹æœå›­æ•°æ®è¿›è¡Œé¢„æµ‹ï¼Œä½†ç½®ä¿¡åº¦æ™®éè¾ƒä½")
    print("2. é¢„æµ‹ç»“æœæ˜¾ç¤ºä¸€å®šçš„å¤šæ ·æ€§ï¼Œè¯´æ˜æ¨¡å‹åœ¨å°è¯•åŒºåˆ†ä¸åŒåœºæ™¯")
    print("3. æœªå‘ç°æ˜æ˜¾çš„å›ç¯æ¨¡å¼ï¼Œå¯èƒ½æ˜¯ç¯å¢ƒå·®å¼‚å¯¼è‡´çš„")
    
    print("\næ”¹è¿›å»ºè®®:")
    print("1. ä½¿ç”¨æ›´å……åˆ†è®­ç»ƒçš„æ¨¡å‹ (æ›´å¤šepochå’Œæ›´å¥½çš„è¶…å‚æ•°)")
    print("2. åœ¨æœå›­æ•°æ®ä¸Šè¿›è¡ŒåŸŸé€‚åº”æˆ–å¾®è°ƒ")
    print("3. æ”¶é›†æ›´å¤šæœå›­ç¯å¢ƒçš„è®­ç»ƒæ•°æ®")
    print("4. è€ƒè™‘ä½¿ç”¨æ— ç›‘ç£æˆ–åŠç›‘ç£æ–¹æ³•")
    print("5. åˆ†æå†œç”°å’Œæœå›­ç¯å¢ƒçš„ScanContextç‰¹å¾å·®å¼‚")
    
    print(f"\nğŸ“Š å¯è§†åŒ–æ–‡ä»¶ä½ç½®:")
    print(f"  é¢„æµ‹åºåˆ—å›¾: results/simple_test/simple_predictions.png")
    print(f"  è¯¦ç»†ç»“æœæ•°æ®: results/simple_test_results.pkl")
    
    print(f"\n" + "="*80)
    print("æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    print("="*80)

if __name__ == '__main__':
    generate_final_report()
