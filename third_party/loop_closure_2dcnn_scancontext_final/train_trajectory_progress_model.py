#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è®­ç»ƒè½¨è¿¹è¿›å±•é¢„æµ‹æ¨¡å‹ - é¢„æµ‹å½“å‰å¤„äºè½¨è¿¹çš„ç¬¬å‡ æ®µï¼ˆ0-19ï¼‰
åŸºäºæ—¶é—´é¡ºåºçš„çœŸå®æ ‡ç­¾
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pickle
import matplotlib.pyplot as plt
from pathlib import Path
import time
from sklearn.metrics import accuracy_score, classification_report
from models.temporal_models import Temporal3DCNN
from utils.ply_reader import PLYReader
from utils.scan_context import ScanContext
import glob
import warnings
warnings.filterwarnings('ignore')

class TrajectoryProgressTrainer:
    """è½¨è¿¹è¿›å±•é¢„æµ‹è®­ç»ƒå™¨"""
    
    def __init__(self, sequence_length=5, num_classes=20, learning_rate=0.001):
        self.sequence_length = sequence_length
        self.num_classes = num_classes
        self.learning_rate = learning_rate
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"ğŸ¯ è½¨è¿¹è¿›å±•é¢„æµ‹è®­ç»ƒå™¨")
        print(f"è®¾å¤‡: {self.device}")
        print(f"ç›®æ ‡: é¢„æµ‹è½¨è¿¹è¿›å±• 0â†’1â†’2â†’...â†’19")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = Temporal3DCNN(
            input_shape=(1, sequence_length, 20, 60),
            num_classes=num_classes
        )
        self.model = self.model.to(self.device)
        
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=20, gamma=0.5)
        
        # è®­ç»ƒå†å²
        self.train_losses = []
        self.train_accuracies = []
        self.val_losses = []
        self.val_accuracies = []
        
        print(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    def create_trajectory_progress_dataset(self, data_dir, sequence_length=5):
        """åŸºäºæ—¶é—´é¡ºåºåˆ›å»ºè½¨è¿¹è¿›å±•æ•°æ®é›†"""
        print(f"ğŸ“‚ åˆ›å»ºè½¨è¿¹è¿›å±•æ•°æ®é›†...")
        
        # è·å–æ‰€æœ‰plyæ–‡ä»¶å¹¶æŒ‰æ—¶é—´æ’åº
        ply_files = sorted(glob.glob(f"{data_dir}/*.ply"))
        print(f"æ‰¾åˆ° {len(ply_files)} ä¸ªplyæ–‡ä»¶")
        
        if len(ply_files) == 0:
            print("âŒ æœªæ‰¾åˆ°plyæ–‡ä»¶")
            return None, None
        
        # ç”ŸæˆScanContextç‰¹å¾
        sc_generator = ScanContext()
        scan_contexts = []
        
        print("ç”ŸæˆScanContextç‰¹å¾...")
        for i, ply_file in enumerate(ply_files):
            if i % 100 == 0:
                print(f"  å¤„ç† {i+1}/{len(ply_files)}")
            
            try:
                points = PLYReader.read_ply_file(ply_file)
                if points is not None and len(points) > 100:
                    points = points[:, :3]  # åªå–x,y,z
                    sc = sc_generator.generate_scan_context(points)
                    scan_contexts.append(sc)
                else:
                    scan_contexts.append(None)
            except Exception as e:
                print(f"å¤„ç†å¤±è´¥ {ply_file}: {e}")
                scan_contexts.append(None)
        
        # åˆ›å»ºæ—¶åºåºåˆ—å’ŒåŸºäºæ—¶é—´çš„æ ‡ç­¾
        sequences = []
        labels = []
        
        print("åˆ›å»ºæ—¶åºåºåˆ—å’Œæ—¶é—´æ ‡ç­¾...")
        total_files = len(scan_contexts)
        
        for i in range(len(scan_contexts) - sequence_length + 1):
            # æ£€æŸ¥åºåˆ—ä¸­çš„æ‰€æœ‰ScanContextéƒ½æœ‰æ•ˆ
            sequence_scs = scan_contexts[i:i+sequence_length]
            if all(sc is not None for sc in sequence_scs):
                # è®¡ç®—å½“å‰ä½ç½®åœ¨æ•´ä¸ªè½¨è¿¹ä¸­çš„è¿›å±•
                middle_idx = i + sequence_length // 2
                progress = int((middle_idx / total_files) * self.num_classes)
                progress = min(progress, self.num_classes - 1)  # ç¡®ä¿ä¸è¶…è¿‡19
                
                sequence = np.stack(sequence_scs, axis=0)
                sequences.append(sequence)
                labels.append(progress)
        
        sequences = np.array(sequences)
        labels = np.array(labels)
        
        print(f"åˆ›å»ºäº† {len(sequences)} ä¸ªåºåˆ—")
        print(f"æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(labels)}")
        
        return sequences, labels
    
    def create_data_loaders(self, sequences, labels, batch_size=16):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        print(f"ğŸ”„ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        
        # æ•°æ®åˆ’åˆ†
        from sklearn.model_selection import train_test_split
        
        train_seq, temp_seq, train_labels, temp_labels = train_test_split(
            sequences, labels, test_size=0.4, random_state=42, stratify=labels
        )
        
        val_seq, test_seq, val_labels, test_labels = train_test_split(
            temp_seq, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels
        )
        
        print(f"è®­ç»ƒé›†: {len(train_seq)} æ ·æœ¬")
        print(f"éªŒè¯é›†: {len(val_seq)} æ ·æœ¬")
        print(f"æµ‹è¯•é›†: {len(test_seq)} æ ·æœ¬")
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        train_tensor = torch.FloatTensor(train_seq).to(self.device)
        val_tensor = torch.FloatTensor(val_seq).to(self.device)
        test_tensor = torch.FloatTensor(test_seq).to(self.device)
        
        train_labels_tensor = torch.LongTensor(train_labels).to(self.device)
        val_labels_tensor = torch.LongTensor(val_labels).to(self.device)
        test_labels_tensor = torch.LongTensor(test_labels).to(self.device)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(train_tensor, train_labels_tensor)
        val_dataset = TensorDataset(val_tensor, val_labels_tensor)
        test_dataset = TensorDataset(test_tensor, test_labels_tensor)
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        return train_loader, val_loader, test_loader
    
    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            self.optimizer.zero_grad()
            
            output = self.model(data)
            loss = self.criterion(output, target)
            
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
            
            if batch_idx % 5 == 0:
                print(f'  æ‰¹æ¬¡ {batch_idx:2d}/{len(train_loader):2d} | '
                      f'æŸå¤±: {loss.item():.4f} | '
                      f'å‡†ç¡®ç‡: {100.*correct/total:.1f}%')
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def validate(self, val_loader):
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                output = self.model(data)
                loss = self.criterion(output, target)
                
                total_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                total += target.size(0)
                correct += (predicted == target).sum().item()
        
        avg_loss = total_loss / len(val_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def train(self, train_loader, val_loader, epochs=50):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ (epochs={epochs})...")
        
        best_val_acc = 0
        best_model_state = None
        patience = 10
        patience_counter = 0
        
        start_time = time.time()
        
        for epoch in range(epochs):
            epoch_start = time.time()
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader)
            
            # éªŒè¯
            val_loss, val_acc = self.validate(val_loader)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()
            
            # è®°å½•å†å²
            self.train_losses.append(train_loss)
            self.train_accuracies.append(train_acc)
            self.val_losses.append(val_loss)
            self.val_accuracies.append(val_acc)
            
            epoch_time = time.time() - epoch_start
            
            print(f'\nEpoch {epoch+1:2d}/{epochs:2d} | æ—¶é—´: {epoch_time:.1f}s')
            print(f'è®­ç»ƒ - æŸå¤±: {train_loss:.4f} | å‡†ç¡®ç‡: {train_acc:.1f}%')
            print(f'éªŒè¯ - æŸå¤±: {val_loss:.4f} | å‡†ç¡®ç‡: {val_acc:.1f}%')
            print(f'å­¦ä¹ ç‡: {self.optimizer.param_groups[0]["lr"]:.6f}')
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_model_state = self.model.state_dict().copy()
                patience_counter = 0
                print(f'ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.1f}%')
            else:
                patience_counter += 1
            
            # æ—©åœ
            if patience_counter >= patience:
                print(f'\nâ¹ï¸  æ—©åœè§¦å‘ (patience={patience})')
                break
            
            print('-' * 50)
        
        total_time = time.time() - start_time
        print(f'\nâœ… è®­ç»ƒå®Œæˆ!')
        print(f'æ€»è®­ç»ƒæ—¶é—´: {total_time:.1f}ç§’')
        print(f'æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.1f}%')
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if best_model_state is not None:
            self.model.load_state_dict(best_model_state)
            print('âœ… å·²åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡')
        
        return best_val_acc
    
    def test(self, test_loader):
        """æµ‹è¯•æ¨¡å‹"""
        print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹...")
        
        self.model.eval()
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for data, target in test_loader:
                output = self.model(data)
                _, predicted = torch.max(output.data, 1)
                
                all_predictions.extend(predicted.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
        
        test_acc = accuracy_score(all_targets, all_predictions) * 100
        print(f'æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.1f}%')
        
        return test_acc, all_predictions, all_targets
    
    def save_model(self, filepath, metadata=None):
        """ä¿å­˜æ¨¡å‹"""
        save_dir = Path(filepath).parent
        save_dir.mkdir(parents=True, exist_ok=True)
        
        save_dict = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'train_losses': self.train_losses,
            'train_accuracies': self.train_accuracies,
            'val_losses': self.val_losses,
            'val_accuracies': self.val_accuracies,
            'sequence_length': self.sequence_length,
            'num_classes': self.num_classes,
            'learning_rate': self.learning_rate,
        }
        
        if metadata:
            save_dict.update(metadata)
        
        torch.save(save_dict, filepath)
        print(f'âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}')

def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("ğŸ¯ è½¨è¿¹è¿›å±•é¢„æµ‹æ¨¡å‹è®­ç»ƒ")
    print("="*80)
    
    # æ•°æ®è·¯å¾„
    data_dir = "/mysda/shared_dir/2025.7.3/2025-07-03-16-28-57.ply"
    
    # è®­ç»ƒå‚æ•°
    sequence_length = 5
    num_classes = 20
    learning_rate = 0.001
    batch_size = 16
    epochs = 50
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = TrajectoryProgressTrainer(
        sequence_length=sequence_length,
        num_classes=num_classes,
        learning_rate=learning_rate
    )
    
    # åˆ›å»ºæ•°æ®é›†
    sequences, labels = trainer.create_trajectory_progress_dataset(data_dir, sequence_length)
    
    if sequences is None:
        print("âŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥")
        return
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader, test_loader = trainer.create_data_loaders(
        sequences, labels, batch_size
    )
    
    # è®­ç»ƒæ¨¡å‹
    best_val_acc = trainer.train(train_loader, val_loader, epochs)
    
    # æµ‹è¯•æ¨¡å‹
    test_acc, predictions, targets = trainer.test(test_loader)
    
    # ä¿å­˜æ¨¡å‹
    model_path = f"models/saved/trajectory_progress_model_acc{test_acc:.1f}.pth"
    metadata = {
        'best_val_acc': best_val_acc,
        'test_acc': test_acc,
        'batch_size': batch_size,
        'epochs': epochs,
        'data_type': 'trajectory_progress'
    }
    trainer.save_model(model_path, metadata)
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.1f}%")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.1f}%")
    print(f"æ¨¡å‹ä¿å­˜è·¯å¾„: {model_path}")

if __name__ == '__main__':
    main()
