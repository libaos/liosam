#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è®­ç»ƒ2D CNNè½¨è¿¹åˆ†æ®µé¢„æµ‹æ¨¡å‹
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import time
from sklearn.metrics import accuracy_score, classification_report
from sklearn.utils.class_weight import compute_class_weight
from models.cnn_2d_models import Simple2DCNN, Enhanced2DCNN, ResNet2D
from utils.ply_reader import PLYReader
from utils.scan_context import ScanContext
import glob
import warnings
warnings.filterwarnings('ignore')

class CNN2DTrajectoryTrainer:
    """2D CNNè½¨è¿¹åˆ†æ®µé¢„æµ‹è®­ç»ƒå™¨"""
    
    def __init__(self, model_type='Enhanced2DCNN', num_classes=20, learning_rate=0.001):
        self.model_type = model_type
        self.num_classes = num_classes
        self.learning_rate = learning_rate
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"ğŸ¯ 2D CNNè½¨è¿¹åˆ†æ®µé¢„æµ‹è®­ç»ƒå™¨")
        print(f"è®¾å¤‡: {self.device}")
        print(f"æ¨¡å‹ç±»å‹: {model_type}")
        print(f"ç›®æ ‡: æé«˜è½¨è¿¹åˆ†æ®µé¢„æµ‹å‡†ç¡®ç‡")
        
        # åˆå§‹åŒ–æ¨¡å‹
        if model_type == 'Simple2DCNN':
            self.model = Simple2DCNN(num_classes=num_classes)
        elif model_type == 'Enhanced2DCNN':
            self.model = Enhanced2DCNN(num_classes=num_classes)
        elif model_type == 'ResNet2D':
            self.model = ResNet2D(num_classes=num_classes)
        else:
            raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_type}")
        
        self.model = self.model.to(self.device)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in self.model.parameters())
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {total_params:,}")
        
        # è®­ç»ƒå†å²
        self.train_losses = []
        self.train_accuracies = []
        self.val_losses = []
        self.val_accuracies = []
        self.class_accuracies = []
        
        print(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    def create_2d_dataset(self, data_dir):
        """åˆ›å»º2D CNNæ•°æ®é›†ï¼ˆå•å¸§ScanContextï¼‰"""
        print(f"ğŸ“‚ åˆ›å»º2D CNNæ•°æ®é›†...")
        
        # è·å–æ‰€æœ‰plyæ–‡ä»¶å¹¶æŒ‰æ—¶é—´æ’åº
        ply_files = sorted(glob.glob(f"{data_dir}/*.ply"))
        print(f"æ‰¾åˆ° {len(ply_files)} ä¸ªplyæ–‡ä»¶")
        
        if len(ply_files) == 0:
            print("âŒ æœªæ‰¾åˆ°plyæ–‡ä»¶")
            return None, None, None
        
        # ç”ŸæˆScanContextç‰¹å¾
        sc_generator = ScanContext()
        scan_contexts = []
        labels = []
        
        print("ç”ŸæˆScanContextç‰¹å¾å’Œæ ‡ç­¾...")
        total_files = len(ply_files)
        target_samples_per_class = 150  # æ¯ä¸ªç±»åˆ«ç›®æ ‡æ ·æœ¬æ•°
        
        for i, ply_file in enumerate(ply_files):
            if i % 100 == 0:
                print(f"  å¤„ç† {i+1}/{len(ply_files)}")
            
            try:
                points = PLYReader.read_ply_file(ply_file)
                if points is not None and len(points) > 100:
                    points = points[:, :3]  # åªå–x,y,z
                    sc = sc_generator.generate_scan_context(points)
                    
                    # è®¡ç®—åŸºäºæ—¶é—´è¿›å±•çš„æ ‡ç­¾
                    progress = int((i / total_files) * self.num_classes)
                    progress = min(progress, self.num_classes - 1)
                    
                    scan_contexts.append(sc)
                    labels.append(progress)
                    
            except Exception as e:
                print(f"å¤„ç†å¤±è´¥ {ply_file}: {e}")
                continue
        
        print(f"ç”Ÿæˆäº† {len(scan_contexts)} ä¸ªScanContextç‰¹å¾")
        
        # å¹³è¡¡æ•°æ®é›†
        print("å¹³è¡¡æ•°æ®é›†...")
        balanced_contexts = []
        balanced_labels = []
        
        for class_id in range(self.num_classes):
            class_indices = [i for i, label in enumerate(labels) if label == class_id]
            
            if len(class_indices) > 0:
                if len(class_indices) < target_samples_per_class:
                    # æ•°æ®å¢å¼ºï¼šé‡å¤é‡‡æ ·
                    indices = np.random.choice(class_indices, target_samples_per_class, replace=True)
                else:
                    # éšæœºé‡‡æ ·
                    indices = np.random.choice(class_indices, target_samples_per_class, replace=False)
                
                for idx in indices:
                    balanced_contexts.append(scan_contexts[idx])
                    balanced_labels.append(class_id)
                
                print(f"ç±»åˆ« {class_id}: {len(class_indices)} -> {len(indices)} æ ·æœ¬")
        
        scan_contexts = np.array(balanced_contexts)
        labels = np.array(balanced_labels)
        
        print(f"å¹³è¡¡åæ•°æ®é›†: {len(scan_contexts)} ä¸ªæ ·æœ¬")
        print(f"æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(labels)}")
        
        # è®¡ç®—ç±»åˆ«æƒé‡
        class_weights = compute_class_weight('balanced', 
                                           classes=np.unique(labels), 
                                           y=labels)
        class_weights = torch.FloatTensor(class_weights).to(self.device)
        
        return scan_contexts, labels, class_weights
    
    def create_data_loaders(self, scan_contexts, labels, class_weights, batch_size=32):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        print(f"ğŸ”„ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        
        # æ•°æ®åˆ’åˆ†
        from sklearn.model_selection import train_test_split
        
        train_contexts, temp_contexts, train_labels, temp_labels = train_test_split(
            scan_contexts, labels, test_size=0.4, random_state=42, stratify=labels
        )
        
        val_contexts, test_contexts, val_labels, test_labels = train_test_split(
            temp_contexts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels
        )
        
        print(f"è®­ç»ƒé›†: {len(train_contexts)} æ ·æœ¬")
        print(f"éªŒè¯é›†: {len(val_contexts)} æ ·æœ¬")
        print(f"æµ‹è¯•é›†: {len(test_contexts)} æ ·æœ¬")
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡ (æ·»åŠ é€šé“ç»´åº¦)
        train_tensor = torch.FloatTensor(train_contexts).unsqueeze(1).to(self.device)  # (N, 1, 20, 60)
        val_tensor = torch.FloatTensor(val_contexts).unsqueeze(1).to(self.device)
        test_tensor = torch.FloatTensor(test_contexts).unsqueeze(1).to(self.device)
        
        train_labels_tensor = torch.LongTensor(train_labels).to(self.device)
        val_labels_tensor = torch.LongTensor(val_labels).to(self.device)
        test_labels_tensor = torch.LongTensor(test_labels).to(self.device)
        
        # åˆ›å»ºåŠ æƒé‡‡æ ·å™¨
        sample_weights = [class_weights[label] for label in train_labels]
        sampler = WeightedRandomSampler(sample_weights, len(sample_weights))
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(train_tensor, train_labels_tensor)
        val_dataset = TensorDataset(val_tensor, val_labels_tensor)
        test_dataset = TensorDataset(test_tensor, test_labels_tensor)
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        return train_loader, val_loader, test_loader
    
    def train_model(self, train_loader, val_loader, class_weights, epochs=60):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"\nğŸ¯ å¼€å§‹2D CNNè®­ç»ƒ (epochs={epochs})...")
        
        # ä½¿ç”¨åŠ æƒæŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss(weight=class_weights)
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=20, gamma=0.5)
        
        best_avg_acc = 0
        best_model_state = None
        patience = 15
        patience_counter = 0
        
        start_time = time.time()
        
        for epoch in range(epochs):
            epoch_start = time.time()
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader)
            
            # éªŒè¯
            val_loss, val_acc, class_accs = self.validate_with_class_accuracy(val_loader)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()
            
            # è®°å½•å†å²
            self.train_losses.append(train_loss)
            self.train_accuracies.append(train_acc)
            self.val_losses.append(val_loss)
            self.val_accuracies.append(val_acc)
            self.class_accuracies.append(class_accs)
            
            epoch_time = time.time() - epoch_start
            
            # è®¡ç®—å¹³å‡ç±»åˆ«å‡†ç¡®ç‡
            avg_class_acc = np.mean([acc for acc in class_accs.values() if acc > 0])
            
            print(f'\nEpoch {epoch+1:2d}/{epochs:2d} | æ—¶é—´: {epoch_time:.1f}s')
            print(f'è®­ç»ƒ - æŸå¤±: {train_loss:.4f} | å‡†ç¡®ç‡: {train_acc:.1f}%')
            print(f'éªŒè¯ - æŸå¤±: {val_loss:.4f} | å‡†ç¡®ç‡: {val_acc:.1f}%')
            print(f'å¹³å‡ç±»åˆ«å‡†ç¡®ç‡: {avg_class_acc:.1f}%')
            print(f'å­¦ä¹ ç‡: {self.optimizer.param_groups[0]["lr"]:.6f}')
            
            # æ˜¾ç¤ºæœ€å·®çš„å‡ ä¸ªç±»åˆ«
            sorted_classes = sorted(class_accs.items(), key=lambda x: x[1])
            print(f'æœ€å·®ç±»åˆ«: {sorted_classes[:3]}')
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºå¹³å‡ç±»åˆ«å‡†ç¡®ç‡ï¼‰
            if avg_class_acc > best_avg_acc:
                best_avg_acc = avg_class_acc
                best_model_state = self.model.state_dict().copy()
                patience_counter = 0
                print(f'ğŸ‰ æ–°çš„æœ€ä½³å¹³å‡ç±»åˆ«å‡†ç¡®ç‡: {best_avg_acc:.1f}%')
            else:
                patience_counter += 1
            
            # æ—©åœ
            if patience_counter >= patience:
                print(f'\nâ¹ï¸  æ—©åœè§¦å‘ (patience={patience})')
                break
            
            print('-' * 60)
        
        total_time = time.time() - start_time
        print(f'\nâœ… è®­ç»ƒå®Œæˆ!')
        print(f'æ€»è®­ç»ƒæ—¶é—´: {total_time:.1f}ç§’')
        print(f'æœ€ä½³å¹³å‡ç±»åˆ«å‡†ç¡®ç‡: {best_avg_acc:.1f}%')
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if best_model_state is not None:
            self.model.load_state_dict(best_model_state)
            print('âœ… å·²åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡')
        
        return best_avg_acc
    
    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            self.optimizer.zero_grad()
            
            output = self.model(data)
            loss = self.criterion(output, target)
            
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
            
            if batch_idx % 10 == 0:
                print(f'  æ‰¹æ¬¡ {batch_idx:2d}/{len(train_loader):2d} | '
                      f'æŸå¤±: {loss.item():.4f} | '
                      f'å‡†ç¡®ç‡: {100.*correct/total:.1f}%')
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def validate_with_class_accuracy(self, val_loader):
        """éªŒè¯æ¨¡å‹å¹¶è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡"""
        self.model.eval()
        total_loss = 0
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for data, target in val_loader:
                output = self.model(data)
                loss = self.criterion(output, target)
                
                total_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                
                all_predictions.extend(predicted.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
        
        avg_loss = total_loss / len(val_loader)
        overall_accuracy = accuracy_score(all_targets, all_predictions) * 100
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        class_accuracies = {}
        for class_id in range(self.num_classes):
            class_mask = np.array(all_targets) == class_id
            if np.sum(class_mask) > 0:
                class_predictions = np.array(all_predictions)[class_mask]
                class_targets = np.array(all_targets)[class_mask]
                class_acc = accuracy_score(class_targets, class_predictions) * 100
                class_accuracies[class_id] = class_acc
            else:
                class_accuracies[class_id] = 0
        
        return avg_loss, overall_accuracy, class_accuracies
    
    def test_detailed_analysis(self, test_loader):
        """è¯¦ç»†æµ‹è¯•åˆ†æ"""
        print(f"\nğŸ§ª è¯¦ç»†æµ‹è¯•åˆ†æ...")
        
        self.model.eval()
        all_predictions = []
        all_targets = []
        all_confidences = []
        
        with torch.no_grad():
            for data, target in test_loader:
                output = self.model(data)
                probabilities = torch.softmax(output, dim=1)
                confidence, predicted = torch.max(probabilities, 1)
                
                all_predictions.extend(predicted.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
                all_confidences.extend(confidence.cpu().numpy())
        
        # æ€»ä½“å‡†ç¡®ç‡
        overall_acc = accuracy_score(all_targets, all_predictions) * 100
        print(f'æ€»ä½“æµ‹è¯•å‡†ç¡®ç‡: {overall_acc:.1f}%')
        
        # æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†åˆ†æ
        print(f"\nğŸ“Š æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†åˆ†æ:")
        print(f"{'ç±»åˆ«':<4} {'å‡†ç¡®ç‡':<8} {'æ ·æœ¬æ•°':<6} {'å¹³å‡ç½®ä¿¡åº¦':<10}")
        print("-" * 35)
        
        class_accuracies = []
        for class_id in range(self.num_classes):
            class_mask = np.array(all_targets) == class_id
            if np.sum(class_mask) > 0:
                class_predictions = np.array(all_predictions)[class_mask]
                class_targets = np.array(all_targets)[class_mask]
                class_confidences = np.array(all_confidences)[class_mask]
                
                class_acc = accuracy_score(class_targets, class_predictions) * 100
                avg_conf = np.mean(class_confidences) * 100
                sample_count = np.sum(class_mask)
                
                class_accuracies.append(class_acc)
                print(f"{class_id:2d}   {class_acc:6.1f}%   {sample_count:4d}    {avg_conf:6.1f}%")
            else:
                class_accuracies.append(0)
                print(f"{class_id:2d}   {0:6.1f}%   {0:4d}    {0:6.1f}%")
        
        # ç»Ÿè®¡åˆ†æ
        avg_class_acc = np.mean([acc for acc in class_accuracies if acc > 0])
        min_class_acc = min([acc for acc in class_accuracies if acc > 0])
        max_class_acc = max(class_accuracies)
        
        print(f"\nğŸ“ˆ ç»Ÿè®¡åˆ†æ:")
        print(f"å¹³å‡ç±»åˆ«å‡†ç¡®ç‡: {avg_class_acc:.1f}%")
        print(f"æœ€ä½ç±»åˆ«å‡†ç¡®ç‡: {min_class_acc:.1f}%")
        print(f"æœ€é«˜ç±»åˆ«å‡†ç¡®ç‡: {max_class_acc:.1f}%")
        print(f"å‡†ç¡®ç‡æ ‡å‡†å·®: {np.std(class_accuracies):.1f}%")
        
        return overall_acc, avg_class_acc, class_accuracies
    
    def save_model(self, filepath, metadata=None):
        """ä¿å­˜æ¨¡å‹"""
        save_dir = Path(filepath).parent
        save_dir.mkdir(parents=True, exist_ok=True)
        
        save_dict = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'model_type': self.model_type,
            'train_losses': self.train_losses,
            'train_accuracies': self.train_accuracies,
            'val_losses': self.val_losses,
            'val_accuracies': self.val_accuracies,
            'class_accuracies': self.class_accuracies,
            'num_classes': self.num_classes,
            'learning_rate': self.learning_rate,
        }
        
        if metadata:
            save_dict.update(metadata)
        
        torch.save(save_dict, filepath)
        print(f'âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}')

def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("ğŸ¯ 2D CNNè½¨è¿¹åˆ†æ®µé¢„æµ‹æ¨¡å‹è®­ç»ƒ")
    print("="*80)
    
    # æ•°æ®è·¯å¾„
    data_dir = "/mysda/shared_dir/2025.7.3/2025-07-03-16-28-57.ply"
    
    # è®­ç»ƒå‚æ•°
    model_types = ['Simple2DCNN', 'Enhanced2DCNN', 'ResNet2D']
    
    for model_type in model_types:
        print(f"\n{'='*60}")
        print(f"è®­ç»ƒ {model_type} æ¨¡å‹")
        print(f"{'='*60}")
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = CNN2DTrajectoryTrainer(
            model_type=model_type,
            num_classes=20,
            learning_rate=0.001
        )
        
        # åˆ›å»ºæ•°æ®é›†
        scan_contexts, labels, class_weights = trainer.create_2d_dataset(data_dir)
        
        if scan_contexts is None:
            print("âŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥")
            continue
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader, test_loader = trainer.create_data_loaders(
            scan_contexts, labels, class_weights, batch_size=32
        )
        
        # è®­ç»ƒæ¨¡å‹
        best_avg_acc = trainer.train_model(train_loader, val_loader, class_weights, epochs=50)
        
        # æµ‹è¯•æ¨¡å‹
        overall_acc, avg_class_acc, class_accuracies = trainer.test_detailed_analysis(test_loader)
        
        # ä¿å­˜æ¨¡å‹
        model_path = f"models/saved/{model_type.lower()}_trajectory_avg{avg_class_acc:.1f}.pth"
        metadata = {
            'best_avg_class_acc': best_avg_acc,
            'test_overall_acc': overall_acc,
            'test_avg_class_acc': avg_class_acc,
            'class_accuracies': class_accuracies,
            'data_type': '2d_cnn_trajectory'
        }
        trainer.save_model(model_path, metadata)
        
        print(f"\nğŸ‰ {model_type} è®­ç»ƒå®Œæˆ!")
        print(f"æœ€ä½³å¹³å‡ç±»åˆ«å‡†ç¡®ç‡: {best_avg_acc:.1f}%")
        print(f"æµ‹è¯•æ€»ä½“å‡†ç¡®ç‡: {overall_acc:.1f}%")
        print(f"æµ‹è¯•å¹³å‡ç±»åˆ«å‡†ç¡®ç‡: {avg_class_acc:.1f}%")

if __name__ == '__main__':
    main()
