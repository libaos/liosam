#!/usr/bin/env python3
"""
è¯„ä¼°æŒ‡æ ‡è®¡ç®—
ç”¨äºå›ç¯æ£€æµ‹æ¨¡å‹æ€§èƒ½è¯„ä¼°
"""

import torch
import torch.nn.functional as F
import numpy as np
from sklearn.metrics import average_precision_score
import time

def evaluate_model(model, data_loader, device, top_k_list=[1, 3, 5, 10], logger=None):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    
    å‚æ•°:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        data_loader: æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        top_k_list: Top-Kå‡†ç¡®ç‡åˆ—è¡¨
        logger: æ—¥å¿—è®°å½•å™¨
        
    è¿”å›:
        dict: è¯„ä¼°æŒ‡æ ‡å­—å…¸
    """
    model.eval()
    
    all_embeddings = []
    all_labels = []
    
    if logger:
        logger.info("ğŸ” å¼€å§‹æå–ç‰¹å¾...")
    
    # æå–æ‰€æœ‰ç‰¹å¾
    with torch.no_grad():
        for batch_idx, (scan_contexts, labels) in enumerate(data_loader):
            scan_contexts = scan_contexts.to(device)
            labels = labels.to(device)
            
            # å‰å‘ä¼ æ’­
            embeddings = model(scan_contexts)
            
            all_embeddings.append(embeddings.cpu())
            all_labels.append(labels.cpu())
            
            if logger and (batch_idx + 1) % 5 == 0:
                logger.info(f"   å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{len(data_loader)}")
    
    # åˆå¹¶æ‰€æœ‰ç‰¹å¾
    all_embeddings = torch.cat(all_embeddings, dim=0)
    all_labels = torch.cat(all_labels, dim=0)
    
    if logger:
        logger.info(f"âœ… ç‰¹å¾æå–å®Œæˆï¼Œå…± {len(all_embeddings)} ä¸ªæ ·æœ¬")
    
    # è®¡ç®—å„ç§æŒ‡æ ‡
    metrics = {}
    
    # è®¡ç®—Top-Kå‡†ç¡®ç‡
    for k in top_k_list:
        top_k_acc = compute_top_k_accuracy(all_embeddings, all_labels, k)
        metrics[f'top_{k}'] = top_k_acc
        if logger:
            logger.info(f"   Top-{k} å‡†ç¡®ç‡: {top_k_acc:.4f}")
    
    # è®¡ç®—mAP
    mAP = compute_mean_average_precision(all_embeddings, all_labels)
    metrics['mAP'] = mAP
    if logger:
        logger.info(f"   mAP: {mAP:.4f}")
    
    # è®¡ç®—åˆ†ç¦»æ¯”
    separation_ratio = compute_separation_ratio(all_embeddings, all_labels)
    metrics['separation_ratio'] = separation_ratio
    if logger:
        logger.info(f"   åˆ†ç¦»æ¯”: {separation_ratio:.4f}")
    
    # è®¡ç®—å¹³å‡è·ç¦»
    intra_class_dist, inter_class_dist = compute_class_distances(all_embeddings, all_labels)
    metrics['intra_class_distance'] = intra_class_dist
    metrics['inter_class_distance'] = inter_class_dist
    if logger:
        logger.info(f"   ç±»å†…å¹³å‡è·ç¦»: {intra_class_dist:.4f}")
        logger.info(f"   ç±»é—´å¹³å‡è·ç¦»: {inter_class_dist:.4f}")
    
    return metrics

def compute_top_k_accuracy(embeddings, labels, k):
    """
    è®¡ç®—Top-Kå‡†ç¡®ç‡
    
    å‚æ•°:
        embeddings (torch.Tensor): åµŒå…¥å‘é‡ [N, D]
        labels (torch.Tensor): æ ‡ç­¾ [N]
        k (int): Kå€¼
        
    è¿”å›:
        float: Top-Kå‡†ç¡®ç‡
    """
    n_samples = embeddings.size(0)
    
    # L2å½’ä¸€åŒ–
    embeddings = F.normalize(embeddings, p=2, dim=1)
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    similarity_matrix = torch.matmul(embeddings, embeddings.t())
    
    correct = 0
    total = 0
    
    for i in range(n_samples):
        # è·å–å½“å‰æ ·æœ¬çš„ç›¸ä¼¼åº¦
        similarities = similarity_matrix[i]
        
        # æ’é™¤è‡ªå·±
        similarities[i] = -float('inf')
        
        # è·å–Top-Kæœ€ç›¸ä¼¼çš„æ ·æœ¬
        _, top_k_indices = torch.topk(similarities, k)
        
        # æ£€æŸ¥Top-Kä¸­æ˜¯å¦æœ‰ç›¸åŒæ ‡ç­¾çš„æ ·æœ¬
        current_label = labels[i]
        top_k_labels = labels[top_k_indices]
        
        if torch.any(top_k_labels == current_label):
            correct += 1
        total += 1
    
    return correct / total if total > 0 else 0.0

def compute_mean_average_precision(embeddings, labels):
    """
    è®¡ç®—å¹³å‡ç²¾åº¦å‡å€¼ (mAP)
    
    å‚æ•°:
        embeddings (torch.Tensor): åµŒå…¥å‘é‡ [N, D]
        labels (torch.Tensor): æ ‡ç­¾ [N]
        
    è¿”å›:
        float: mAPå€¼
    """
    n_samples = embeddings.size(0)
    
    # L2å½’ä¸€åŒ–
    embeddings = F.normalize(embeddings, p=2, dim=1)
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    similarity_matrix = torch.matmul(embeddings, embeddings.t())
    
    average_precisions = []
    
    for i in range(n_samples):
        # è·å–å½“å‰æ ·æœ¬çš„ç›¸ä¼¼åº¦
        similarities = similarity_matrix[i]
        
        # æ’é™¤è‡ªå·±
        similarities[i] = -float('inf')
        
        # åˆ›å»ºçœŸå®æ ‡ç­¾ï¼ˆç›¸åŒæ ‡ç­¾ä¸º1ï¼Œä¸åŒæ ‡ç­¾ä¸º0ï¼‰
        current_label = labels[i]
        true_labels = (labels == current_label).float()
        true_labels[i] = 0  # æ’é™¤è‡ªå·±
        
        # å¦‚æœæ²¡æœ‰ç›¸åŒæ ‡ç­¾çš„æ ·æœ¬ï¼Œè·³è¿‡
        if torch.sum(true_labels) == 0:
            continue
        
        # è®¡ç®—AP
        similarities_np = similarities.numpy()
        true_labels_np = true_labels.numpy()
        
        try:
            ap = average_precision_score(true_labels_np, similarities_np)
            average_precisions.append(ap)
        except:
            continue
    
    return np.mean(average_precisions) if len(average_precisions) > 0 else 0.0

def compute_separation_ratio(embeddings, labels):
    """
    è®¡ç®—ç±»é—´ç±»å†…è·ç¦»åˆ†ç¦»æ¯”
    
    å‚æ•°:
        embeddings (torch.Tensor): åµŒå…¥å‘é‡ [N, D]
        labels (torch.Tensor): æ ‡ç­¾ [N]
        
    è¿”å›:
        float: åˆ†ç¦»æ¯” (ç±»é—´è·ç¦»/ç±»å†…è·ç¦»)
    """
    intra_dist, inter_dist = compute_class_distances(embeddings, labels)
    
    if intra_dist == 0:
        return float('inf')
    
    return inter_dist / intra_dist

def compute_class_distances(embeddings, labels):
    """
    è®¡ç®—ç±»å†…å’Œç±»é—´å¹³å‡è·ç¦»
    
    å‚æ•°:
        embeddings (torch.Tensor): åµŒå…¥å‘é‡ [N, D]
        labels (torch.Tensor): æ ‡ç­¾ [N]
        
    è¿”å›:
        tuple: (ç±»å†…å¹³å‡è·ç¦», ç±»é—´å¹³å‡è·ç¦»)
    """
    # L2å½’ä¸€åŒ–
    embeddings = F.normalize(embeddings, p=2, dim=1)
    
    # è®¡ç®—è·ç¦»çŸ©é˜µ
    distances = compute_distance_matrix(embeddings)
    
    # åˆ›å»ºæ ‡ç­¾æ©ç 
    labels_equal = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()
    labels_not_equal = 1.0 - labels_equal
    
    # æ’é™¤å¯¹è§’çº¿
    labels_equal.fill_diagonal_(0)
    labels_not_equal.fill_diagonal_(0)
    
    # è®¡ç®—ç±»å†…å¹³å‡è·ç¦»
    intra_distances = distances * labels_equal
    intra_count = torch.sum(labels_equal)
    intra_class_dist = torch.sum(intra_distances) / intra_count if intra_count > 0 else 0.0
    
    # è®¡ç®—ç±»é—´å¹³å‡è·ç¦»
    inter_distances = distances * labels_not_equal
    inter_count = torch.sum(labels_not_equal)
    inter_class_dist = torch.sum(inter_distances) / inter_count if inter_count > 0 else 0.0
    
    # ç¡®ä¿è¿”å›Pythonæ ‡é‡
    if hasattr(intra_class_dist, 'item'):
        intra_class_dist = intra_class_dist.item()
    if hasattr(inter_class_dist, 'item'):
        inter_class_dist = inter_class_dist.item()

    return float(intra_class_dist), float(inter_class_dist)

def compute_distance_matrix(embeddings):
    """
    è®¡ç®—è·ç¦»çŸ©é˜µ
    
    å‚æ•°:
        embeddings (torch.Tensor): åµŒå…¥å‘é‡ [N, D]
        
    è¿”å›:
        torch.Tensor: è·ç¦»çŸ©é˜µ [N, N]
    """
    # è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»çŸ©é˜µ
    dot_product = torch.matmul(embeddings, embeddings.t())
    square_norm = torch.diag(dot_product)
    
    distances = square_norm.unsqueeze(0) - 2.0 * dot_product + square_norm.unsqueeze(1)
    distances = F.relu(distances)
    
    # é¿å…æ•°å€¼ä¸ç¨³å®š
    mask = torch.eq(distances, 0.0).float()
    distances = distances + mask * 1e-16
    distances = torch.sqrt(distances)
    distances = distances * (1.0 - mask)
    
    return distances

def compute_retrieval_metrics(embeddings, labels, distance_threshold=0.5):
    """
    è®¡ç®—æ£€ç´¢ç›¸å…³æŒ‡æ ‡
    
    å‚æ•°:
        embeddings (torch.Tensor): åµŒå…¥å‘é‡ [N, D]
        labels (torch.Tensor): æ ‡ç­¾ [N]
        distance_threshold (float): è·ç¦»é˜ˆå€¼
        
    è¿”å›:
        dict: æ£€ç´¢æŒ‡æ ‡
    """
    # L2å½’ä¸€åŒ–
    embeddings = F.normalize(embeddings, p=2, dim=1)
    
    # è®¡ç®—è·ç¦»çŸ©é˜µ
    distances = compute_distance_matrix(embeddings)
    
    # åˆ›å»ºçœŸå®æ ‡ç­¾çŸ©é˜µ
    labels_matrix = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()
    labels_matrix.fill_diagonal_(0)  # æ’é™¤è‡ªå·±
    
    # åŸºäºè·ç¦»é˜ˆå€¼çš„é¢„æµ‹
    predictions = (distances <= distance_threshold).float()
    predictions.fill_diagonal_(0)  # æ’é™¤è‡ªå·±
    
    # è®¡ç®—TP, FP, TN, FN
    tp = torch.sum(predictions * labels_matrix)
    fp = torch.sum(predictions * (1 - labels_matrix))
    tn = torch.sum((1 - predictions) * (1 - labels_matrix))
    fn = torch.sum((1 - predictions) * labels_matrix)
    
    # è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    accuracy = (tp + tn) / (tp + fp + tn + fn) if (tp + fp + tn + fn) > 0 else 0.0
    
    return {
        'precision': precision.item(),
        'recall': recall.item(),
        'f1_score': f1_score.item(),
        'accuracy': accuracy.item(),
        'tp': tp.item(),
        'fp': fp.item(),
        'tn': tn.item(),
        'fn': fn.item()
    }

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 20
    embedding_dim = 256
    
    embeddings = torch.randn(batch_size, embedding_dim)
    labels = torch.tensor([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 0, 1, 2, 3, 0, 1, 2, 3])
    
    print("æµ‹è¯•è¯„ä¼°æŒ‡æ ‡:")
    
    # æµ‹è¯•Top-Kå‡†ç¡®ç‡
    for k in [1, 3, 5]:
        top_k_acc = compute_top_k_accuracy(embeddings, labels, k)
        print(f"Top-{k} å‡†ç¡®ç‡: {top_k_acc:.4f}")
    
    # æµ‹è¯•mAP
    mAP = compute_mean_average_precision(embeddings, labels)
    print(f"mAP: {mAP:.4f}")
    
    # æµ‹è¯•åˆ†ç¦»æ¯”
    separation_ratio = compute_separation_ratio(embeddings, labels)
    print(f"åˆ†ç¦»æ¯”: {separation_ratio:.4f}")
    
    # æµ‹è¯•ç±»å†…å¤–è·ç¦»
    intra_dist, inter_dist = compute_class_distances(embeddings, labels)
    print(f"ç±»å†…è·ç¦»: {intra_dist:.4f}")
    print(f"ç±»é—´è·ç¦»: {inter_dist:.4f}")
    
    # æµ‹è¯•æ£€ç´¢æŒ‡æ ‡
    retrieval_metrics = compute_retrieval_metrics(embeddings, labels)
    print(f"æ£€ç´¢æŒ‡æ ‡: {retrieval_metrics}")
