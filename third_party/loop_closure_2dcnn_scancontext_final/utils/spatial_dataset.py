#!/usr/bin/env python3
"""
ç®€åŒ–çš„ç©ºé—´æ³¨æ„åŠ›è®­ç»ƒæ•°æ®é›†
ä¸“é—¨ç”¨äºSCStandardSpatialCNNè®­ç»ƒ
"""

import torch
from torch.utils.data import Dataset
import numpy as np
from pathlib import Path
import glob
import random
from .scan_context import ScanContext
from .ply_reader import PLYReader

class SpatialScanContextDataset(Dataset):
    """ç®€åŒ–çš„ScanContextæ•°æ®é›†ï¼Œç”¨äºç©ºé—´æ³¨æ„åŠ›æ¨¡å‹è®­ç»ƒ"""
    
    def __init__(self, data_dir, split='train', split_ratio=0.8, max_files=None, 
                 use_augmentation=False, seed=42):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        å‚æ•°:
            data_dir (str): PLYæ–‡ä»¶ç›®å½•
            split (str): 'train' æˆ– 'val'
            split_ratio (float): è®­ç»ƒé›†æ¯”ä¾‹
            max_files (int): æœ€å¤§æ–‡ä»¶æ•°é‡
            use_augmentation (bool): æ˜¯å¦ä½¿ç”¨æ•°æ®å¢å¼º
            seed (int): éšæœºç§å­
        """
        self.data_dir = Path(data_dir)
        self.split = split
        self.use_augmentation = use_augmentation
        
        # è·å–æ‰€æœ‰PLYæ–‡ä»¶
        ply_files = sorted(glob.glob(str(self.data_dir / "*.ply")))
        
        if len(ply_files) == 0:
            raise ValueError(f"åœ¨ {self.data_dir} ä¸­æœªæ‰¾åˆ°PLYæ–‡ä»¶")
        
        # é™åˆ¶æ–‡ä»¶æ•°é‡
        if max_files and max_files < len(ply_files):
            ply_files = ply_files[:max_files]
        
        # å¯¹äºè¿ç»­è·¯å¾„æ•°æ®ï¼Œä½¿ç”¨æ—¶åºåˆ’åˆ†è€Œä¸æ˜¯éšæœºåˆ’åˆ†
        # ä¿æŒæ–‡ä»¶çš„æ—¶åºé¡ºåº
        print(f"ğŸ“ æ•°æ®é›†ç±»å‹: è¿ç»­è·¯å¾„æ•°æ®ï¼Œä½¿ç”¨æ—¶åºåˆ’åˆ†")

        # æ—¶åºåˆ’åˆ†ï¼šå‰80%ä½œä¸ºè®­ç»ƒé›†ï¼Œå20%ä½œä¸ºéªŒè¯é›†
        split_idx = int(len(ply_files) * split_ratio)

        if split == 'train':
            self.files = ply_files[:split_idx]
            print(f"ğŸ“Š è®­ç»ƒé›†: è·¯å¾„å‰{split_ratio*100:.0f}% ({len(self.files)}ä¸ªæ–‡ä»¶)")
        else:  # val
            self.files = ply_files[split_idx:]
            print(f"ğŸ“Š éªŒè¯é›†: è·¯å¾„å{(1-split_ratio)*100:.0f}% ({len(self.files)}ä¸ªæ–‡ä»¶)")
        
        if len(self.files) == 0:
            raise ValueError(f"åˆ’åˆ†åçš„{split}é›†ä¸ºç©º")
        
        print(f"ğŸ“Š {split}é›†: {len(self.files)} ä¸ªæ–‡ä»¶")
        
        # åˆ›å»ºScanContextç”Ÿæˆå™¨
        self.sc_generator = ScanContext()
        
        # åˆ›å»ºPLYè¯»å–å™¨
        self.ply_reader = PLYReader()

        # åˆ›å»ºæ›´åˆç†çš„æ ‡ç­¾ç­–ç•¥ - åŸºäºä½ç½®åˆ†ç»„
        # å°†è¿ç»­çš„ä½ç½®åˆ†ç»„ï¼Œæ¯ç»„åŒ…å«å¤šä¸ªç›¸ä¼¼ä½ç½®
        self.labels = self._create_position_labels()

        print(f"ğŸ“Š æ ‡ç­¾ç»Ÿè®¡: å…±{len(set(self.labels))}ä¸ªä¸åŒæ ‡ç­¾ï¼Œå¹³å‡æ¯ä¸ªæ ‡ç­¾{len(self.labels)/len(set(self.labels)):.1f}ä¸ªæ ·æœ¬")

    def _create_position_labels(self):
        """
        åˆ›å»ºåŸºäºè¿ç»­è·¯å¾„çš„æ ‡ç­¾
        å¯¹äºè¿ç»­è·¯å¾„æ•°æ®ï¼Œç›¸é‚»ä½ç½®åº”è¯¥æœ‰ç›¸ä¼¼çš„æ ‡ç­¾
        """
        labels = []
        group_size = 10  # æ¯10ä¸ªè¿ç»­ä½ç½®ä¸ºä¸€ç»„ï¼ˆå¢åŠ ç»„å¤§å°ä»¥è·å¾—æ›´å¤šæ­£æ ·æœ¬ï¼‰

        for i, file_path in enumerate(self.files):
            # ä»æ–‡ä»¶åä¸­æå–ä½ç½®ç´¢å¼•
            file_name = Path(file_path).stem
            try:
                # å‡è®¾æ–‡ä»¶åæ ¼å¼ä¸º "cloud_NNNNN.ply"
                import re
                numbers = re.findall(r'\d+', file_name)
                if numbers:
                    position_idx = int(numbers[-1])  # ä½¿ç”¨æœ€åä¸€ä¸ªæ•°å­—ä½œä¸ºä½ç½®ç´¢å¼•
                else:
                    position_idx = i  # å¦‚æœæ²¡æœ‰æ•°å­—ï¼Œä½¿ç”¨æ–‡ä»¶ç´¢å¼•
            except:
                # å¦‚æœæå–å¤±è´¥ï¼Œä½¿ç”¨æ–‡ä»¶åœ¨åˆ—è¡¨ä¸­çš„ç´¢å¼•
                position_idx = i

            # å°†ä½ç½®ç´¢å¼•åˆ†ç»„
            group_label = position_idx // group_size
            labels.append(group_label)

        # ç¡®ä¿æ ‡ç­¾ä»0å¼€å§‹è¿ç»­
        unique_labels = sorted(set(labels))
        label_mapping = {old_label: new_label for new_label, old_label in enumerate(unique_labels)}
        labels = [label_mapping[label] for label in labels]

        print(f"ğŸ“Š åˆ›å»ºäº† {len(unique_labels)} ä¸ªä½ç½®ç»„ï¼Œæ¯ç»„çº¦ {group_size} ä¸ªè¿ç»­ä½ç½®")

        # æ˜¾ç¤ºæ ‡ç­¾åˆ†å¸ƒ
        from collections import Counter
        label_counts = Counter(labels)
        avg_samples_per_label = len(labels) / len(unique_labels)
        print(f"ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ: å¹³å‡æ¯ç»„ {avg_samples_per_label:.1f} ä¸ªæ ·æœ¬")

        return labels
        
        # åˆ›å»ºæ›´åˆç†çš„æ ‡ç­¾ç­–ç•¥ - åŸºäºä½ç½®åˆ†ç»„
        # å°†è¿ç»­çš„ä½ç½®åˆ†ç»„ï¼Œæ¯ç»„åŒ…å«å¤šä¸ªç›¸ä¼¼ä½ç½®
        self.labels = self._create_position_labels()

        print(f"ğŸ“Š æ ‡ç­¾ç»Ÿè®¡: å…±{len(set(self.labels))}ä¸ªä¸åŒæ ‡ç­¾ï¼Œå¹³å‡æ¯ä¸ªæ ‡ç­¾{len(self.labels)/len(set(self.labels)):.1f}ä¸ªæ ·æœ¬")
        
    def __len__(self):
        return len(self.files)
    
    def __getitem__(self, idx):
        """
        è·å–æ•°æ®é¡¹
        
        è¿”å›:
            scan_context (torch.Tensor): ScanContextç‰¹å¾å›¾ [1, H, W]
            label (int): æ ‡ç­¾ï¼ˆæ–‡ä»¶ç´¢å¼•ï¼‰
        """
        try:
            # è¯»å–PLYæ–‡ä»¶
            ply_file = self.files[idx]
            points = self.ply_reader.read_ply_file(ply_file)
            
            # ç”ŸæˆScanContext
            scan_context = self.sc_generator.generate_scan_context(points)
            
            # æ•°æ®å¢å¼º
            if self.use_augmentation and self.split == 'train':
                scan_context = self._augment_scan_context(scan_context)
            
            # è½¬æ¢ä¸ºtensor
            scan_context = torch.from_numpy(scan_context).float()
            
            # æ·»åŠ é€šé“ç»´åº¦
            if len(scan_context.shape) == 2:
                scan_context = scan_context.unsqueeze(0)  # [1, H, W]
            
            # æ ‡ç­¾
            label = self.labels[idx]
            
            return scan_context, label
            
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {self.files[idx]}: {e}")
            # è¿”å›é›¶æ•°æ®
            return torch.zeros(1, 20, 60), 0
    
    def _augment_scan_context(self, scan_context):
        """
        ç®€å•çš„æ•°æ®å¢å¼º
        
        å‚æ•°:
            scan_context (np.ndarray): åŸå§‹ScanContext
            
        è¿”å›:
            np.ndarray: å¢å¼ºåçš„ScanContext
        """
        # éšæœºæ—‹è½¬ï¼ˆå¾ªç¯ç§»ä½ï¼‰
        if random.random() < 0.7:
            shift = random.randint(1, scan_context.shape[1] - 1)
            scan_context = np.roll(scan_context, shift, axis=1)
        
        # æ·»åŠ å™ªå£°
        if random.random() < 0.3:
            noise = np.random.normal(0, 0.01, scan_context.shape)
            scan_context = scan_context + noise
            scan_context = np.clip(scan_context, 0, None)
        
        # å¼ºåº¦ç¼©æ”¾
        if random.random() < 0.3:
            scale = random.uniform(0.9, 1.1)
            scan_context = scan_context * scale
        
        return scan_context
    
    def get_file_info(self, idx):
        """è·å–æ–‡ä»¶ä¿¡æ¯"""
        return {
            'file_path': self.files[idx],
            'label': self.labels[idx],
            'split': self.split
        }

class TripletScanContextDataset(Dataset):
    """ä¸‰å…ƒç»„ScanContextæ•°æ®é›†ï¼Œç”¨äºä¸‰å…ƒç»„æŸå¤±è®­ç»ƒ"""
    
    def __init__(self, base_dataset, triplets_per_sample=5):
        """
        åˆå§‹åŒ–ä¸‰å…ƒç»„æ•°æ®é›†
        
        å‚æ•°:
            base_dataset: åŸºç¡€æ•°æ®é›†
            triplets_per_sample: æ¯ä¸ªæ ·æœ¬ç”Ÿæˆçš„ä¸‰å…ƒç»„æ•°é‡
        """
        self.base_dataset = base_dataset
        self.triplets_per_sample = triplets_per_sample
        
        # æŒ‰æ ‡ç­¾ç»„ç»‡æ•°æ®
        self.label_to_indices = {}
        for idx in range(len(base_dataset)):
            label = base_dataset.labels[idx]
            if label not in self.label_to_indices:
                self.label_to_indices[label] = []
            self.label_to_indices[label].append(idx)
        
        # ç”Ÿæˆä¸‰å…ƒç»„
        self.triplets = self._generate_triplets()
        
    def _generate_triplets(self):
        """ç”Ÿæˆä¸‰å…ƒç»„"""
        triplets = []
        
        for anchor_idx in range(len(self.base_dataset)):
            anchor_label = self.base_dataset.labels[anchor_idx]
            
            # è·å–æ­£æ ·æœ¬å€™é€‰ï¼ˆç›¸åŒæ ‡ç­¾ï¼‰
            positive_candidates = [idx for idx in self.label_to_indices[anchor_label] 
                                 if idx != anchor_idx]
            
            # è·å–è´Ÿæ ·æœ¬å€™é€‰ï¼ˆä¸åŒæ ‡ç­¾ï¼‰
            negative_candidates = []
            for label, indices in self.label_to_indices.items():
                if label != anchor_label:
                    negative_candidates.extend(indices)
            
            # ç”Ÿæˆä¸‰å…ƒç»„
            for _ in range(self.triplets_per_sample):
                if len(positive_candidates) > 0 and len(negative_candidates) > 0:
                    positive_idx = random.choice(positive_candidates)
                    negative_idx = random.choice(negative_candidates)
                    triplets.append((anchor_idx, positive_idx, negative_idx))
        
        return triplets
    
    def __len__(self):
        return len(self.triplets)
    
    def __getitem__(self, idx):
        """
        è·å–ä¸‰å…ƒç»„æ•°æ®
        
        è¿”å›:
            anchor, positive, negative: ä¸‰ä¸ªScanContextç‰¹å¾å›¾
            labels: å¯¹åº”çš„æ ‡ç­¾
        """
        anchor_idx, positive_idx, negative_idx = self.triplets[idx]
        
        anchor_sc, anchor_label = self.base_dataset[anchor_idx]
        positive_sc, positive_label = self.base_dataset[positive_idx]
        negative_sc, negative_label = self.base_dataset[negative_idx]
        
        return (anchor_sc, positive_sc, negative_sc), (anchor_label, positive_label, negative_label)

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®é›†
    try:
        dataset = SpatialScanContextDataset(
            data_dir="data/raw/ply_files",
            split='train',
            max_files=10,
            use_augmentation=True
        )
        
        print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
        
        # æµ‹è¯•è·å–æ•°æ®
        scan_context, label = dataset[0]
        print(f"ScanContextå½¢çŠ¶: {scan_context.shape}")
        print(f"æ ‡ç­¾: {label}")
        
        # æµ‹è¯•ä¸‰å…ƒç»„æ•°æ®é›†
        triplet_dataset = TripletScanContextDataset(dataset, triplets_per_sample=2)
        print(f"ä¸‰å…ƒç»„æ•°æ®é›†å¤§å°: {len(triplet_dataset)}")
        
        triplet_data, triplet_labels = triplet_dataset[0]
        print(f"ä¸‰å…ƒç»„æ•°æ®å½¢çŠ¶: {[x.shape for x in triplet_data]}")
        print(f"ä¸‰å…ƒç»„æ ‡ç­¾: {triplet_labels}")
        
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")
