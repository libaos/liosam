#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è®­ç»ƒå¹³è¡¡çš„è½¨è¿¹è¿›å±•é¢„æµ‹æ¨¡å‹ - ä¸“æ³¨äºæé«˜å¹³å‡å‡†ç¡®ç‡
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import time
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
from models.temporal_models import Temporal3DCNN
from utils.ply_reader import PLYReader
from utils.scan_context import ScanContext
import glob
import warnings
warnings.filterwarnings('ignore')

class BalancedTrajectoryTrainer:
    """å¹³è¡¡çš„è½¨è¿¹è¿›å±•é¢„æµ‹è®­ç»ƒå™¨"""
    
    def __init__(self, sequence_length=5, num_classes=20, learning_rate=0.001):
        self.sequence_length = sequence_length
        self.num_classes = num_classes
        self.learning_rate = learning_rate
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"ğŸ¯ å¹³è¡¡è½¨è¿¹è¿›å±•é¢„æµ‹è®­ç»ƒå™¨")
        print(f"è®¾å¤‡: {self.device}")
        print(f"ç›®æ ‡: æé«˜å¹³å‡å‡†ç¡®ç‡")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = Temporal3DCNN(
            input_shape=(1, sequence_length, 20, 60),
            num_classes=num_classes
        )
        self.model = self.model.to(self.device)
        
        # è®­ç»ƒå†å²
        self.train_losses = []
        self.train_accuracies = []
        self.val_losses = []
        self.val_accuracies = []
        self.class_accuracies = []
        
        print(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    def create_balanced_dataset(self, data_dir, sequence_length=5):
        """åˆ›å»ºå¹³è¡¡çš„è½¨è¿¹è¿›å±•æ•°æ®é›†"""
        print(f"ğŸ“‚ åˆ›å»ºå¹³è¡¡è½¨è¿¹è¿›å±•æ•°æ®é›†...")
        
        # è·å–æ‰€æœ‰plyæ–‡ä»¶å¹¶æŒ‰æ—¶é—´æ’åº
        ply_files = sorted(glob.glob(f"{data_dir}/*.ply"))
        print(f"æ‰¾åˆ° {len(ply_files)} ä¸ªplyæ–‡ä»¶")
        
        if len(ply_files) == 0:
            print("âŒ æœªæ‰¾åˆ°plyæ–‡ä»¶")
            return None, None, None
        
        # ç”ŸæˆScanContextç‰¹å¾
        sc_generator = ScanContext()
        scan_contexts = []
        
        print("ç”ŸæˆScanContextç‰¹å¾...")
        for i, ply_file in enumerate(ply_files):
            if i % 100 == 0:
                print(f"  å¤„ç† {i+1}/{len(ply_files)}")
            
            try:
                points = PLYReader.read_ply_file(ply_file)
                if points is not None and len(points) > 100:
                    points = points[:, :3]  # åªå–x,y,z
                    sc = sc_generator.generate_scan_context(points)
                    scan_contexts.append(sc)
                else:
                    scan_contexts.append(None)
            except Exception as e:
                print(f"å¤„ç†å¤±è´¥ {ply_file}: {e}")
                scan_contexts.append(None)
        
        # åˆ›å»ºå¹³è¡¡çš„æ—¶åºåºåˆ—å’Œæ ‡ç­¾
        sequences = []
        labels = []
        
        print("åˆ›å»ºå¹³è¡¡æ—¶åºåºåˆ—...")
        total_files = len(scan_contexts)
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«åº”è¯¥æœ‰å¤šå°‘æ ·æœ¬
        target_samples_per_class = 100  # æ¯ä¸ªç±»åˆ«ç›®æ ‡æ ·æœ¬æ•°
        
        for class_id in range(self.num_classes):
            class_sequences = []
            
            # è®¡ç®—è¿™ä¸ªç±»åˆ«å¯¹åº”çš„æ–‡ä»¶èŒƒå›´
            start_idx = int((class_id / self.num_classes) * total_files)
            end_idx = int(((class_id + 1) / self.num_classes) * total_files)
            
            # åœ¨è¿™ä¸ªèŒƒå›´å†…åˆ›å»ºåºåˆ—
            for i in range(start_idx, min(end_idx - sequence_length + 1, total_files - sequence_length + 1)):
                sequence_scs = scan_contexts[i:i+sequence_length]
                if all(sc is not None for sc in sequence_scs):
                    sequence = np.stack(sequence_scs, axis=0)
                    class_sequences.append(sequence)
            
            print(f"ç±»åˆ« {class_id}: åŸå§‹æ ·æœ¬æ•° {len(class_sequences)}")
            
            # å¹³è¡¡æ ·æœ¬æ•°é‡
            if len(class_sequences) > 0:
                if len(class_sequences) < target_samples_per_class:
                    # æ•°æ®å¢å¼ºï¼šé‡å¤é‡‡æ ·
                    indices = np.random.choice(len(class_sequences), 
                                             target_samples_per_class, 
                                             replace=True)
                    balanced_sequences = [class_sequences[i] for i in indices]
                else:
                    # éšæœºé‡‡æ ·
                    indices = np.random.choice(len(class_sequences), 
                                             target_samples_per_class, 
                                             replace=False)
                    balanced_sequences = [class_sequences[i] for i in indices]
                
                sequences.extend(balanced_sequences)
                labels.extend([class_id] * len(balanced_sequences))
                
                print(f"ç±»åˆ« {class_id}: å¹³è¡¡åæ ·æœ¬æ•° {len(balanced_sequences)}")
        
        sequences = np.array(sequences)
        labels = np.array(labels)
        
        print(f"åˆ›å»ºäº† {len(sequences)} ä¸ªå¹³è¡¡åºåˆ—")
        print(f"æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(labels)}")
        
        # è®¡ç®—ç±»åˆ«æƒé‡
        class_weights = compute_class_weight('balanced', 
                                           classes=np.unique(labels), 
                                           y=labels)
        class_weights = torch.FloatTensor(class_weights).to(self.device)
        
        return sequences, labels, class_weights
    
    def create_balanced_data_loaders(self, sequences, labels, class_weights, batch_size=16):
        """åˆ›å»ºå¹³è¡¡çš„æ•°æ®åŠ è½½å™¨"""
        print(f"ğŸ”„ åˆ›å»ºå¹³è¡¡æ•°æ®åŠ è½½å™¨...")
        
        # æ•°æ®åˆ’åˆ†
        from sklearn.model_selection import train_test_split
        
        train_seq, temp_seq, train_labels, temp_labels = train_test_split(
            sequences, labels, test_size=0.4, random_state=42, stratify=labels
        )
        
        val_seq, test_seq, val_labels, test_labels = train_test_split(
            temp_seq, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels
        )
        
        print(f"è®­ç»ƒé›†: {len(train_seq)} æ ·æœ¬")
        print(f"éªŒè¯é›†: {len(val_seq)} æ ·æœ¬")
        print(f"æµ‹è¯•é›†: {len(test_seq)} æ ·æœ¬")
        
        # æ‰“å°æ¯ä¸ªé›†åˆçš„ç±»åˆ«åˆ†å¸ƒ
        print("è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ:", np.bincount(train_labels))
        print("éªŒè¯é›†ç±»åˆ«åˆ†å¸ƒ:", np.bincount(val_labels))
        print("æµ‹è¯•é›†ç±»åˆ«åˆ†å¸ƒ:", np.bincount(test_labels))
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        train_tensor = torch.FloatTensor(train_seq).to(self.device)
        val_tensor = torch.FloatTensor(val_seq).to(self.device)
        test_tensor = torch.FloatTensor(test_seq).to(self.device)
        
        train_labels_tensor = torch.LongTensor(train_labels).to(self.device)
        val_labels_tensor = torch.LongTensor(val_labels).to(self.device)
        test_labels_tensor = torch.LongTensor(test_labels).to(self.device)
        
        # åˆ›å»ºåŠ æƒé‡‡æ ·å™¨
        sample_weights = [class_weights[label] for label in train_labels]
        sampler = WeightedRandomSampler(sample_weights, len(sample_weights))
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(train_tensor, train_labels_tensor)
        val_dataset = TensorDataset(val_tensor, val_labels_tensor)
        test_dataset = TensorDataset(test_tensor, test_labels_tensor)
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        return train_loader, val_loader, test_loader
    
    def train_with_class_balance(self, train_loader, val_loader, class_weights, epochs=50):
        """ä½¿ç”¨ç±»åˆ«å¹³è¡¡è®­ç»ƒæ¨¡å‹"""
        print(f"\nğŸ¯ å¼€å§‹å¹³è¡¡è®­ç»ƒ (epochs={epochs})...")
        
        # ä½¿ç”¨åŠ æƒæŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss(weight=class_weights)
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=15, gamma=0.5)
        
        best_avg_acc = 0
        best_model_state = None
        patience = 15
        patience_counter = 0
        
        start_time = time.time()
        
        for epoch in range(epochs):
            epoch_start = time.time()
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader)
            
            # éªŒè¯
            val_loss, val_acc, class_accs = self.validate_with_class_accuracy(val_loader)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()
            
            # è®°å½•å†å²
            self.train_losses.append(train_loss)
            self.train_accuracies.append(train_acc)
            self.val_losses.append(val_loss)
            self.val_accuracies.append(val_acc)
            self.class_accuracies.append(class_accs)
            
            epoch_time = time.time() - epoch_start
            
            # è®¡ç®—å¹³å‡ç±»åˆ«å‡†ç¡®ç‡
            avg_class_acc = np.mean([acc for acc in class_accs.values() if acc > 0])
            
            print(f'\nEpoch {epoch+1:2d}/{epochs:2d} | æ—¶é—´: {epoch_time:.1f}s')
            print(f'è®­ç»ƒ - æŸå¤±: {train_loss:.4f} | å‡†ç¡®ç‡: {train_acc:.1f}%')
            print(f'éªŒè¯ - æŸå¤±: {val_loss:.4f} | å‡†ç¡®ç‡: {val_acc:.1f}%')
            print(f'å¹³å‡ç±»åˆ«å‡†ç¡®ç‡: {avg_class_acc:.1f}%')
            print(f'å­¦ä¹ ç‡: {self.optimizer.param_groups[0]["lr"]:.6f}')
            
            # æ˜¾ç¤ºæœ€å·®çš„å‡ ä¸ªç±»åˆ«
            sorted_classes = sorted(class_accs.items(), key=lambda x: x[1])
            print(f'æœ€å·®ç±»åˆ«: {sorted_classes[:3]}')
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºå¹³å‡ç±»åˆ«å‡†ç¡®ç‡ï¼‰
            if avg_class_acc > best_avg_acc:
                best_avg_acc = avg_class_acc
                best_model_state = self.model.state_dict().copy()
                patience_counter = 0
                print(f'ğŸ‰ æ–°çš„æœ€ä½³å¹³å‡ç±»åˆ«å‡†ç¡®ç‡: {best_avg_acc:.1f}%')
            else:
                patience_counter += 1
            
            # æ—©åœ
            if patience_counter >= patience:
                print(f'\nâ¹ï¸  æ—©åœè§¦å‘ (patience={patience})')
                break
            
            print('-' * 60)
        
        total_time = time.time() - start_time
        print(f'\nâœ… è®­ç»ƒå®Œæˆ!')
        print(f'æ€»è®­ç»ƒæ—¶é—´: {total_time:.1f}ç§’')
        print(f'æœ€ä½³å¹³å‡ç±»åˆ«å‡†ç¡®ç‡: {best_avg_acc:.1f}%')
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if best_model_state is not None:
            self.model.load_state_dict(best_model_state)
            print('âœ… å·²åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡')
        
        return best_avg_acc
    
    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            self.optimizer.zero_grad()
            
            output = self.model(data)
            loss = self.criterion(output, target)
            
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
            
            if batch_idx % 10 == 0:
                print(f'  æ‰¹æ¬¡ {batch_idx:2d}/{len(train_loader):2d} | '
                      f'æŸå¤±: {loss.item():.4f} | '
                      f'å‡†ç¡®ç‡: {100.*correct/total:.1f}%')
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def validate_with_class_accuracy(self, val_loader):
        """éªŒè¯æ¨¡å‹å¹¶è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡"""
        self.model.eval()
        total_loss = 0
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for data, target in val_loader:
                output = self.model(data)
                loss = self.criterion(output, target)
                
                total_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                
                all_predictions.extend(predicted.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
        
        avg_loss = total_loss / len(val_loader)
        overall_accuracy = accuracy_score(all_targets, all_predictions) * 100
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        class_accuracies = {}
        for class_id in range(self.num_classes):
            class_mask = np.array(all_targets) == class_id
            if np.sum(class_mask) > 0:
                class_predictions = np.array(all_predictions)[class_mask]
                class_targets = np.array(all_targets)[class_mask]
                class_acc = accuracy_score(class_targets, class_predictions) * 100
                class_accuracies[class_id] = class_acc
            else:
                class_accuracies[class_id] = 0
        
        return avg_loss, overall_accuracy, class_accuracies
    
    def test_with_detailed_analysis(self, test_loader):
        """æµ‹è¯•æ¨¡å‹å¹¶æä¾›è¯¦ç»†åˆ†æ"""
        print(f"\nğŸ§ª è¯¦ç»†æµ‹è¯•åˆ†æ...")
        
        self.model.eval()
        all_predictions = []
        all_targets = []
        all_confidences = []
        
        with torch.no_grad():
            for data, target in test_loader:
                output = self.model(data)
                probabilities = torch.softmax(output, dim=1)
                confidence, predicted = torch.max(probabilities, 1)
                
                all_predictions.extend(predicted.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
                all_confidences.extend(confidence.cpu().numpy())
        
        # æ€»ä½“å‡†ç¡®ç‡
        overall_acc = accuracy_score(all_targets, all_predictions) * 100
        print(f'æ€»ä½“æµ‹è¯•å‡†ç¡®ç‡: {overall_acc:.1f}%')
        
        # æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†åˆ†æ
        print(f"\nğŸ“Š æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†åˆ†æ:")
        print(f"{'ç±»åˆ«':<4} {'å‡†ç¡®ç‡':<8} {'æ ·æœ¬æ•°':<6} {'å¹³å‡ç½®ä¿¡åº¦':<10}")
        print("-" * 35)
        
        class_accuracies = []
        for class_id in range(self.num_classes):
            class_mask = np.array(all_targets) == class_id
            if np.sum(class_mask) > 0:
                class_predictions = np.array(all_predictions)[class_mask]
                class_targets = np.array(all_targets)[class_mask]
                class_confidences = np.array(all_confidences)[class_mask]
                
                class_acc = accuracy_score(class_targets, class_predictions) * 100
                avg_conf = np.mean(class_confidences) * 100
                sample_count = np.sum(class_mask)
                
                class_accuracies.append(class_acc)
                print(f"{class_id:2d}   {class_acc:6.1f}%   {sample_count:4d}    {avg_conf:6.1f}%")
            else:
                class_accuracies.append(0)
                print(f"{class_id:2d}   {0:6.1f}%   {0:4d}    {0:6.1f}%")
        
        # ç»Ÿè®¡åˆ†æ
        avg_class_acc = np.mean([acc for acc in class_accuracies if acc > 0])
        min_class_acc = min([acc for acc in class_accuracies if acc > 0])
        max_class_acc = max(class_accuracies)
        
        print(f"\nğŸ“ˆ ç»Ÿè®¡åˆ†æ:")
        print(f"å¹³å‡ç±»åˆ«å‡†ç¡®ç‡: {avg_class_acc:.1f}%")
        print(f"æœ€ä½ç±»åˆ«å‡†ç¡®ç‡: {min_class_acc:.1f}%")
        print(f"æœ€é«˜ç±»åˆ«å‡†ç¡®ç‡: {max_class_acc:.1f}%")
        print(f"å‡†ç¡®ç‡æ ‡å‡†å·®: {np.std(class_accuracies):.1f}%")
        
        return overall_acc, avg_class_acc, class_accuracies
    
    def save_model(self, filepath, metadata=None):
        """ä¿å­˜æ¨¡å‹"""
        save_dir = Path(filepath).parent
        save_dir.mkdir(parents=True, exist_ok=True)
        
        save_dict = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'train_losses': self.train_losses,
            'train_accuracies': self.train_accuracies,
            'val_losses': self.val_losses,
            'val_accuracies': self.val_accuracies,
            'class_accuracies': self.class_accuracies,
            'sequence_length': self.sequence_length,
            'num_classes': self.num_classes,
            'learning_rate': self.learning_rate,
        }
        
        if metadata:
            save_dict.update(metadata)
        
        torch.save(save_dict, filepath)
        print(f'âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}')

def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("ğŸ¯ å¹³è¡¡è½¨è¿¹è¿›å±•é¢„æµ‹æ¨¡å‹è®­ç»ƒ")
    print("="*80)
    
    # æ•°æ®è·¯å¾„
    data_dir = "/mysda/shared_dir/2025.7.3/2025-07-03-16-28-57.ply"
    
    # è®­ç»ƒå‚æ•°
    sequence_length = 5
    num_classes = 20
    learning_rate = 0.0005  # é™ä½å­¦ä¹ ç‡
    batch_size = 12  # å‡å°æ‰¹æ¬¡å¤§å°
    epochs = 60  # å¢åŠ è®­ç»ƒè½®æ•°
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = BalancedTrajectoryTrainer(
        sequence_length=sequence_length,
        num_classes=num_classes,
        learning_rate=learning_rate
    )
    
    # åˆ›å»ºå¹³è¡¡æ•°æ®é›†
    sequences, labels, class_weights = trainer.create_balanced_dataset(data_dir, sequence_length)
    
    if sequences is None:
        print("âŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥")
        return
    
    # åˆ›å»ºå¹³è¡¡æ•°æ®åŠ è½½å™¨
    train_loader, val_loader, test_loader = trainer.create_balanced_data_loaders(
        sequences, labels, class_weights, batch_size
    )
    
    # è®­ç»ƒæ¨¡å‹
    best_avg_acc = trainer.train_with_class_balance(train_loader, val_loader, class_weights, epochs)
    
    # æµ‹è¯•æ¨¡å‹
    overall_acc, avg_class_acc, class_accuracies = trainer.test_with_detailed_analysis(test_loader)
    
    # ä¿å­˜æ¨¡å‹
    model_path = f"models/saved/balanced_trajectory_model_avg{avg_class_acc:.1f}.pth"
    metadata = {
        'best_avg_class_acc': best_avg_acc,
        'test_overall_acc': overall_acc,
        'test_avg_class_acc': avg_class_acc,
        'class_accuracies': class_accuracies,
        'batch_size': batch_size,
        'epochs': epochs,
        'data_type': 'balanced_trajectory_progress'
    }
    trainer.save_model(model_path, metadata)
    
    print(f"\nğŸ‰ å¹³è¡¡è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³å¹³å‡ç±»åˆ«å‡†ç¡®ç‡: {best_avg_acc:.1f}%")
    print(f"æµ‹è¯•æ€»ä½“å‡†ç¡®ç‡: {overall_acc:.1f}%")
    print(f"æµ‹è¯•å¹³å‡ç±»åˆ«å‡†ç¡®ç‡: {avg_class_acc:.1f}%")
    print(f"æ¨¡å‹ä¿å­˜è·¯å¾„: {model_path}")

if __name__ == '__main__':
    main()
