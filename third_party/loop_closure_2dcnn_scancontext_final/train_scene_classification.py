#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åŸºäºçœŸå®åœºæ™¯å˜åŒ–çš„åˆ†ç±»æ¨¡å‹è®­ç»ƒ
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from models.cnn_2d_models import Simple2DCNN, Enhanced2DCNN
import pickle
from pathlib import Path
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

class SceneClassificationTrainer:
    """åŸºäºçœŸå®åœºæ™¯å˜åŒ–çš„åˆ†ç±»è®­ç»ƒå™¨"""
    
    def __init__(self, model_type='simple2dcnn', learning_rate=0.001):
        self.model_type = model_type
        self.learning_rate = learning_rate
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"ğŸ¯ çœŸå®åœºæ™¯åˆ†ç±»è®­ç»ƒå™¨")
        print(f"è®¾å¤‡: {self.device}")
        print(f"æ¨¡å‹ç±»å‹: {model_type}")
        print(f"ç›®æ ‡: åŸºäºçœŸå®åœºæ™¯å†…å®¹è¿›è¡Œåˆ†ç±»")
        
    def load_scene_analysis_results(self, results_file='scene_analysis_results.pkl'):
        """åŠ è½½åœºæ™¯åˆ†æç»“æœ"""
        if not Path(results_file).exists():
            print(f"âŒ åœºæ™¯åˆ†æç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {results_file}")
            print("è¯·å…ˆè¿è¡Œ scene_change_detector.py")
            return None
        
        with open(results_file, 'rb') as f:
            results = pickle.load(f)
        
        print(f"âœ… åŠ è½½åœºæ™¯åˆ†æç»“æœ:")
        print(f"  ç‰¹å¾ç»´åº¦: {results['features'].shape}")
        print(f"  èšç±»æ ‡ç­¾: {len(np.unique(results['cluster_labels']))} ä¸ªç±»åˆ«")
        print(f"  åœºæ™¯å˜åŒ–ç‚¹: {len(results['change_points'])} ä¸ª")
        
        return results
    
    def create_realistic_dataset(self, results):
        """åˆ›å»ºåŸºäºçœŸå®åœºæ™¯å˜åŒ–çš„æ•°æ®é›†"""
        features = results['features']
        cluster_labels = results['cluster_labels']
        
        # æå–ScanContextç‰¹å¾ï¼ˆå‰1200ç»´ï¼‰
        scan_contexts = features[:, :1200].reshape(-1, 20, 60)
        
        # ä½¿ç”¨èšç±»æ ‡ç­¾ä½œä¸ºçœŸå®æ ‡ç­¾
        labels = np.array(cluster_labels)
        
        print(f"ğŸ“Š çœŸå®åœºæ™¯æ•°æ®é›†:")
        unique_labels, counts = np.unique(labels, return_counts=True)
        for label, count in zip(unique_labels, counts):
            percentage = count / len(labels) * 100
            print(f"  åœºæ™¯ç±»åˆ« {label}: {count:4d} æ ·æœ¬ ({percentage:5.1f}%)")
        
        num_classes = len(unique_labels)
        
        return scan_contexts, labels, num_classes
    
    def create_balanced_dataset(self, scan_contexts, labels, samples_per_class=500):
        """åˆ›å»ºå¹³è¡¡çš„æ•°æ®é›†"""
        print(f"ğŸ”„ åˆ›å»ºå¹³è¡¡æ•°æ®é›† (æ¯ç±»{samples_per_class}æ ·æœ¬)...")
        
        balanced_contexts = []
        balanced_labels = []
        
        unique_labels = np.unique(labels)
        
        for label in unique_labels:
            label_indices = np.where(labels == label)[0]
            
            if len(label_indices) >= samples_per_class:
                # éšæœºé‡‡æ ·
                selected_indices = np.random.choice(label_indices, samples_per_class, replace=False)
            else:
                # é‡å¤é‡‡æ ·
                selected_indices = np.random.choice(label_indices, samples_per_class, replace=True)
            
            for idx in selected_indices:
                balanced_contexts.append(scan_contexts[idx])
                balanced_labels.append(label)
            
            print(f"  ç±»åˆ« {label}: {len(label_indices)} -> {samples_per_class} æ ·æœ¬")
        
        return np.array(balanced_contexts), np.array(balanced_labels)
    
    def train_scene_classifier(self, scan_contexts, labels, num_classes):
        """è®­ç»ƒåœºæ™¯åˆ†ç±»å™¨"""
        print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒçœŸå®åœºæ™¯åˆ†ç±»å™¨...")
        
        # åˆ›å»ºæ¨¡å‹
        if self.model_type == 'simple2dcnn':
            model = Simple2DCNN(num_classes=num_classes)
        elif self.model_type == 'enhanced2dcnn':
            model = Enhanced2DCNN(num_classes=num_classes)
        else:
            raise ValueError(f"æœªçŸ¥æ¨¡å‹ç±»å‹: {self.model_type}")
        
        model = model.to(self.device)
        
        # æ•°æ®åˆ’åˆ†
        from sklearn.model_selection import train_test_split
        
        train_contexts, temp_contexts, train_labels, temp_labels = train_test_split(
            scan_contexts, labels, test_size=0.4, random_state=42, stratify=labels
        )
        
        val_contexts, test_contexts, val_labels, test_labels = train_test_split(
            temp_contexts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels
        )
        
        print(f"è®­ç»ƒé›†: {len(train_contexts)} æ ·æœ¬")
        print(f"éªŒè¯é›†: {len(val_contexts)} æ ·æœ¬")
        print(f"æµ‹è¯•é›†: {len(test_contexts)} æ ·æœ¬")
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        train_tensor = torch.FloatTensor(train_contexts).unsqueeze(1).to(self.device)
        val_tensor = torch.FloatTensor(val_contexts).unsqueeze(1).to(self.device)
        test_tensor = torch.FloatTensor(test_contexts).unsqueeze(1).to(self.device)
        
        train_labels_tensor = torch.LongTensor(train_labels).to(self.device)
        val_labels_tensor = torch.LongTensor(val_labels).to(self.device)
        test_labels_tensor = torch.LongTensor(test_labels).to(self.device)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(train_tensor, train_labels_tensor)
        val_dataset = TensorDataset(val_tensor, val_labels_tensor)
        test_dataset = TensorDataset(test_tensor, test_labels_tensor)
        
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
        
        # è®­ç»ƒè®¾ç½®
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=self.learning_rate)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)
        
        # è®­ç»ƒå¾ªç¯
        best_val_acc = 0
        best_model_state = None
        patience = 10
        patience_counter = 0
        
        train_losses = []
        val_accuracies = []
        
        epochs = 50
        
        for epoch in range(epochs):
            # è®­ç»ƒ
            model.train()
            total_loss = 0
            
            for batch_idx, (data, target) in enumerate(train_loader):
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            
            avg_train_loss = total_loss / len(train_loader)
            
            # éªŒè¯
            model.eval()
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for data, target in val_loader:
                    output = model(data)
                    _, predicted = torch.max(output.data, 1)
                    val_total += target.size(0)
                    val_correct += (predicted == target).sum().item()
            
            val_acc = 100. * val_correct / val_total
            
            train_losses.append(avg_train_loss)
            val_accuracies.append(val_acc)
            
            print(f'Epoch {epoch+1:2d}/{epochs:2d} | '
                  f'è®­ç»ƒæŸå¤±: {avg_train_loss:.4f} | '
                  f'éªŒè¯å‡†ç¡®ç‡: {val_acc:.1f}%')
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_model_state = model.state_dict().copy()
                patience_counter = 0
                print(f'ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.1f}%')
            else:
                patience_counter += 1
            
            # æ—©åœ
            if patience_counter >= patience:
                print(f'â¹ï¸  æ—©åœè§¦å‘')
                break
            
            scheduler.step()
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if best_model_state is not None:
            model.load_state_dict(best_model_state)
        
        # æµ‹è¯•
        model.eval()
        test_predictions = []
        test_targets = []
        
        with torch.no_grad():
            for data, target in test_loader:
                output = model(data)
                _, predicted = torch.max(output.data, 1)
                test_predictions.extend(predicted.cpu().numpy())
                test_targets.extend(target.cpu().numpy())
        
        test_acc = accuracy_score(test_targets, test_predictions) * 100
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.1f}%")
        print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.1f}%")
        
        # è¯¦ç»†åˆ†æ
        print(f"\nğŸ“Š è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(test_targets, test_predictions))
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(test_targets, test_predictions)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title('Scene Classification Confusion Matrix')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.savefig('scene_classification_confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # ä¿å­˜æ¨¡å‹
        model_path = f"models/saved/scene_classifier_{self.model_type}_acc{test_acc:.1f}.pth"
        Path(model_path).parent.mkdir(parents=True, exist_ok=True)
        
        torch.save({
            'model_state_dict': model.state_dict(),
            'model_type': self.model_type,
            'num_classes': num_classes,
            'test_accuracy': test_acc,
            'best_val_accuracy': best_val_acc,
            'train_losses': train_losses,
            'val_accuracies': val_accuracies
        }, model_path)
        
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
        
        return model, test_acc

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ğŸ¯ åŸºäºçœŸå®åœºæ™¯å˜åŒ–çš„åˆ†ç±»æ¨¡å‹è®­ç»ƒ")
    print("="*60)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SceneClassificationTrainer(model_type='simple2dcnn')
    
    # åŠ è½½åœºæ™¯åˆ†æç»“æœ
    results = trainer.load_scene_analysis_results()
    if results is None:
        return
    
    # åˆ›å»ºçœŸå®åœºæ™¯æ•°æ®é›†
    scan_contexts, labels, num_classes = trainer.create_realistic_dataset(results)
    
    # åˆ›å»ºå¹³è¡¡æ•°æ®é›†
    balanced_contexts, balanced_labels = trainer.create_balanced_dataset(
        scan_contexts, labels, samples_per_class=500
    )
    
    # è®­ç»ƒåˆ†ç±»å™¨
    model, test_acc = trainer.train_scene_classifier(
        balanced_contexts, balanced_labels, num_classes
    )
    
    print(f"\nğŸ‰ çœŸå®åœºæ™¯åˆ†ç±»è®­ç»ƒå®Œæˆ!")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.1f}%")
    print(f"è¿™æ˜¯åŸºäºçœŸå®åœºæ™¯å†…å®¹çš„åˆ†ç±»ç»“æœï¼Œæ¯”ä¹‹å‰çš„è™šå‡99.6%æ›´æœ‰æ„ä¹‰ï¼")

if __name__ == '__main__':
    main()
