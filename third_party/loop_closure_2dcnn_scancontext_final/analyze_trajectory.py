#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åˆ†æè½¨è¿¹æ•°æ®ï¼Œæ£€æŸ¥æ˜¯å¦çœŸçš„æœ‰å›ç¯
"""

import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import pickle
import glob
from collections import defaultdict

def load_gps_data():
    """åŠ è½½GPSè½¨è¿¹æ•°æ®"""
    # æ£€æŸ¥æ˜¯å¦æœ‰GPSæ•°æ®æ–‡ä»¶
    gps_files = glob.glob("data/raw/**/*gps*.txt", recursive=True) + \
                glob.glob("data/raw/**/*GPS*.txt", recursive=True) + \
                glob.glob("data/raw/**/*pose*.txt", recursive=True) + \
                glob.glob("data/raw/**/*trajectory*.txt", recursive=True)
    
    print(f"æ‰¾åˆ°GPSç›¸å…³æ–‡ä»¶: {gps_files}")
    
    if gps_files:
        # å°è¯•è¯»å–ç¬¬ä¸€ä¸ªæ–‡ä»¶
        try:
            gps_data = np.loadtxt(gps_files[0])
            print(f"GPSæ•°æ®å½¢çŠ¶: {gps_data.shape}")
            print(f"GPSæ•°æ®å‰5è¡Œ:\n{gps_data[:5]}")
            return gps_data
        except Exception as e:
            print(f"è¯»å–GPSæ–‡ä»¶å¤±è´¥: {e}")
    
    return None

def analyze_spatial_distribution():
    """åˆ†æç©ºé—´åˆ†å¸ƒï¼Œæ£€æŸ¥æ˜¯å¦æœ‰çœŸæ­£çš„å›ç¯"""
    
    # åŠ è½½åºåˆ—æ•°æ®
    data_file = Path("data/processed/temporal_sequences_len5.pkl")
    if not data_file.exists():
        print("æœªæ‰¾åˆ°åºåˆ—æ•°æ®æ–‡ä»¶")
        return
    
    with open(data_file, 'rb') as f:
        data = pickle.load(f)
    
    sequences = data['sequences']
    labels = data['labels']
    file_paths = data.get('file_paths', [])
    
    print(f"åˆ†æ {len(sequences)} ä¸ªåºåˆ—...")
    
    # åˆ†ææ¯ä¸ªç±»åˆ«çš„åºåˆ—ç‰¹å¾
    class_features = defaultdict(list)
    
    for i, (seq, label) in enumerate(zip(sequences, labels)):
        # è®¡ç®—åºåˆ—çš„ç‰¹å¾å‘é‡ï¼ˆç®€å•çš„ç»Ÿè®¡ç‰¹å¾ï¼‰
        feature_vector = [
            np.mean(seq),           # å‡å€¼
            np.std(seq),            # æ ‡å‡†å·®
            np.max(seq),            # æœ€å¤§å€¼
            np.min(seq),            # æœ€å°å€¼
            np.sum(seq > 0.5),      # é«˜å€¼ç‚¹æ•°é‡
            np.sum(seq < 0.1),      # ä½å€¼ç‚¹æ•°é‡
        ]
        class_features[label].append(feature_vector)
    
    # è®¡ç®—ç±»å†…å’Œç±»é—´ç›¸ä¼¼æ€§
    print("\nç±»åˆ«å†…éƒ¨ç›¸ä¼¼æ€§åˆ†æ:")
    class_similarities = {}
    
    for label, features in class_features.items():
        if len(features) > 1:
            features = np.array(features)
            # è®¡ç®—ç±»å†…å¹³å‡ç›¸å…³æ€§
            correlations = []
            for i in range(len(features)):
                for j in range(i+1, len(features)):
                    corr = np.corrcoef(features[i], features[j])[0, 1]
                    if not np.isnan(corr):
                        correlations.append(corr)
            
            if correlations:
                avg_corr = np.mean(correlations)
                class_similarities[label] = avg_corr
                print(f"  ç±»åˆ« {label:2d}: å¹³å‡ç›¸å…³æ€§ = {avg_corr:.4f} ({len(features)} ä¸ªæ ·æœ¬)")
    
    # åˆ†æç±»é—´ç›¸ä¼¼æ€§
    print("\nç±»é—´ç›¸ä¼¼æ€§åˆ†æ (æ£€æŸ¥æ˜¯å¦æœ‰çœŸæ­£çš„å›ç¯):")
    
    # è®¡ç®—æ‰€æœ‰ç±»åˆ«çš„å¹³å‡ç‰¹å¾
    class_centroids = {}
    for label, features in class_features.items():
        class_centroids[label] = np.mean(features, axis=0)
    
    # æ‰¾å‡ºæœ€ç›¸ä¼¼çš„ç±»åˆ«å¯¹
    similar_pairs = []
    labels_list = list(class_centroids.keys())
    
    for i in range(len(labels_list)):
        for j in range(i+1, len(labels_list)):
            label1, label2 = labels_list[i], labels_list[j]
            centroid1, centroid2 = class_centroids[label1], class_centroids[label2]
            
            similarity = np.corrcoef(centroid1, centroid2)[0, 1]
            if not np.isnan(similarity):
                similar_pairs.append((label1, label2, similarity))
    
    # æ’åºå¹¶æ˜¾ç¤ºæœ€ç›¸ä¼¼çš„ç±»åˆ«å¯¹
    similar_pairs.sort(key=lambda x: x[2], reverse=True)
    
    print("æœ€ç›¸ä¼¼çš„ç±»åˆ«å¯¹ (å¯èƒ½çš„å›ç¯ä½ç½®):")
    for i, (label1, label2, sim) in enumerate(similar_pairs[:10]):
        print(f"  {i+1:2d}. ç±»åˆ« {label1:2d} â†” ç±»åˆ« {label2:2d}: ç›¸ä¼¼åº¦ = {sim:.4f}")
        
        # å¦‚æœç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œå¯èƒ½æ˜¯çœŸæ­£çš„å›ç¯
        if sim > 0.8:
            print(f"      â­ å¯èƒ½çš„å›ç¯: ç±»åˆ«{label1} å’Œ ç±»åˆ«{label2}")
    
    # æ£€æŸ¥æ—¶é—´ä¸Šçš„åˆ†å¸ƒ
    print(f"\næ—¶é—´åˆ†å¸ƒåˆ†æ:")
    print(f"ç±»åˆ«æŒ‰æ—¶é—´é¡ºåº: {sorted(set(labels))}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è·³è·ƒå¼çš„ç›¸ä¼¼æ€§ï¼ˆçœŸæ­£çš„å›ç¯ç‰¹å¾ï¼‰
    high_similarity_pairs = [pair for pair in similar_pairs if pair[2] > 0.7]
    
    if high_similarity_pairs:
        print(f"\nå‘ç° {len(high_similarity_pairs)} å¯¹é«˜ç›¸ä¼¼æ€§ç±»åˆ«:")
        for label1, label2, sim in high_similarity_pairs:
            time_gap = abs(label1 - label2)
            print(f"  ç±»åˆ« {label1} â†” ç±»åˆ« {label2}: ç›¸ä¼¼åº¦={sim:.4f}, æ—¶é—´é—´éš”={time_gap}")
            
            if time_gap > 5:  # æ—¶é—´é—´éš”è¾ƒå¤§ä½†ç›¸ä¼¼åº¦é«˜ï¼Œå¯èƒ½æ˜¯çœŸå›ç¯
                print(f"    ğŸ”„ ç–‘ä¼¼çœŸæ­£å›ç¯: æ—¶é—´é—´éš”{time_gap}æ®µä½†é«˜åº¦ç›¸ä¼¼")
    
    return class_features, similar_pairs

def visualize_sequence_similarity():
    """å¯è§†åŒ–åºåˆ—ç›¸ä¼¼æ€§çŸ©é˜µ"""
    
    data_file = Path("data/processed/temporal_sequences_len5.pkl")
    if not data_file.exists():
        return
    
    with open(data_file, 'rb') as f:
        data = pickle.load(f)
    
    sequences = data['sequences']
    labels = data['labels']
    
    # éšæœºé€‰æ‹©ä¸€äº›åºåˆ—è®¡ç®—ç›¸ä¼¼æ€§çŸ©é˜µ
    n_samples = min(50, len(sequences))
    indices = np.random.choice(len(sequences), n_samples, replace=False)
    
    selected_sequences = [sequences[i] for i in indices]
    selected_labels = [labels[i] for i in indices]
    
    # è®¡ç®—ç›¸ä¼¼æ€§çŸ©é˜µ
    similarity_matrix = np.zeros((n_samples, n_samples))
    
    for i in range(n_samples):
        for j in range(n_samples):
            seq1 = selected_sequences[i].flatten()
            seq2 = selected_sequences[j].flatten()
            similarity = np.corrcoef(seq1, seq2)[0, 1]
            if not np.isnan(similarity):
                similarity_matrix[i, j] = similarity
    
    # ç»˜åˆ¶ç›¸ä¼¼æ€§çŸ©é˜µ
    plt.figure(figsize=(12, 10))
    plt.imshow(similarity_matrix, cmap='viridis', aspect='auto')
    plt.colorbar(label='ç›¸å…³ç³»æ•°')
    plt.title('åºåˆ—ç›¸ä¼¼æ€§çŸ©é˜µ')
    plt.xlabel('åºåˆ—ç´¢å¼•')
    plt.ylabel('åºåˆ—ç´¢å¼•')
    
    # æ·»åŠ æ ‡ç­¾ä¿¡æ¯
    for i in range(0, n_samples, 5):
        plt.axhline(y=i-0.5, color='red', alpha=0.3, linewidth=0.5)
        plt.axvline(x=i-0.5, color='red', alpha=0.3, linewidth=0.5)
    
    plt.tight_layout()
    plt.savefig('sequence_similarity_matrix.png', dpi=150, bbox_inches='tight')
    print("ç›¸ä¼¼æ€§çŸ©é˜µå·²ä¿å­˜ä¸º sequence_similarity_matrix.png")
    
    # åˆ†æå¯¹è§’çº¿å¤–çš„é«˜ç›¸ä¼¼æ€§
    high_sim_pairs = []
    for i in range(n_samples):
        for j in range(i+1, n_samples):
            if similarity_matrix[i, j] > 0.8:
                label_diff = abs(selected_labels[i] - selected_labels[j])
                high_sim_pairs.append((i, j, similarity_matrix[i, j], 
                                     selected_labels[i], selected_labels[j], label_diff))
    
    if high_sim_pairs:
        print(f"\nå‘ç° {len(high_sim_pairs)} å¯¹é«˜ç›¸ä¼¼æ€§åºåˆ—:")
        for i, j, sim, label1, label2, label_diff in high_sim_pairs:
            print(f"  åºåˆ—{i}(ç±»åˆ«{label1}) â†” åºåˆ—{j}(ç±»åˆ«{label2}): ç›¸ä¼¼åº¦={sim:.4f}, æ ‡ç­¾å·®={label_diff}")

def check_file_timestamps():
    """æ£€æŸ¥æ–‡ä»¶æ—¶é—´æˆ³ï¼Œåˆ†æè½¨è¿¹çš„æ—¶é—´ç‰¹æ€§"""
    
    data_file = Path("data/processed/temporal_sequences_len5.pkl")
    if not data_file.exists():
        return
    
    with open(data_file, 'rb') as f:
        data = pickle.load(f)
    
    file_paths = data.get('file_paths', [])
    labels = data['labels']
    
    if not file_paths:
        print("æ²¡æœ‰æ–‡ä»¶è·¯å¾„ä¿¡æ¯")
        return
    
    print("æ–‡ä»¶è·¯å¾„åˆ†æ:")
    print(f"æ€»å…± {len(file_paths)} ä¸ªåºåˆ—")
    
    # åˆ†ææ¯ä¸ªç±»åˆ«å¯¹åº”çš„æ–‡ä»¶
    class_files = defaultdict(list)
    for i, (paths, label) in enumerate(zip(file_paths, labels)):
        if isinstance(paths, list) and len(paths) > 0:
            # æå–æ–‡ä»¶åä¸­çš„æ•°å­—ï¼ˆé€šå¸¸æ˜¯æ—¶é—´æˆ³æˆ–å¸§å·ï¼‰
            first_file = Path(paths[0]).name
            class_files[label].append(first_file)
    
    print("\næ¯ä¸ªç±»åˆ«çš„æ–‡ä»¶åˆ†å¸ƒ:")
    for label in sorted(class_files.keys()):
        files = class_files[label]
        print(f"ç±»åˆ« {label:2d}: {len(files)} ä¸ªæ–‡ä»¶")
        if len(files) <= 3:
            for f in files:
                print(f"    {f}")
        else:
            print(f"    {files[0]} ... {files[-1]}")

if __name__ == '__main__':
    print("="*60)
    print("è½¨è¿¹å›ç¯åˆ†æ")
    print("="*60)
    
    # 1. å°è¯•åŠ è½½GPSæ•°æ®
    gps_data = load_gps_data()
    
    # 2. åˆ†æç©ºé—´åˆ†å¸ƒ
    class_features, similar_pairs = analyze_spatial_distribution()
    
    # 3. å¯è§†åŒ–ç›¸ä¼¼æ€§
    visualize_sequence_similarity()
    
    # 4. æ£€æŸ¥æ–‡ä»¶æ—¶é—´æˆ³
    check_file_timestamps()
    
    print("\n" + "="*60)
    print("åˆ†ææ€»ç»“")
    print("="*60)
    print("1. å¦‚æœå‘ç°é«˜ç›¸ä¼¼æ€§ä½†æ—¶é—´é—´éš”å¤§çš„ç±»åˆ«å¯¹ï¼Œè¯´æ˜ç¡®å®å­˜åœ¨å›ç¯")
    print("2. å¦‚æœåªæœ‰æ—¶é—´ç›¸é‚»çš„ç±»åˆ«ç›¸ä¼¼ï¼Œè¯´æ˜ä¸»è¦æ˜¯æ—¶åºè¿ç»­æ€§")
    print("3. éœ€è¦æ£€æŸ¥GPSè½¨è¿¹æ•°æ®æ¥ç¡®è®¤ç©ºé—´ä¸Šçš„å›ç¯æ¨¡å¼")
