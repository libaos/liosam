#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åŸºäº3D CNNçš„è½¨è¿¹å®šä½ç³»ç»Ÿ
ä½¿ç”¨3Dä½“ç´ åŒ–ç‚¹äº‘è¿›è¡Œä½ç½®è¯†åˆ«
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from models.cnn_3d_models import Simple3DCNN, Enhanced3DCNN, ResNet3D, PointCloudVoxelizer
from utils.ply_reader import PLYReader
import glob
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import pickle
import time

class TrajectoryLocalization3D:
    """åŸºäº3D CNNçš„è½¨è¿¹å®šä½ç³»ç»Ÿ"""
    
    def __init__(self, num_locations=20, model_type='simple3dcnn', voxel_size=(32, 32, 32)):
        self.num_locations = num_locations
        self.model_type = model_type
        self.voxel_size = voxel_size
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆå§‹åŒ–ä½“ç´ åŒ–å™¨
        self.voxelizer = PointCloudVoxelizer(
            voxel_size=voxel_size,
            point_cloud_range=[[-25, 25], [-25, 25], [-5, 5]]  # 50x50x10ç±³èŒƒå›´
        )
        
        print(f"ğŸ¯ åŸºäº3D CNNçš„è½¨è¿¹å®šä½ç³»ç»Ÿ")
        print(f"è®¾å¤‡: {self.device}")
        print(f"ç›®æ ‡ä½ç½®æ•°: {num_locations}")
        print(f"æ¨¡å‹ç±»å‹: {model_type}")
        print(f"ä½“ç´ å°ºå¯¸: {voxel_size}")
        print(f"ç›®æ ‡: åŸºäº3Dä½“ç´ åŒ–ç‰¹å¾è¿›è¡Œç²¾ç¡®è½¨è¿¹å®šä½")
        
        # åˆå§‹åŒ–æ¨¡å‹
        if model_type == 'simple3dcnn':
            self.model = Simple3DCNN(num_classes=num_locations, input_size=voxel_size)
        elif model_type == 'enhanced3dcnn':
            self.model = Enhanced3DCNN(num_classes=num_locations, input_size=voxel_size)
        elif model_type == 'resnet3d':
            self.model = ResNet3D(num_classes=num_locations, input_size=voxel_size)
        else:
            raise ValueError(f"æœªçŸ¥æ¨¡å‹ç±»å‹: {model_type}")
        
        self.model = self.model.to(self.device)
        
        # ä½ç½®ä¿¡æ¯å­˜å‚¨
        self.location_database = {}
        self.location_features = []
        self.location_labels = []
        
        # ä¼˜åŒ–å‚æ•°
        self.confidence_threshold = 0.7
        self.temporal_smoothing = True
        self.location_history = []
        self.confidence_history = []
        
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
    
    def create_3d_location_database(self, data_dir, save_path='location_database_3d.pkl'):
        """åˆ›å»ºåŸºäº3Dä½“ç´ çš„ä½ç½®æ•°æ®åº“"""
        print(f"ğŸ“ åˆ›å»º3Dä½“ç´ ä½ç½®æ•°æ®åº“...")
        
        # è·å–æ‰€æœ‰plyæ–‡ä»¶
        ply_files = sorted(glob.glob(f"{data_dir}/*.ply"))
        print(f"æ‰¾åˆ° {len(ply_files)} ä¸ªplyæ–‡ä»¶")
        
        if len(ply_files) == 0:
            print("âŒ æœªæ‰¾åˆ°plyæ–‡ä»¶")
            return False
        
        # è®¡ç®—æ¯ä¸ªä½ç½®æ®µçš„æ–‡ä»¶èŒƒå›´
        files_per_location = len(ply_files) // self.num_locations
        print(f"æ¯ä¸ªä½ç½®æ®µåŒ…å«çº¦ {files_per_location} ä¸ªæ–‡ä»¶")
        
        location_data = {}
        all_features = []
        all_labels = []
        
        for location_id in range(self.num_locations):
            print(f"  å¤„ç†ä½ç½® {location_id+1}/{self.num_locations}")
            
            # ç¡®å®šè¿™ä¸ªä½ç½®çš„æ–‡ä»¶èŒƒå›´
            start_idx = location_id * files_per_location
            if location_id == self.num_locations - 1:
                end_idx = len(ply_files)
            else:
                end_idx = (location_id + 1) * files_per_location
            
            location_files = ply_files[start_idx:end_idx]
            location_features = []
            
            # å¤„ç†è¿™ä¸ªä½ç½®çš„æ‰€æœ‰æ–‡ä»¶
            for ply_file in location_files:
                try:
                    points = PLYReader.read_ply_file(ply_file)
                    if points is not None and len(points) > 100:
                        points = points[:, :3]  # åªä½¿ç”¨x,y,zåæ ‡
                        
                        # ä½“ç´ åŒ–
                        voxel_grid = self.voxelizer.voxelize(points)
                        
                        if voxel_grid is not None:
                            location_features.append(voxel_grid)
                            all_features.append(voxel_grid)
                            all_labels.append(location_id)
                            
                except Exception as e:
                    print(f"    å¤„ç†å¤±è´¥ {ply_file}: {e}")
                    continue
            
            if len(location_features) > 0:
                # è®¡ç®—è¿™ä¸ªä½ç½®çš„ä»£è¡¨æ€§ç‰¹å¾ï¼ˆå¹³å‡å€¼ï¼‰
                representative_voxel = np.mean(location_features, axis=0)
                location_data[location_id] = {
                    'representative_voxel': representative_voxel,
                    'sample_count': len(location_features),
                    'file_range': (start_idx, end_idx),
                    'all_features': location_features
                }
                print(f"    ä½ç½® {location_id}: {len(location_features)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
            else:
                print(f"    âš ï¸  ä½ç½® {location_id}: æ— æœ‰æ•ˆæ ·æœ¬")
        
        self.location_database = location_data
        self.location_features = np.array(all_features)
        self.location_labels = np.array(all_labels)
        
        # ä¿å­˜ä½ç½®æ•°æ®åº“
        with open(save_path, 'wb') as f:
            pickle.dump({
                'location_database': self.location_database,
                'location_features': self.location_features,
                'location_labels': self.location_labels,
                'num_locations': self.num_locations,
                'voxel_size': self.voxel_size
            }, f)
        
        print(f"âœ… 3Dä½ç½®æ•°æ®åº“å·²ä¿å­˜åˆ°: {save_path}")
        print(f"æ€»æ ·æœ¬æ•°: {len(all_features)}")
        print(f"ä½“ç´ ç‰¹å¾å½¢çŠ¶: {self.location_features.shape}")
        
        return True
    
    def load_3d_location_database(self, load_path='location_database_3d.pkl'):
        """åŠ è½½3Dä½ç½®æ•°æ®åº“"""
        if not Path(load_path).exists():
            print(f"âŒ 3Dä½ç½®æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {load_path}")
            return False
        
        with open(load_path, 'rb') as f:
            data = pickle.load(f)
        
        self.location_database = data['location_database']
        self.location_features = data['location_features']
        self.location_labels = data['location_labels']
        self.num_locations = data['num_locations']
        
        print(f"âœ… å·²åŠ è½½3Dä½ç½®æ•°æ®åº“")
        print(f"ä½ç½®æ•°é‡: {self.num_locations}")
        print(f"æ€»æ ·æœ¬æ•°: {len(self.location_features)}")
        print(f"ä½“ç´ ç‰¹å¾å½¢çŠ¶: {self.location_features.shape}")
        
        return True
    
    def train_3d_localization_model(self, epochs=50, batch_size=16):
        """è®­ç»ƒ3Då®šä½æ¨¡å‹"""
        print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ3Dè½¨è¿¹å®šä½æ¨¡å‹...")
        
        if len(self.location_features) == 0:
            print("âŒ æ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œè¯·å…ˆåˆ›å»ºä½ç½®æ•°æ®åº“")
            return False
        
        # æ•°æ®åˆ’åˆ†
        print("ğŸ” æ£€æŸ¥æ•°æ®åˆ†å¸ƒ...")
        unique_labels, counts = np.unique(self.location_labels, return_counts=True)
        min_samples = np.min(counts)
        
        if min_samples < 3:
            print(f"âš ï¸  æ£€æµ‹åˆ°æ ·æœ¬ä¸è¶³çš„ç±»åˆ« (æœ€å°‘{min_samples}ä¸ªæ ·æœ¬)")
            print("ä½¿ç”¨éšæœºåˆ’åˆ†è€Œä¸æ˜¯åˆ†å±‚åˆ’åˆ†")
            
            X_train, X_temp, y_train, y_temp = train_test_split(
                self.location_features, self.location_labels, 
                test_size=0.4, random_state=42
            )
            
            X_val, X_test, y_val, y_test = train_test_split(
                X_temp, y_temp, test_size=0.5, random_state=42
            )
        else:
            print("âœ… æ‰€æœ‰ç±»åˆ«æ ·æœ¬å……è¶³ï¼Œä½¿ç”¨åˆ†å±‚åˆ’åˆ†")
            X_train, X_temp, y_train, y_temp = train_test_split(
                self.location_features, self.location_labels, 
                test_size=0.4, random_state=42, stratify=self.location_labels
            )
            
            X_val, X_test, y_val, y_test = train_test_split(
                X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
            )
        
        print(f"è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
        print(f"éªŒè¯é›†: {len(X_val)} æ ·æœ¬")
        print(f"æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡ (æ·»åŠ é€šé“ç»´åº¦)
        X_train_tensor = torch.FloatTensor(X_train).unsqueeze(1).to(self.device)
        X_val_tensor = torch.FloatTensor(X_val).unsqueeze(1).to(self.device)
        X_test_tensor = torch.FloatTensor(X_test).unsqueeze(1).to(self.device)
        
        y_train_tensor = torch.LongTensor(y_train).to(self.device)
        y_val_tensor = torch.LongTensor(y_val).to(self.device)
        y_test_tensor = torch.LongTensor(y_test).to(self.device)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        # è®­ç»ƒè®¾ç½®
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)
        
        # è®­ç»ƒå¾ªç¯
        best_val_acc = 0
        best_model_state = None
        patience = 10
        patience_counter = 0
        
        train_losses = []
        val_accuracies = []
        
        print(f"\nå¼€å§‹è®­ç»ƒ (æ‰¹æ¬¡å¤§å°: {batch_size})...")
        
        for epoch in range(epochs):
            # è®­ç»ƒ
            self.model.train()
            total_loss = 0
            
            for batch_idx, (data, target) in enumerate(train_loader):
                optimizer.zero_grad()
                output = self.model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            
            avg_train_loss = total_loss / len(train_loader)
            
            # éªŒè¯
            self.model.eval()
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for data, target in val_loader:
                    output = self.model(data)
                    _, predicted = torch.max(output.data, 1)
                    val_total += target.size(0)
                    val_correct += (predicted == target).sum().item()
            
            val_acc = 100. * val_correct / val_total
            
            train_losses.append(avg_train_loss)
            val_accuracies.append(val_acc)
            
            print(f'Epoch {epoch+1:2d}/{epochs:2d} | '
                  f'è®­ç»ƒæŸå¤±: {avg_train_loss:.4f} | '
                  f'éªŒè¯å‡†ç¡®ç‡: {val_acc:.1f}%')
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_model_state = self.model.state_dict().copy()
                patience_counter = 0
                print(f'ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.1f}%')
            else:
                patience_counter += 1
            
            # æ—©åœ
            if patience_counter >= patience:
                print(f'â¹ï¸  æ—©åœè§¦å‘')
                break
            
            scheduler.step()
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if best_model_state is not None:
            self.model.load_state_dict(best_model_state)
        
        # æµ‹è¯•
        self.model.eval()
        test_predictions = []
        test_targets = []
        
        with torch.no_grad():
            for data, target in test_loader:
                output = self.model(data)
                _, predicted = torch.max(output.data, 1)
                test_predictions.extend(predicted.cpu().numpy())
                test_targets.extend(target.cpu().numpy())
        
        test_acc = accuracy_score(test_targets, test_predictions) * 100
        
        print(f"\nâœ… 3D CNNè®­ç»ƒå®Œæˆ!")
        print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.1f}%")
        print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.1f}%")
        
        # è¯¦ç»†åˆ†æ
        print(f"\nğŸ“Š è¯¦ç»†3Då®šä½æ€§èƒ½åˆ†æ:")
        
        # è®¡ç®—æ¯ä¸ªä½ç½®çš„å‡†ç¡®ç‡
        location_accuracies = {}
        for location_id in range(self.num_locations):
            location_mask = np.array(test_targets) == location_id
            if np.sum(location_mask) > 0:
                location_predictions = np.array(test_predictions)[location_mask]
                location_targets = np.array(test_targets)[location_mask]
                location_acc = accuracy_score(location_targets, location_predictions) * 100
                location_accuracies[location_id] = location_acc
                sample_count = np.sum(location_mask)
                print(f"  ä½ç½® {location_id:2d}: {location_acc:6.1f}% ({sample_count:2d} æ ·æœ¬)")
        
        avg_location_acc = np.mean(list(location_accuracies.values()))
        print(f"\nå¹³å‡ä½ç½®å‡†ç¡®ç‡: {avg_location_acc:.1f}%")
        
        # ä¿å­˜æ¨¡å‹
        model_path = f"models/saved/trajectory_localizer_3d_{self.model_type}_acc{test_acc:.1f}.pth"
        Path(model_path).parent.mkdir(parents=True, exist_ok=True)
        
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_type': self.model_type,
            'num_locations': self.num_locations,
            'voxel_size': self.voxel_size,
            'test_accuracy': test_acc,
            'best_val_accuracy': best_val_acc,
            'location_accuracies': location_accuracies,
            'train_losses': train_losses,
            'val_accuracies': val_accuracies
        }, model_path)
        
        print(f"âœ… 3Då®šä½æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
        
        return test_acc

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ğŸ¯ åŸºäº3D CNNçš„è½¨è¿¹å®šä½ç³»ç»Ÿ")
    print("="*60)
    
    # æ•°æ®è·¯å¾„
    data_dir = "/mysda/shared_dir/2025.7.3/2025-07-03-16-28-57.ply"
    
    # åˆ›å»º3Då®šä½ç³»ç»Ÿ
    localizer = TrajectoryLocalization3D(
        num_locations=20, 
        model_type='simple3dcnn',
        voxel_size=(32, 32, 32)
    )
    
    # 1. åˆ›å»º3Dä½ç½®æ•°æ®åº“
    print("\næ­¥éª¤1: åˆ›å»º3Dä½“ç´ ä½ç½®æ•°æ®åº“")
    success = localizer.create_3d_location_database(data_dir)
    
    if not success:
        print("âŒ 3Dä½ç½®æ•°æ®åº“åˆ›å»ºå¤±è´¥")
        return
    
    # 2. è®­ç»ƒ3Då®šä½æ¨¡å‹
    print("\næ­¥éª¤2: è®­ç»ƒ3Då®šä½æ¨¡å‹")
    test_acc = localizer.train_3d_localization_model(epochs=30, batch_size=8)
    
    print(f"\nğŸ‰ 3Dè½¨è¿¹å®šä½ç³»ç»Ÿè®­ç»ƒå®Œæˆ!")
    print(f"å®šä½å‡†ç¡®ç‡: {test_acc:.1f}%")
    print(f"ç³»ç»Ÿå¯ä»¥è¯†åˆ«è½¨è¿¹ä¸­çš„ {localizer.num_locations} ä¸ªä¸åŒä½ç½®")
    print(f"âœ¨ 3D CNNç‰¹æ€§:")
    print(f"  - 3Dä½“ç´ åŒ–ç‚¹äº‘è¡¨ç¤º")
    print(f"  - ç©ºé—´å‡ ä½•ç‰¹å¾å­¦ä¹ ")
    print(f"  - ç«¯åˆ°ç«¯3Dç‰¹å¾æå–")
    print(f"ä¸‹æ¬¡æœºå™¨äººæ¥åˆ°ç›¸åŒåŒºåŸŸæ—¶ï¼Œå¯ä»¥åŸºäº3Dç©ºé—´ç‰¹å¾è¿›è¡Œå®šä½ï¼")

if __name__ == '__main__':
    main()
