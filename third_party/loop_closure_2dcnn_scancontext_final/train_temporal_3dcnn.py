#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è®­ç»ƒTemporal 3D CNNæ¨¡å‹ç”¨äºå›ç¯æ£€æµ‹
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pickle
import matplotlib.pyplot as plt
from pathlib import Path
import time
import json
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from models.temporal_models import Temporal3DCNN
import warnings
warnings.filterwarnings('ignore')

class Temporal3DCNNTrainer:
    """Temporal 3D CNNè®­ç»ƒå™¨"""
    
    def __init__(self, sequence_length=5, num_classes=20, learning_rate=0.001):
        self.sequence_length = sequence_length
        self.num_classes = num_classes
        self.learning_rate = learning_rate
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"ğŸš€ åˆå§‹åŒ–Temporal 3D CNNè®­ç»ƒå™¨")
        print(f"è®¾å¤‡: {self.device}")
        print(f"åºåˆ—é•¿åº¦: {sequence_length}")
        print(f"ç±»åˆ«æ•°: {num_classes}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = Temporal3DCNN(
            input_shape=(1, sequence_length, 20, 60),
            num_classes=num_classes
        )
        self.model = self.model.to(self.device)
        
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.1)
        
        # è®­ç»ƒå†å²
        self.train_losses = []
        self.train_accuracies = []
        self.val_losses = []
        self.val_accuracies = []
        
        print(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
    
    def load_data(self, sequence_length=5):
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        print(f"\nğŸ“‚ åŠ è½½åºåˆ—é•¿åº¦ä¸º{sequence_length}çš„æ•°æ®...")
        
        data_file = Path(f"data/processed/temporal_sequences_len{sequence_length}.pkl")
        if not data_file.exists():
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
            return None, None, None, None, None, None
        
        with open(data_file, 'rb') as f:
            data = pickle.load(f)
        
        # æå–æ•°æ®
        sequences = np.array(data['sequences'])
        labels = np.array(data['labels'])
        
        print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {sequences.shape}")
        print(f"æ ‡ç­¾æ•°é‡: {len(labels)}")
        print(f"ç±»åˆ«åˆ†å¸ƒ: {np.bincount(labels)}")
        
        # æ•°æ®åˆ’åˆ†
        from sklearn.model_selection import train_test_split
        
        # å…ˆåˆ’åˆ†è®­ç»ƒé›†å’Œä¸´æ—¶é›†
        train_sequences, temp_sequences, train_labels, temp_labels = train_test_split(
            sequences, labels, test_size=0.4, random_state=42, stratify=labels
        )
        
        # å†å°†ä¸´æ—¶é›†åˆ’åˆ†ä¸ºéªŒè¯é›†å’Œæµ‹è¯•é›†
        val_sequences, test_sequences, val_labels, test_labels = train_test_split(
            temp_sequences, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels
        )
        
        print(f"\nğŸ“Š æ•°æ®åˆ’åˆ†:")
        print(f"è®­ç»ƒé›†: {len(train_sequences)} æ ·æœ¬")
        print(f"éªŒè¯é›†: {len(val_sequences)} æ ·æœ¬")
        print(f"æµ‹è¯•é›†: {len(test_sequences)} æ ·æœ¬")
        
        return train_sequences, val_sequences, test_sequences, train_labels, val_labels, test_labels
    
    def create_data_loaders(self, train_sequences, val_sequences, test_sequences, 
                           train_labels, val_labels, test_labels, batch_size=32):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        print(f"\nğŸ”„ åˆ›å»ºæ•°æ®åŠ è½½å™¨ (batch_size={batch_size})...")
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        train_tensor = torch.FloatTensor(train_sequences).to(self.device)
        val_tensor = torch.FloatTensor(val_sequences).to(self.device)
        test_tensor = torch.FloatTensor(test_sequences).to(self.device)
        
        train_labels_tensor = torch.LongTensor(train_labels).to(self.device)
        val_labels_tensor = torch.LongTensor(val_labels).to(self.device)
        test_labels_tensor = torch.LongTensor(test_labels).to(self.device)
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = TensorDataset(train_tensor, train_labels_tensor)
        val_dataset = TensorDataset(val_tensor, val_labels_tensor)
        test_dataset = TensorDataset(test_tensor, test_labels_tensor)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
        print(f"è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
        print(f"éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
        print(f"æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)}")
        
        return train_loader, val_loader, test_loader
    
    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            self.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            output = self.model(data)
            loss = self.criterion(output, target)
            
            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
            
            if batch_idx % 10 == 0:
                print(f'  æ‰¹æ¬¡ {batch_idx:3d}/{len(train_loader):3d} | '
                      f'æŸå¤±: {loss.item():.4f} | '
                      f'å‡†ç¡®ç‡: {100.*correct/total:.2f}%')
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def validate(self, val_loader):
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                output = self.model(data)
                loss = self.criterion(output, target)
                
                total_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                total += target.size(0)
                correct += (predicted == target).sum().item()
        
        avg_loss = total_loss / len(val_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def train(self, train_loader, val_loader, epochs=100, save_best=True):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ (epochs={epochs})...")
        
        best_val_acc = 0
        best_model_state = None
        patience = 15
        patience_counter = 0
        
        start_time = time.time()
        
        for epoch in range(epochs):
            epoch_start = time.time()
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader)
            
            # éªŒè¯
            val_loss, val_acc = self.validate(val_loader)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()
            
            # è®°å½•å†å²
            self.train_losses.append(train_loss)
            self.train_accuracies.append(train_acc)
            self.val_losses.append(val_loss)
            self.val_accuracies.append(val_acc)
            
            epoch_time = time.time() - epoch_start
            
            print(f'\nEpoch {epoch+1:3d}/{epochs:3d} | æ—¶é—´: {epoch_time:.1f}s')
            print(f'è®­ç»ƒ - æŸå¤±: {train_loss:.4f} | å‡†ç¡®ç‡: {train_acc:.2f}%')
            print(f'éªŒè¯ - æŸå¤±: {val_loss:.4f} | å‡†ç¡®ç‡: {val_acc:.2f}%')
            print(f'å­¦ä¹ ç‡: {self.optimizer.param_groups[0]["lr"]:.6f}')
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_model_state = self.model.state_dict().copy()
                patience_counter = 0
                print(f'ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%')
            else:
                patience_counter += 1
            
            # æ—©åœ
            if patience_counter >= patience:
                print(f'\nâ¹ï¸  æ—©åœè§¦å‘ (patience={patience})')
                break
            
            print('-' * 60)
        
        total_time = time.time() - start_time
        print(f'\nâœ… è®­ç»ƒå®Œæˆ!')
        print(f'æ€»è®­ç»ƒæ—¶é—´: {total_time:.1f}ç§’')
        print(f'æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%')
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if best_model_state is not None:
            self.model.load_state_dict(best_model_state)
            print('âœ… å·²åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡')
        
        return best_val_acc
    
    def test(self, test_loader):
        """æµ‹è¯•æ¨¡å‹"""
        print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹...")
        
        self.model.eval()
        all_predictions = []
        all_targets = []
        total_loss = 0
        
        with torch.no_grad():
            for data, target in test_loader:
                output = self.model(data)
                loss = self.criterion(output, target)
                
                total_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                
                all_predictions.extend(predicted.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        test_loss = total_loss / len(test_loader)
        test_acc = accuracy_score(all_targets, all_predictions) * 100
        
        print(f'æµ‹è¯•æŸå¤±: {test_loss:.4f}')
        print(f'æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%')
        
        # è¯¦ç»†æŠ¥å‘Š
        print(f"\nğŸ“Š åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(all_targets, all_predictions))
        
        return test_acc, all_predictions, all_targets
    
    def save_model(self, filepath, metadata=None):
        """ä¿å­˜æ¨¡å‹"""
        save_dir = Path(filepath).parent
        save_dir.mkdir(parents=True, exist_ok=True)
        
        save_dict = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'train_losses': self.train_losses,
            'train_accuracies': self.train_accuracies,
            'val_losses': self.val_losses,
            'val_accuracies': self.val_accuracies,
            'sequence_length': self.sequence_length,
            'num_classes': self.num_classes,
            'learning_rate': self.learning_rate,
        }
        
        if metadata:
            save_dict.update(metadata)
        
        torch.save(save_dict, filepath)
        print(f'âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}')
    
    def plot_training_history(self, save_path=None):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # æŸå¤±æ›²çº¿
        ax1.plot(self.train_losses, label='è®­ç»ƒæŸå¤±', color='blue')
        ax1.plot(self.val_losses, label='éªŒè¯æŸå¤±', color='red')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('æŸå¤±')
        ax1.set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(self.train_accuracies, label='è®­ç»ƒå‡†ç¡®ç‡', color='blue')
        ax2.plot(self.val_accuracies, label='éªŒè¯å‡†ç¡®ç‡', color='red')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('å‡†ç¡®ç‡ (%)')
        ax2.set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f'âœ… è®­ç»ƒå†å²å›¾å·²ä¿å­˜åˆ°: {save_path}')
        
        plt.show()

def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("ğŸš€ Temporal 3D CNN è®­ç»ƒå¼€å§‹")
    print("="*80)
    
    # è®­ç»ƒå‚æ•°
    sequence_length = 5
    num_classes = 20
    learning_rate = 0.001
    batch_size = 16  # 3D CNNå†…å­˜å ç”¨è¾ƒå¤§ï¼Œä½¿ç”¨è¾ƒå°çš„batch size
    epochs = 100
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Temporal3DCNNTrainer(
        sequence_length=sequence_length,
        num_classes=num_classes,
        learning_rate=learning_rate
    )
    
    # åŠ è½½æ•°æ®
    train_seq, val_seq, test_seq, train_labels, val_labels, test_labels = trainer.load_data(sequence_length)
    
    if train_seq is None:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
        return
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader, test_loader = trainer.create_data_loaders(
        train_seq, val_seq, test_seq, train_labels, val_labels, test_labels, batch_size
    )
    
    # è®­ç»ƒæ¨¡å‹
    best_val_acc = trainer.train(train_loader, val_loader, epochs)
    
    # æµ‹è¯•æ¨¡å‹
    test_acc, predictions, targets = trainer.test(test_loader)
    
    # ä¿å­˜æ¨¡å‹
    model_path = f"models/saved/temporal_3dcnn_seq{sequence_length}_acc{test_acc:.1f}.pth"
    metadata = {
        'best_val_acc': best_val_acc,
        'test_acc': test_acc,
        'batch_size': batch_size,
        'epochs': epochs
    }
    trainer.save_model(model_path, metadata)
    
    # ç»˜åˆ¶è®­ç»ƒå†å²
    plot_path = f"outputs/temporal_3dcnn_seq{sequence_length}_training_history.png"
    trainer.plot_training_history(plot_path)
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")
    print(f"æ¨¡å‹ä¿å­˜è·¯å¾„: {model_path}")

if __name__ == '__main__':
    main()
