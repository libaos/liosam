#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åŠ å¼ºè®­ç»ƒè„šæœ¬ - æ›´å¤§è®­ç»ƒé‡å’Œæ›´å¤šå®éªŒ
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import pickle
import numpy as np
from pathlib import Path
import time
import json
from datetime import datetime

# å¯¼å…¥æ¨¡å‹å’Œæ•°æ®é›†
from models.temporal_3d_cnn import Temporal3DCNN, Temporal3DCNNLite, Temporal3DCNNDeep
from models.temporal_2d_cnn import Temporal2DCNN, Temporal2DCNNLite, Temporal2DCNNResNet
from utils.temporal_dataset import TemporalScanContextDataset
from utils.logger import Logger

class IntensiveTrainer:
    """åŠ å¼ºè®­ç»ƒå™¨"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = Path(f"outputs/intensive_training_{timestamp}")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ—¥å¿—
        self.logger = Logger(self.output_dir / 'intensive_training.log')
        
        print(f"ğŸš€ åŠ å¼ºè®­ç»ƒé…ç½®:")
        print(f"  è®¾å¤‡: {self.device}")
        print(f"  è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"  è®­ç»ƒè½®æ•°: {config['epochs']}")
        print(f"  æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
        print(f"  å­¦ä¹ ç‡: {config['lr']}")
    
    def create_model(self, model_type):
        """åˆ›å»ºæ¨¡å‹"""
        models = {
            'temporal_3d_cnn': Temporal3DCNN(sequence_length=5, num_classes=20),
            'temporal_3d_cnn_lite': Temporal3DCNNLite(sequence_length=5, num_classes=20),
            'temporal_3d_cnn_deep': Temporal3DCNNDeep(sequence_length=5, num_classes=20),
            'temporal_2d_cnn': Temporal2DCNN(sequence_length=5, num_classes=20),
            'temporal_2d_cnn_lite': Temporal2DCNNLite(sequence_length=5, num_classes=20),
            'temporal_2d_cnn_resnet': Temporal2DCNNResNet(sequence_length=5, num_classes=20)
        }
        
        if model_type not in models:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
        
        return models[model_type].to(self.device)
    
    def create_dataset(self, use_all_sequences=True):
        """åˆ›å»ºæ•°æ®é›† - ä½¿ç”¨æ›´å¤šæ•°æ®"""
        
        class IntensiveTemporalDataset(TemporalScanContextDataset):
            def _load_data(self):
                # åŠ è½½æ‰€æœ‰é•¿åº¦çš„åºåˆ—æ•°æ®
                all_sequences = []
                all_labels = []
                all_file_paths = []
                
                if use_all_sequences:
                    # ä½¿ç”¨å¤šç§åºåˆ—é•¿åº¦çš„æ•°æ®
                    for seq_len in [3, 5, 7, 10]:
                        data_file = Path(f"data/processed/temporal_sequences_len{seq_len}.pkl")
                        if data_file.exists():
                            with open(data_file, 'rb') as f:
                                data = pickle.load(f)
                                # å°†ä¸åŒé•¿åº¦çš„åºåˆ—å¡«å……æˆ–æˆªæ–­åˆ°ç»Ÿä¸€é•¿åº¦5
                                for seq in data['sequences']:
                                    if seq.shape[0] < 5:
                                        # å¡«å……
                                        padded_seq = np.zeros((5, seq.shape[1], seq.shape[2]))
                                        padded_seq[:seq.shape[0]] = seq
                                        all_sequences.append(padded_seq)
                                    elif seq.shape[0] > 5:
                                        # æˆªæ–­
                                        all_sequences.append(seq[:5])
                                    else:
                                        all_sequences.append(seq)
                                
                                all_labels.extend(data['labels'])
                                all_file_paths.extend(data.get('file_paths', []))
                            
                            print(f"åŠ è½½åºåˆ—é•¿åº¦ {seq_len}: {len(data['sequences'])} ä¸ªæ ·æœ¬")
                else:
                    # åªä½¿ç”¨é•¿åº¦ä¸º5çš„åºåˆ—
                    data_file = Path("data/processed/temporal_sequences_len5.pkl")
                    if data_file.exists():
                        with open(data_file, 'rb') as f:
                            data = pickle.load(f)
                            all_sequences = data['sequences']
                            all_labels = data['labels']
                            all_file_paths = data.get('file_paths', [])
                
                self.sequences = all_sequences
                self.labels = all_labels
                self.file_paths = all_file_paths
                print(f"æ€»å…±åŠ è½½äº† {len(self.sequences)} ä¸ªæ ·æœ¬")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = IntensiveTemporalDataset(
            data_dir="data/processed",
            split='train',
            sequence_length=5,
            use_augmentation=True
        )
        
        # æ•°æ®é›†åˆ’åˆ†
        total_size = len(dataset)
        train_size = int(0.7 * total_size)
        val_size = int(0.15 * total_size)
        test_size = total_size - train_size - val_size
        
        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size, test_size],
            generator=torch.Generator().manual_seed(42)
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset, 
            batch_size=self.config['batch_size'], 
            shuffle=True,
            num_workers=4,
            pin_memory=True
        )
        val_loader = DataLoader(
            val_dataset, 
            batch_size=self.config['batch_size'], 
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        test_loader = DataLoader(
            test_dataset, 
            batch_size=self.config['batch_size'], 
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        
        print(f"æ•°æ®é›†åˆ’åˆ†:")
        print(f"  è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
        print(f"  éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
        print(f"  æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
        
        return train_loader, val_loader, test_loader
    
    def train_model(self, model_type):
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        print(f"\nğŸ”¥ å¼€å§‹è®­ç»ƒ {model_type}...")
        
        # åˆ›å»ºæ¨¡å‹
        model = self.create_model(model_type)
        
        # åˆ›å»ºæ•°æ®é›†
        train_loader, val_loader, test_loader = self.create_dataset(
            use_all_sequences=self.config.get('use_all_sequences', True)
        )
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = optim.AdamW(
            model.parameters(), 
            lr=self.config['lr'],
            weight_decay=self.config.get('weight_decay', 1e-4),
            betas=(0.9, 0.999)
        )
        
        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # æ ‡ç­¾å¹³æ»‘
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer, T_0=20, T_mult=2, eta_min=1e-6
        )
        
        # è®­ç»ƒè®°å½•
        train_history = []
        val_history = []
        best_val_acc = 0.0
        patience = 0
        max_patience = 30
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(self.config['epochs']):
            start_time = time.time()
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(
                model, train_loader, optimizer, criterion, epoch
            )
            
            # éªŒè¯
            val_loss, val_acc = self.validate_epoch(
                model, val_loader, criterion
            )
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()
            current_lr = optimizer.param_groups[0]['lr']
            
            # æ—©åœæ£€æŸ¥
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                self.save_checkpoint(model, optimizer, epoch, val_acc, model_type, is_best=True)
            else:
                patience += 1
            
            # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
            if epoch % 10 == 0:
                self.save_checkpoint(model, optimizer, epoch, val_acc, model_type, is_best=False)
            
            # è®°å½•å†å²
            train_history.append({'epoch': epoch, 'loss': train_loss, 'acc': train_acc})
            val_history.append({'epoch': epoch, 'loss': val_loss, 'acc': val_acc})
            
            # æ‰“å°ç»“æœ
            epoch_time = time.time() - start_time
            print(f'Epoch {epoch:3d}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '
                  f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, '
                  f'Time: {epoch_time:.2f}s, LR: {current_lr:.6f}, Patience: {patience}')
            
            # è®°å½•æ—¥å¿—
            self.logger.info(
                f'{model_type} Epoch {epoch}: Train Loss: {train_loss:.4f}, '
                f'Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%'
            )
            
            # æ—©åœ
            if patience >= max_patience:
                print(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch} è½®åœæ­¢è®­ç»ƒ")
                break
        
        # ä¿å­˜è®­ç»ƒå†å²
        history = {
            'model_type': model_type,
            'train': train_history,
            'val': val_history,
            'best_val_acc': best_val_acc,
            'config': self.config
        }
        
        with open(self.output_dir / f'{model_type}_history.json', 'w') as f:
            json.dump(history, f, indent=2)
        
        print(f"\nâœ… {model_type} è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
        return model, best_val_acc, history
    
    def train_epoch(self, model, train_loader, optimizer, criterion, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            total_loss += loss.item()
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()
            
            if batch_idx % 20 == 0:
                print(f'  Batch {batch_idx:3d}/{len(train_loader)}: Loss: {loss.item():.4f}, '
                      f'Acc: {100.*correct/total:.2f}%')
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def validate_epoch(self, model, val_loader, criterion):
        """éªŒè¯ä¸€ä¸ªepoch"""
        model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = model(data)
                loss = criterion(output, target)
                
                total_loss += loss.item()
                _, predicted = output.max(1)
                total += target.size(0)
                correct += predicted.eq(target).sum().item()
        
        avg_loss = total_loss / len(val_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def save_checkpoint(self, model, optimizer, epoch, val_acc, model_type, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_acc': val_acc,
            'model_type': model_type,
            'config': self.config
        }
        
        if is_best:
            torch.save(checkpoint, self.output_dir / f'{model_type}_best.pth')
        else:
            torch.save(checkpoint, self.output_dir / f'{model_type}_epoch_{epoch}.pth')
    
    def run_intensive_training(self):
        """è¿è¡ŒåŠ å¼ºè®­ç»ƒ"""
        print("ğŸš€ å¼€å§‹åŠ å¼ºè®­ç»ƒå®éªŒ...")
        
        # è¦è®­ç»ƒçš„æ¨¡å‹åˆ—è¡¨
        models_to_train = [
            'temporal_3d_cnn',
            'temporal_3d_cnn_deep',
            'temporal_2d_cnn',
            'temporal_2d_cnn_resnet'
        ]
        
        results = {}
        
        for model_type in models_to_train:
            try:
                model, best_acc, history = self.train_model(model_type)
                results[model_type] = {
                    'best_val_acc': best_acc,
                    'total_epochs': len(history['train']),
                    'final_train_acc': history['train'][-1]['acc'],
                    'final_val_acc': history['val'][-1]['acc']
                }
                
                # æ¸…ç†GPUå†…å­˜
                del model
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                
            except Exception as e:
                print(f"âŒ è®­ç»ƒ {model_type} æ—¶å‡ºé”™: {e}")
                continue
        
        # ä¿å­˜æ€»ç»“æœ
        with open(self.output_dir / 'intensive_results_summary.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        # æ‰“å°æ€»ç»“
        print("\n" + "="*80)
        print("ğŸ† åŠ å¼ºè®­ç»ƒç»“æœæ€»ç»“")
        print("="*80)
        
        for model_type, result in results.items():
            print(f"{model_type:<25}: æœ€ä½³éªŒè¯å‡†ç¡®ç‡ {result['best_val_acc']:.2f}%, "
                  f"è®­ç»ƒè½®æ•° {result['total_epochs']}")
        
        return results


def main():
    """ä¸»å‡½æ•°"""
    # åŠ å¼ºè®­ç»ƒé…ç½®
    intensive_config = {
        'epochs': 150,           # å¢åŠ åˆ°150è½®
        'batch_size': 16,        # å¢åŠ æ‰¹æ¬¡å¤§å°
        'lr': 0.001,            # åˆå§‹å­¦ä¹ ç‡
        'weight_decay': 1e-4,   # æƒé‡è¡°å‡
        'use_all_sequences': True  # ä½¿ç”¨æ‰€æœ‰åºåˆ—é•¿åº¦çš„æ•°æ®
    }
    
    print("ğŸš€ å¯åŠ¨åŠ å¼ºè®­ç»ƒ...")
    print(f"é…ç½®: {intensive_config}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = IntensiveTrainer(intensive_config)
    
    # å¼€å§‹è®­ç»ƒ
    results = trainer.run_intensive_training()
    
    print("\nâœ… åŠ å¼ºè®­ç»ƒå®Œæˆï¼")


if __name__ == '__main__':
    main()
