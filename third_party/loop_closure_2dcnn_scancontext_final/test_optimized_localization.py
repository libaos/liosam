#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æµ‹è¯•ä¼˜åŒ–çš„è½¨è¿¹å®šä½ç³»ç»Ÿ
"""

import numpy as np
import torch
from trajectory_localization_system import TrajectoryLocalizationSystem
from utils.scan_context import ScanContext
import time
import matplotlib.pyplot as plt
import pickle

# å°è¯•å¯¼å…¥rosbagç›¸å…³åº“
try:
    import rosbag
    import sensor_msgs.point_cloud2 as pc2
    ROSBAG_AVAILABLE = True
except ImportError:
    ROSBAG_AVAILABLE = False

class OptimizedLocalizationTester:
    """ä¼˜åŒ–çš„è½¨è¿¹å®šä½æµ‹è¯•å™¨"""
    
    def __init__(self, model_path, database_path):
        self.localizer = TrajectoryLocalizationSystem(
            num_locations=20,
            adaptive_segments=True  # å¯ç”¨è‡ªé€‚åº”åˆ†æ®µ
        )
        self.sc_generator = ScanContext()
        
        print(f"ğŸ¯ ä¼˜åŒ–çš„è½¨è¿¹å®šä½æµ‹è¯•å™¨")
        print(f"ç›®æ ‡: æµ‹è¯•ä¼˜åŒ–åçš„å®æ—¶å®šä½æ€§èƒ½")
        
        # åŠ è½½ä½ç½®æ•°æ®åº“
        if not self.localizer.load_location_database(database_path):
            print("âŒ ä½ç½®æ•°æ®åº“åŠ è½½å¤±è´¥")
            return
        
        # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        if not self.localizer.load_trained_model(model_path):
            print("âŒ å®šä½æ¨¡å‹åŠ è½½å¤±è´¥")
            return
        
        print("âœ… ä¼˜åŒ–çš„è½¨è¿¹å®šä½ç³»ç»Ÿå‡†å¤‡å°±ç»ª")
        
        # å®šä½å†å²å’Œç»Ÿè®¡
        self.localization_history = []
        self.confidence_history = []
        self.timestamp_history = []
        self.processing_times = []
        
        # æ€§èƒ½ç»Ÿè®¡
        self.high_confidence_count = 0
        self.low_confidence_count = 0
        self.temporal_smoothing_count = 0
        
    def pointcloud2_to_numpy(self, cloud_msg):
        """å°†PointCloud2æ¶ˆæ¯è½¬æ¢ä¸ºnumpyæ•°ç»„"""
        if not ROSBAG_AVAILABLE:
            return None
            
        try:
            points_list = []
            for point in pc2.read_points(cloud_msg, field_names=("x", "y", "z"), skip_nans=True):
                points_list.append([point[0], point[1], point[2]])
            
            if len(points_list) == 0:
                return None
                
            points = np.array(points_list, dtype=np.float32)
            valid_mask = np.isfinite(points).all(axis=1)
            points = points[valid_mask]
            
            distances = np.linalg.norm(points[:, :2], axis=1)
            distance_mask = distances < 50.0
            points = points[distance_mask]
            
            if len(points) < 100:
                return None
                
            return points
            
        except Exception as e:
            print(f"ç‚¹äº‘è½¬æ¢å¤±è´¥: {e}")
            return None
    
    def test_optimized_localization(self, bag_path, topic_name=None):
        """æµ‹è¯•ä¼˜åŒ–çš„è½¨è¿¹å®šä½"""
        
        if not ROSBAG_AVAILABLE:
            print("âŒ rosbagåº“ä¸å¯ç”¨")
            return None
            
        try:
            bag = rosbag.Bag(bag_path, 'r')
        except Exception as e:
            print(f"æ— æ³•æ‰“å¼€rosbagæ–‡ä»¶: {e}")
            return None
        
        # è·å–è¯é¢˜ä¿¡æ¯
        topics_info = bag.get_type_and_topic_info()[1]
        print(f"\nrosbagè¯é¢˜ä¿¡æ¯:")
        
        pointcloud_topics = []
        for topic, info in topics_info.items():
            print(f"  {topic}: {info.msg_type} ({info.message_count} æ¶ˆæ¯)")
            if 'PointCloud2' in info.msg_type:
                pointcloud_topics.append(topic)
        
        if not pointcloud_topics:
            print("âŒ æœªæ‰¾åˆ°PointCloud2è¯é¢˜")
            bag.close()
            return None
        
        if topic_name is None:
            topic_name = pointcloud_topics[0]
        
        print(f"\nğŸ¯ å¼€å§‹ä¼˜åŒ–è½¨è¿¹å®šä½æµ‹è¯•")
        print(f"ä½¿ç”¨è¯é¢˜: {topic_name}")
        print(f"ä¼˜åŒ–ç‰¹æ€§: è‡ªé€‚åº”åˆ†æ®µ + æ—¶åºå¹³æ»‘ + ç½®ä¿¡åº¦è¿‡æ»¤")
        print("-" * 60)
        
        # å¤„ç†æ¶ˆæ¯
        total_messages = 0
        valid_localizations = 0
        start_time = time.time()
        
        for topic, msg, t in bag.read_messages(topics=[topic_name]):
            total_messages += 1
            timestamp = t.to_sec()
            
            # è®°å½•å¤„ç†å¼€å§‹æ—¶é—´
            process_start = time.time()
            
            # è½¬æ¢ç‚¹äº‘
            points = self.pointcloud2_to_numpy(msg)
            if points is None:
                continue
            
            # ç”ŸæˆScanContext
            sc_feature = self.sc_generator.generate_scan_context(points)
            if sc_feature is None:
                continue
            
            # ä¼˜åŒ–çš„å®šä½
            predicted_location, confidence = self.localizer.localize_position(sc_feature)
            
            # è®°å½•å¤„ç†æ—¶é—´
            process_time = time.time() - process_start
            self.processing_times.append(process_time)
            
            if predicted_location is not None:
                valid_localizations += 1
                
                # ä¿å­˜ç»“æœ
                self.localization_history.append(predicted_location)
                self.confidence_history.append(confidence)
                self.timestamp_history.append(timestamp)
                
                # ç»Ÿè®¡ç½®ä¿¡åº¦
                if confidence >= self.localizer.confidence_threshold:
                    self.high_confidence_count += 1
                else:
                    self.low_confidence_count += 1
                
                # è®¡ç®—æœŸæœ›ä½ç½®ï¼ˆåŸºäºè¿›åº¦ï¼‰
                progress = total_messages / 2132  # å‡è®¾æ€»é•¿åº¦
                expected_location = int(progress * (self.localizer.num_locations - 1))
                
                # è®¡ç®—å®šä½è¯¯å·®
                location_error = abs(predicted_location - expected_location)
                
                # å®æ—¶è¾“å‡ºï¼ˆæ¯50ä¸ªæ˜¾ç¤ºä¸€æ¬¡ï¼‰
                if valid_localizations % 50 == 0:
                    status = "âœ…" if location_error <= 2 else "âŒ"
                    avg_process_time = np.mean(self.processing_times[-50:]) * 1000
                    print(f"æ¶ˆæ¯ {total_messages:4d} | é¢„æµ‹: {predicted_location:2d} | "
                          f"æœŸæœ›: {expected_location:2d} | è¯¯å·®: {location_error:2d} | "
                          f"ç½®ä¿¡åº¦: {confidence:.3f} | å¤„ç†: {avg_process_time:.1f}ms {status}")
                    
                    # æ˜¾ç¤ºä¼˜åŒ–ç»Ÿè®¡
                    if valid_localizations % 200 == 0:
                        self.show_optimization_stats()
            
            # å¤„ç†å®Œæ•´æ•°æ®é›†
            if total_messages >= 2132:
                print(f"\nå·²å¤„ç†å®Œæ•´æ•°æ®é›† ({total_messages} ä¸ªæ¶ˆæ¯)")
                break
        
        bag.close()
        
        elapsed_time = time.time() - start_time
        print(f"\n" + "="*60)
        print("ä¼˜åŒ–è½¨è¿¹å®šä½æµ‹è¯•å®Œæˆ")
        print("="*60)
        print(f"æ€»æ¶ˆæ¯æ•°: {total_messages}")
        print(f"æœ‰æ•ˆå®šä½æ•°: {valid_localizations}")
        print(f"æˆåŠŸç‡: {valid_localizations/total_messages*100:.1f}%")
        print(f"å¤„ç†æ—¶é—´: {elapsed_time:.1f}ç§’")
        print(f"å®šä½é¢‘ç‡: {valid_localizations/elapsed_time:.2f} Hz")
        print(f"å¹³å‡å¤„ç†æ—¶é—´: {np.mean(self.processing_times)*1000:.2f}ms")
        
        return self.analyze_optimized_results()
    
    def show_optimization_stats(self):
        """æ˜¾ç¤ºä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        total_localizations = len(self.localization_history)
        if total_localizations == 0:
            return
        
        high_conf_rate = self.high_confidence_count / total_localizations * 100
        low_conf_rate = self.low_confidence_count / total_localizations * 100
        
        print(f"\n--- ä¼˜åŒ–æ€§èƒ½ç»Ÿè®¡ ---")
        print(f"é«˜ç½®ä¿¡åº¦å®šä½: {self.high_confidence_count} ({high_conf_rate:.1f}%)")
        print(f"ä½ç½®ä¿¡åº¦å®šä½: {self.low_confidence_count} ({low_conf_rate:.1f}%)")
        print(f"å¹³å‡ç½®ä¿¡åº¦: {np.mean(self.confidence_history):.3f}")
        print(f"ç½®ä¿¡åº¦æ ‡å‡†å·®: {np.std(self.confidence_history):.3f}")
        print("-" * 30)
    
    def analyze_optimized_results(self):
        """åˆ†æä¼˜åŒ–åçš„å®šä½ç»“æœ"""
        if len(self.localization_history) == 0:
            return None
        
        locations = np.array(self.localization_history)
        confidences = np.array(self.confidence_history)
        timestamps = np.array(self.timestamp_history)
        
        print(f"\nğŸ“Š ä¼˜åŒ–è½¨è¿¹å®šä½ç»“æœåˆ†æ")
        print(f"{'='*50}")
        
        # åŸºæœ¬ç»Ÿè®¡
        print(f"å®šä½ä½ç½®æ•°é‡: {len(np.unique(locations))}")
        print(f"ä½ç½®èŒƒå›´: {np.min(locations)} - {np.max(locations)}")
        print(f"å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidences):.4f}")
        print(f"ç½®ä¿¡åº¦æ ‡å‡†å·®: {np.std(confidences):.4f}")
        
        # ä¼˜åŒ–æ•ˆæœåˆ†æ
        high_conf_rate = self.high_confidence_count / len(locations) * 100
        print(f"\nğŸ¯ ä¼˜åŒ–æ•ˆæœ:")
        print(f"é«˜ç½®ä¿¡åº¦å®šä½ç‡: {high_conf_rate:.1f}%")
        print(f"å¹³å‡å¤„ç†æ—¶é—´: {np.mean(self.processing_times)*1000:.2f}ms")
        print(f"å¤„ç†æ—¶é—´æ ‡å‡†å·®: {np.std(self.processing_times)*1000:.2f}ms")
        
        # è®¡ç®—å®šä½å‡†ç¡®æ€§
        expected_locations = np.linspace(0, self.localizer.num_locations-1, len(locations)).astype(int)
        location_errors = np.abs(locations - expected_locations)
        
        accuracy_1 = np.mean(location_errors <= 1) * 100
        accuracy_2 = np.mean(location_errors <= 2) * 100
        accuracy_3 = np.mean(location_errors <= 3) * 100
        
        print(f"\nğŸ“ˆ å®šä½å‡†ç¡®æ€§:")
        print(f"è¯¯å·®â‰¤1ä½ç½®å‡†ç¡®ç‡: {accuracy_1:.1f}%")
        print(f"è¯¯å·®â‰¤2ä½ç½®å‡†ç¡®ç‡: {accuracy_2:.1f}%")
        print(f"è¯¯å·®â‰¤3ä½ç½®å‡†ç¡®ç‡: {accuracy_3:.1f}%")
        print(f"å¹³å‡ä½ç½®è¯¯å·®: {np.mean(location_errors):.2f}")
        
        # æ—¶åºç¨³å®šæ€§åˆ†æ
        location_changes = np.sum(np.diff(locations) != 0)
        stability_score = 1 - (location_changes / len(locations))
        
        print(f"\nğŸ”„ æ—¶åºç¨³å®šæ€§:")
        print(f"ä½ç½®å˜åŒ–æ¬¡æ•°: {location_changes}")
        print(f"ç¨³å®šæ€§è¯„åˆ†: {stability_score:.3f}")
        
        # å¯è§†åŒ–ç»“æœ
        self.visualize_optimized_results(locations, confidences, expected_locations)
        
        return {
            'locations': locations.tolist(),
            'confidences': confidences.tolist(),
            'timestamps': timestamps.tolist(),
            'accuracy_1': accuracy_1,
            'accuracy_2': accuracy_2,
            'accuracy_3': accuracy_3,
            'mean_error': np.mean(location_errors),
            'high_confidence_rate': high_conf_rate,
            'stability_score': stability_score,
            'avg_processing_time': np.mean(self.processing_times),
            'location_changes': location_changes
        }
    
    def visualize_optimized_results(self, locations, confidences, expected_locations):
        """å¯è§†åŒ–ä¼˜åŒ–åçš„å®šä½ç»“æœ"""
        fig, axes = plt.subplots(4, 1, figsize=(15, 12))
        
        # 1. ä½ç½®é¢„æµ‹vsæœŸæœ›
        axes[0].plot(expected_locations, 'b-', alpha=0.7, label='æœŸæœ›ä½ç½®', linewidth=2)
        axes[0].plot(locations, 'r-', alpha=0.8, label='é¢„æµ‹ä½ç½®', linewidth=1)
        axes[0].set_ylabel('ä½ç½®ID')
        axes[0].set_title('ä¼˜åŒ–è½¨è¿¹å®šä½ç»“æœï¼šé¢„æµ‹ä½ç½® vs æœŸæœ›ä½ç½®')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # 2. å®šä½ç½®ä¿¡åº¦
        axes[1].plot(confidences, 'g-', alpha=0.7, label='å®šä½ç½®ä¿¡åº¦')
        axes[1].axhline(y=self.localizer.confidence_threshold, color='red', 
                       linestyle='--', alpha=0.7, label=f'ç½®ä¿¡åº¦é˜ˆå€¼ ({self.localizer.confidence_threshold})')
        axes[1].set_ylabel('ç½®ä¿¡åº¦')
        axes[1].set_title('å®šä½ç½®ä¿¡åº¦å˜åŒ–ï¼ˆä¼˜åŒ–åï¼‰')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        # 3. ä½ç½®è¯¯å·®
        location_errors = np.abs(locations - expected_locations)
        axes[2].plot(location_errors, 'orange', alpha=0.7, label='ä½ç½®è¯¯å·®')
        axes[2].axhline(y=1, color='red', linestyle='--', alpha=0.5, label='è¯¯å·®=1')
        axes[2].axhline(y=2, color='red', linestyle='--', alpha=0.5, label='è¯¯å·®=2')
        axes[2].set_ylabel('ä½ç½®è¯¯å·®')
        axes[2].set_title('å®šä½è¯¯å·®åˆ†æï¼ˆä¼˜åŒ–åï¼‰')
        axes[2].legend()
        axes[2].grid(True, alpha=0.3)
        
        # 4. å¤„ç†æ—¶é—´
        processing_times_ms = np.array(self.processing_times) * 1000
        axes[3].plot(processing_times_ms, 'purple', alpha=0.7, label='å¤„ç†æ—¶é—´')
        axes[3].axhline(y=np.mean(processing_times_ms), color='red', 
                       linestyle='--', alpha=0.7, label=f'å¹³å‡æ—¶é—´ ({np.mean(processing_times_ms):.1f}ms)')
        axes[3].set_ylabel('å¤„ç†æ—¶é—´ (ms)')
        axes[3].set_xlabel('å¸§ç´¢å¼•')
        axes[3].set_title('å®æ—¶å¤„ç†æ€§èƒ½')
        axes[3].legend()
        axes[3].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('optimized_trajectory_localization_results.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š ä¼˜åŒ–å®šä½ç»“æœå›¾è¡¨å·²ä¿å­˜ä¸º optimized_trajectory_localization_results.png")

def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python test_optimized_localization.py <bag_path> [topic_name]")
        return
    
    bag_path = sys.argv[1]
    topic_name = sys.argv[2] if len(sys.argv) > 2 else None
    
    print("="*60)
    print("ğŸ¯ ä¼˜åŒ–è½¨è¿¹å®šä½ç³»ç»Ÿæµ‹è¯•")
    print("="*60)
    print(f"ç›®æ ‡: æµ‹è¯•ä¼˜åŒ–åçš„å®šä½æ€§èƒ½")
    print(f"rosbag: {bag_path}")
    
    # æ¨¡å‹å’Œæ•°æ®åº“è·¯å¾„
    model_path = "models/saved/trajectory_localizer_simple2dcnn_acc*.pth"
    database_path = "location_database.pkl"
    
    # æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶
    import glob
    model_files = glob.glob(model_path)
    if not model_files:
        print(f"âŒ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶")
        print(f"è¯·å…ˆè¿è¡Œ trajectory_localization_system.py è®­ç»ƒæ¨¡å‹")
        return
    
    model_path = sorted(model_files)[-1]
    print(f"ä½¿ç”¨æ¨¡å‹: {model_path}")
    
    # åˆ›å»ºä¼˜åŒ–æµ‹è¯•å™¨
    tester = OptimizedLocalizationTester(model_path, database_path)
    
    # æµ‹è¯•ä¼˜åŒ–å®šä½
    results = tester.test_optimized_localization(bag_path, topic_name)
    
    if results:
        print(f"\nâœ… ä¼˜åŒ–è½¨è¿¹å®šä½æµ‹è¯•å®Œæˆï¼")
        print(f"è¯¯å·®â‰¤1ä½ç½®å‡†ç¡®ç‡: {results['accuracy_1']:.1f}%")
        print(f"è¯¯å·®â‰¤2ä½ç½®å‡†ç¡®ç‡: {results['accuracy_2']:.1f}%")
        print(f"é«˜ç½®ä¿¡åº¦å®šä½ç‡: {results['high_confidence_rate']:.1f}%")
        print(f"æ—¶åºç¨³å®šæ€§è¯„åˆ†: {results['stability_score']:.3f}")
        print(f"å¹³å‡å¤„ç†æ—¶é—´: {results['avg_processing_time']*1000:.2f}ms")
        
        # ä¿å­˜ç»“æœ
        result_path = 'optimized_trajectory_localization_test_results.pkl'
        with open(result_path, 'wb') as f:
            pickle.dump(results, f)
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {result_path}")
    else:
        print("âŒ ä¼˜åŒ–è½¨è¿¹å®šä½æµ‹è¯•å¤±è´¥")

if __name__ == '__main__':
    main()
