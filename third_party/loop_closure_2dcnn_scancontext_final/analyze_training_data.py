#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åˆ†æè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®çš„å·®å¼‚
"""

import pickle
import numpy as np
from pathlib import Path

def analyze_training_data():
    """åˆ†æè®­ç»ƒæ•°æ®"""
    print("="*60)
    print("è®­ç»ƒæ•°æ®åˆ†æ")
    print("="*60)
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    with open('data/processed/temporal_sequences_len5.pkl', 'rb') as f:
        data = pickle.load(f)
    
    sequences = np.array(data['sequences'])
    labels = np.array(data['labels'])
    
    print(f"åºåˆ—æ•°é‡: {len(sequences)}")
    print(f"æ ‡ç­¾æ•°é‡: {len(labels)}")
    print(f"åºåˆ—å½¢çŠ¶: {sequences.shape}")
    print(f"æ ‡ç­¾èŒƒå›´: {np.min(labels)} - {np.max(labels)}")
    print(f"ç±»åˆ«æ•°: {len(np.unique(labels))}")
    
    print(f"\næ ‡ç­¾åˆ†å¸ƒ:")
    unique_labels, counts = np.unique(labels, return_counts=True)
    for label, count in zip(unique_labels, counts):
        percentage = count / len(labels) * 100
        print(f"  ç±»åˆ« {label:2d}: {count:3d} ä¸ªåºåˆ— ({percentage:5.1f}%)")
    
    # åˆ†æScanContextç‰¹å¾
    print(f"\nScanContextç‰¹å¾åˆ†æ:")
    print(f"ç‰¹å¾å½¢çŠ¶: {sequences.shape[1:]}")  # (5, 20, 60)
    print(f"ç‰¹å¾èŒƒå›´: {np.min(sequences):.4f} - {np.max(sequences):.4f}")
    print(f"ç‰¹å¾å‡å€¼: {np.mean(sequences):.4f}")
    print(f"ç‰¹å¾æ ‡å‡†å·®: {np.std(sequences):.4f}")
    
    return sequences, labels

def analyze_test_data():
    """åˆ†ææµ‹è¯•æ—¶çš„ScanContextç‰¹å¾"""
    print("\n" + "="*60)
    print("æµ‹è¯•æ•°æ®åˆ†æ (ä»rosbagå®æ—¶ç”Ÿæˆ)")
    print("="*60)
    
    # è¿™é‡Œæˆ‘ä»¬éœ€è¦åˆ†æå®æ—¶ç”Ÿæˆçš„ScanContextç‰¹å¾
    # ç”±äºæ— æ³•ç›´æ¥è·å–ï¼Œæˆ‘ä»¬åˆ†æä¸€ä¸‹å¯èƒ½çš„é—®é¢˜
    
    print("æµ‹è¯•æ•°æ®ç‰¹å¾:")
    print("- æ•°æ®æ¥æº: rosbag /points_raw è¯é¢˜")
    print("- ç‚¹äº‘èŒƒå›´: 24153 - 32996 ç‚¹")
    print("- å¤„ç†æ–¹å¼: å®æ—¶ScanContextç”Ÿæˆ")
    print("- é¢„æµ‹ç»“æœ: ä¸»è¦æ˜¯ç±»åˆ«1,5,18")
    
    print("\nå¯èƒ½çš„é—®é¢˜:")
    print("1. è®­ç»ƒæ•°æ®æ˜¯ä»plyæ–‡ä»¶ç”Ÿæˆçš„ScanContext")
    print("2. æµ‹è¯•æ•°æ®æ˜¯ä»rosbagå®æ—¶ç”Ÿæˆçš„ScanContext")
    print("3. ä¸¤è€…çš„é¢„å¤„ç†å¯èƒ½ä¸ä¸€è‡´")
    print("4. ç‚¹äº‘åæ ‡ç³»æˆ–å°ºåº¦å¯èƒ½ä¸åŒ")

def compare_scancontext_generation():
    """å¯¹æ¯”ScanContextç”Ÿæˆæ–¹å¼"""
    print("\n" + "="*60)
    print("ScanContextç”Ÿæˆæ–¹å¼å¯¹æ¯”")
    print("="*60)
    
    from utils.scan_context import ScanContext
    from utils.ply_reader import PLYReader
    
    sc_generator = ScanContext()
    
    # 1. ä»plyæ–‡ä»¶ç”ŸæˆScanContext (è®­ç»ƒæ–¹å¼)
    ply_files = list(Path("/mysda/shared_dir/2025.7.3/2025-07-03-16-28-57.ply").glob("*.ply"))
    if len(ply_files) > 0:
        ply_file = ply_files[0]
        print(f"æµ‹è¯•plyæ–‡ä»¶: {ply_file.name}")
        
        points_ply = PLYReader.read_ply_file(str(ply_file))
        if points_ply is not None:
            points_ply = points_ply[:, :3]  # åªå–x,y,z
            sc_ply = sc_generator.generate_scan_context(points_ply)
            
            print(f"PLYç‚¹äº‘:")
            print(f"  ç‚¹æ•°: {len(points_ply)}")
            print(f"  åæ ‡èŒƒå›´: x[{np.min(points_ply[:,0]):.2f}, {np.max(points_ply[:,0]):.2f}]")
            print(f"             y[{np.min(points_ply[:,1]):.2f}, {np.max(points_ply[:,1]):.2f}]")
            print(f"             z[{np.min(points_ply[:,2]):.2f}, {np.max(points_ply[:,2]):.2f}]")
            print(f"  ScanContextå½¢çŠ¶: {sc_ply.shape}")
            print(f"  ScanContextèŒƒå›´: [{np.min(sc_ply):.4f}, {np.max(sc_ply):.4f}]")
            print(f"  ScanContextå‡å€¼: {np.mean(sc_ply):.4f}")
            print(f"  ScanContextæ ‡å‡†å·®: {np.std(sc_ply):.4f}")
    
    # 2. åŠ è½½è®­ç»ƒæ•°æ®ä¸­çš„ScanContext
    print(f"\nè®­ç»ƒæ•°æ®ä¸­çš„ScanContext:")
    with open('data/processed/temporal_sequences_len5.pkl', 'rb') as f:
        data = pickle.load(f)
    
    sequences = np.array(data['sequences'])
    # å–ç¬¬ä¸€ä¸ªåºåˆ—çš„ç¬¬ä¸€å¸§
    sc_train = sequences[0, 0]  # (20, 60)
    
    print(f"  ScanContextå½¢çŠ¶: {sc_train.shape}")
    print(f"  ScanContextèŒƒå›´: [{np.min(sc_train):.4f}, {np.max(sc_train):.4f}]")
    print(f"  ScanContextå‡å€¼: {np.mean(sc_train):.4f}")
    print(f"  ScanContextæ ‡å‡†å·®: {np.std(sc_train):.4f}")

def main():
    """ä¸»å‡½æ•°"""
    
    # åˆ†æè®­ç»ƒæ•°æ®
    sequences, labels = analyze_training_data()
    
    # åˆ†ææµ‹è¯•æ•°æ®
    analyze_test_data()
    
    # å¯¹æ¯”ScanContextç”Ÿæˆ
    compare_scancontext_generation()
    
    print("\n" + "="*60)
    print("é—®é¢˜è¯Šæ–­å’Œè§£å†³æ–¹æ¡ˆ")
    print("="*60)
    
    print("å¯èƒ½çš„é—®é¢˜:")
    print("1. ğŸ” æ•°æ®ä¸åŒ¹é…: è®­ç»ƒç”¨çš„æ˜¯plyæ–‡ä»¶ï¼Œæµ‹è¯•ç”¨çš„æ˜¯rosbag")
    print("2. ğŸ” é¢„å¤„ç†ä¸ä¸€è‡´: ScanContextç”Ÿæˆå‚æ•°å¯èƒ½ä¸åŒ")
    print("3. ğŸ” åæ ‡ç³»å·®å¼‚: plyå’Œrosbagçš„åæ ‡ç³»å¯èƒ½ä¸åŒ")
    print("4. ğŸ” æ•°æ®åˆ†å¸ƒåç§»: è®­ç»ƒå’Œæµ‹è¯•çš„ç‰¹å¾åˆ†å¸ƒä¸åŒ¹é…")
    
    print("\nè§£å†³æ–¹æ¡ˆ:")
    print("1. âœ… æ£€æŸ¥ScanContextç”Ÿæˆå‚æ•°æ˜¯å¦ä¸€è‡´")
    print("2. âœ… å¯¹æ¯”è®­ç»ƒå’Œæµ‹è¯•çš„ç‰¹å¾åˆ†å¸ƒ")
    print("3. âœ… ä½¿ç”¨ç›¸åŒçš„é¢„å¤„ç†æµç¨‹")
    print("4. âœ… è€ƒè™‘ç‰¹å¾å½’ä¸€åŒ–æˆ–æ ‡å‡†åŒ–")

if __name__ == '__main__':
    main()
