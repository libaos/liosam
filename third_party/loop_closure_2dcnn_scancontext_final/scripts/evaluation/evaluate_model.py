#!/usr/bin/env python3
"""
é€šç”¨æ¨¡å‹è¯„ä¼°è„šæœ¬
æ”¯æŒæ‰€æœ‰æ¨¡å‹ç±»å‹çš„è¯„ä¼°
"""
import argparse
import torch
from torch.utils.data import DataLoader
from pathlib import Path
import json
import time
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from config import get_config
from models import SCRingCNN, SCStandardCNN, SCStandardCNNLite, SimpleCNN, SimpleCNNLite
from utils import SimpleLoopClosureDataset, setup_model_logger, get_timestamp, calculate_metrics

def load_model(model_path, device='cpu'):
    """åŠ è½½æ¨¡å‹"""
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
    
    # è·å–æ¨¡å‹ç±»å‹
    model_type = checkpoint.get('model_type', 'SCRingCNN')
    config = checkpoint.get('config', {})
    
    # åˆ›å»ºå¯¹åº”çš„æ¨¡å‹
    if model_type == 'SCRingCNN':
        model = SCRingCNN(
            num_rings=config.get('num_rings', 20),
            num_sectors=config.get('num_sectors', 60),
            descriptor_dim=config.get('descriptor_dim', 256)
        )
    elif model_type == 'SCStandardCNN':
        model = SCStandardCNN(
            num_rings=config.get('num_rings', 20),
            num_sectors=config.get('num_sectors', 60),
            descriptor_dim=config.get('descriptor_dim', 256),
            use_residual=config.get('use_residual', True)
        )
    elif model_type == 'SCStandardCNNLite':
        model = SCStandardCNNLite(
            num_rings=config.get('num_rings', 20),
            num_sectors=config.get('num_sectors', 60),
            descriptor_dim=config.get('descriptor_dim', 128)
        )
    elif model_type == 'SimpleCNN':
        model = SimpleCNN(
            num_rings=config.get('num_rings', 20),
            num_sectors=config.get('num_sectors', 60),
            descriptor_dim=config.get('descriptor_dim', 256)
        )
    elif model_type == 'SimpleCNNLite':
        model = SimpleCNNLite(
            num_rings=config.get('num_rings', 20),
            num_sectors=config.get('num_sectors', 60),
            descriptor_dim=config.get('descriptor_dim', 128)
        )
    else:
        raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_type}")
    
    # åŠ è½½æƒé‡
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    
    return model, model_type, config

def evaluate_model(model, dataloader, device, logger, model_type):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    model.eval()
    all_features = []
    all_labels = []
    
    # è®¡æ—¶
    start_time = time.time()
    inference_times = []
    
    with torch.no_grad():
        for batch_idx, (data, labels) in enumerate(dataloader):
            data = data.to(device)
            
            # å•æ‰¹æ¬¡æ¨ç†è®¡æ—¶
            batch_start = time.time()
            features = model(data)
            batch_time = time.time() - batch_start
            inference_times.append(batch_time)
            
            all_features.append(features.cpu())
            all_labels.append(labels)
    
    total_time = time.time() - start_time
    
    # åˆå¹¶æ‰€æœ‰ç‰¹å¾å’Œæ ‡ç­¾
    all_features = torch.cat(all_features, dim=0)
    all_labels = torch.cat(all_labels, dim=0)
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    metrics = calculate_metrics(all_features, all_labels)
    
    # æ·»åŠ æ€§èƒ½æŒ‡æ ‡
    metrics['total_inference_time'] = total_time
    metrics['avg_batch_time'] = sum(inference_times) / len(inference_times)
    metrics['samples_per_second'] = len(all_features) / total_time
    
    # è®°å½•ç»“æœ
    logger.info(f"{model_type} è¯„ä¼°ç»“æœ:")
    logger.info("="*50)
    
    # å‡†ç¡®ç‡æŒ‡æ ‡
    logger.info("å‡†ç¡®ç‡æŒ‡æ ‡:")
    for key in ['top_1', 'top_3', 'top_5', 'top_10']:
        if key in metrics:
            logger.info(f"  {key}: {metrics[key]:.4f}")
    
    # æ’åºè´¨é‡æŒ‡æ ‡
    logger.info("æ’åºè´¨é‡æŒ‡æ ‡:")
    for key in ['mAP', 'MRR']:
        if key in metrics:
            logger.info(f"  {key}: {metrics[key]:.4f}")
    
    # ç‰¹å¾è´¨é‡æŒ‡æ ‡
    logger.info("ç‰¹å¾è´¨é‡æŒ‡æ ‡:")
    for key in ['separation_ratio', 'classification_accuracy']:
        if key in metrics:
            logger.info(f"  {key}: {metrics[key]:.4f}")
    
    # æ€§èƒ½æŒ‡æ ‡
    logger.info("æ€§èƒ½æŒ‡æ ‡:")
    logger.info(f"  æ€»æ¨ç†æ—¶é—´: {metrics['total_inference_time']:.4f}s")
    logger.info(f"  å¹³å‡æ‰¹æ¬¡æ—¶é—´: {metrics['avg_batch_time']:.4f}s")
    logger.info(f"  å¤„ç†é€Ÿåº¦: {metrics['samples_per_second']:.2f} samples/s")
    
    return metrics

def main():
    parser = argparse.ArgumentParser(description='é€šç”¨æ¨¡å‹è¯„ä¼°è„šæœ¬')
    parser.add_argument('--model', type=str, required=True,
                       help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data_dir', type=str, default=None,
                       help='æµ‹è¯•æ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--batch_size', type=int, default=32,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--max_files', type=int, default=None,
                       help='æœ€å¤§æ–‡ä»¶æ•°é‡')
    parser.add_argument('--device', type=str, default='cpu',
                       help='è®¾å¤‡ç±»å‹')
    parser.add_argument('--output', type=str, default=None,
                       help='è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--detailed', action='store_true',
                       help='æ˜¾ç¤ºè¯¦ç»†çš„è¯„ä¼°ä¿¡æ¯')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    timestamp = get_timestamp()
    project_root = Path(__file__).parent.parent.parent

    # å…ˆå°è¯•ä»æ¨¡å‹æ–‡ä»¶æ¨æ–­æ¨¡å‹ç±»å‹
    model_type = 'general'
    try:
        if Path(args.model).exists():
            checkpoint = torch.load(args.model, map_location='cpu', weights_only=False)
            model_type_from_checkpoint = checkpoint.get('model_type', 'general')
            if model_type_from_checkpoint in ['SCRingCNN', 'SCStandardCNN', 'SimpleCNN', 'SimpleCNNLite']:
                model_type = model_type_from_checkpoint.lower().replace('cnn', '_cnn')
    except:
        pass

    logger, log_file = setup_model_logger(
        model_type=model_type,
        script_type='evaluation',
        timestamp=timestamp,
        project_root=project_root
    )
    
    logger.info("ğŸ” å¼€å§‹æ¨¡å‹è¯„ä¼°")
    logger.info(f"æ¨¡å‹æ–‡ä»¶: {args.model}")
    logger.info(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    logger.info(f"è®¾å¤‡: {args.device}")
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    model_path = Path(args.model)
    if not model_path.exists():
        logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return
    
    # æ•°æ®ç›®å½•
    if args.data_dir:
        data_dir = Path(args.data_dir)
    else:
        data_dir = project_root / "data" / "raw" / "ply_files"
    
    if not data_dir.exists():
        logger.error(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return
    
    # åŠ è½½æ¨¡å‹
    logger.info("åŠ è½½æ¨¡å‹...")
    try:
        model, model_type, model_config = load_model(args.model, args.device)
        logger.info(f"æ¨¡å‹ç±»å‹: {model_type}")
        logger.info(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        if args.detailed:
            logger.info(f"æ¨¡å‹é…ç½®: {model_config}")
    except Exception as e:
        logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return
    
    # åˆ›å»ºæ•°æ®é›†
    logger.info("åŠ è½½æµ‹è¯•æ•°æ®é›†...")
    dataset = SimpleLoopClosureDataset(
        data_dir=data_dir,
        cache_dir=project_root / "data" / "cache",
        max_files=args.max_files
    )
    
    dataloader = DataLoader(
        dataset, 
        batch_size=args.batch_size, 
        shuffle=False
    )
    
    logger.info(f"æµ‹è¯•é›†å¤§å°: {len(dataset)}")
    
    # è¯„ä¼°æ¨¡å‹
    logger.info("å¼€å§‹è¯„ä¼°...")
    metrics = evaluate_model(model, dataloader, torch.device(args.device), logger, model_type)
    
    # ä¿å­˜ç»“æœ
    results = {
        'model_path': str(model_path),
        'model_type': model_type,
        'model_config': model_config,
        'data_dir': str(data_dir),
        'dataset_size': len(dataset),
        'evaluation_config': {
            'batch_size': args.batch_size,
            'device': args.device,
            'max_files': args.max_files
        },
        'metrics': metrics,
        'timestamp': timestamp
    }
    
    # è¾“å‡ºæ–‡ä»¶è·¯å¾„
    if args.output:
        results_path = Path(args.output)
    else:
        results_dir = project_root / "outputs" / "results"
        results_dir.mkdir(parents=True, exist_ok=True)
        results_path = results_dir / f"evaluation_{model_type.lower()}_{timestamp}.json"
    
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    logger.info(f"è¯„ä¼°å®Œæˆï¼ç»“æœä¿å­˜è‡³: {results_path}")
    
    # æ˜¾ç¤ºæ€»ç»“
    logger.info("\n" + "="*60)
    logger.info("è¯„ä¼°æ€»ç»“:")
    logger.info("="*60)
    logger.info(f"æ¨¡å‹ç±»å‹: {model_type}")
    logger.info(f"Top-1å‡†ç¡®ç‡: {metrics.get('top_1', 0):.4f}")
    logger.info(f"mAP: {metrics.get('mAP', 0):.4f}")
    logger.info(f"MRR: {metrics.get('MRR', 0):.4f}")
    logger.info(f"å¤„ç†é€Ÿåº¦: {metrics.get('samples_per_second', 0):.2f} samples/s")

if __name__ == "__main__":
    main()
