#!/usr/bin/env python3
"""
æ—¥å¿—ç®¡ç†å·¥å…·
ç”¨äºæŸ¥çœ‹ã€æ¸…ç†å’Œåˆ†ææ—¥å¿—æ–‡ä»¶
"""
import argparse
from pathlib import Path
import json
from datetime import datetime
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from utils.logger import create_log_structure_info

def list_logs(log_dir, model_type=None, script_type=None):
    """åˆ—å‡ºæ—¥å¿—æ–‡ä»¶"""
    log_dir = Path(log_dir)
    
    if not log_dir.exists():
        print(f"æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {log_dir}")
        return
    
    print("ğŸ“‹ æ—¥å¿—æ–‡ä»¶åˆ—è¡¨")
    print("=" * 80)
    
    # éå†æ—¥å¿—ç›®å½•
    for model_dir in sorted(log_dir.iterdir()):
        if not model_dir.is_dir():
            continue
        
        # è¿‡æ»¤æ¨¡å‹ç±»å‹
        if model_type and model_type != model_dir.name:
            continue
        
        print(f"\nğŸ“ {model_dir.name}/")
        
        for script_dir in sorted(model_dir.iterdir()):
            if not script_dir.is_dir():
                continue
            
            # è¿‡æ»¤è„šæœ¬ç±»å‹
            if script_type and script_type != script_dir.name:
                continue
            
            log_files = list(script_dir.glob("*.log"))
            if log_files:
                print(f"  ğŸ“‚ {script_dir.name}/ ({len(log_files)} ä¸ªæ—¥å¿—)")
                
                for log_file in sorted(log_files, key=lambda x: x.stat().st_mtime, reverse=True):
                    # è·å–æ–‡ä»¶ä¿¡æ¯
                    stat = log_file.stat()
                    size_mb = stat.st_size / (1024 * 1024)
                    mtime = datetime.fromtimestamp(stat.st_mtime)
                    
                    print(f"    ğŸ“„ {log_file.name}")
                    print(f"       å¤§å°: {size_mb:.2f} MB, ä¿®æ”¹æ—¶é—´: {mtime.strftime('%Y-%m-%d %H:%M:%S')}")

def clean_logs(log_dir, days=7, dry_run=True):
    """æ¸…ç†æ—§æ—¥å¿—æ–‡ä»¶"""
    log_dir = Path(log_dir)
    
    if not log_dir.exists():
        print(f"æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {log_dir}")
        return
    
    from datetime import timedelta
    cutoff_time = datetime.now() - timedelta(days=days)
    
    print(f"ğŸ§¹ æ¸…ç† {days} å¤©å‰çš„æ—¥å¿—æ–‡ä»¶")
    print(f"æˆªæ­¢æ—¶é—´: {cutoff_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    deleted_count = 0
    total_size = 0
    
    # éå†æ‰€æœ‰æ—¥å¿—æ–‡ä»¶
    for log_file in log_dir.rglob("*.log"):
        stat = log_file.stat()
        mtime = datetime.fromtimestamp(stat.st_mtime)
        
        if mtime < cutoff_time:
            size_mb = stat.st_size / (1024 * 1024)
            total_size += stat.st_size
            
            print(f"{'[DRY RUN] ' if dry_run else ''}åˆ é™¤: {log_file.relative_to(log_dir)}")
            print(f"  å¤§å°: {size_mb:.2f} MB, ä¿®æ”¹æ—¶é—´: {mtime.strftime('%Y-%m-%d %H:%M:%S')}")
            
            if not dry_run:
                log_file.unlink()
            
            deleted_count += 1
    
    total_size_mb = total_size / (1024 * 1024)
    print(f"\n{'é¢„è®¡' if dry_run else 'å®é™…'}åˆ é™¤ {deleted_count} ä¸ªæ–‡ä»¶ï¼Œé‡Šæ”¾ {total_size_mb:.2f} MB ç©ºé—´")
    
    if dry_run:
        print("\nğŸ’¡ ä½¿ç”¨ --no-dry-run å‚æ•°å®é™…æ‰§è¡Œåˆ é™¤æ“ä½œ")

def analyze_logs(log_dir, model_type=None):
    """åˆ†ææ—¥å¿—æ–‡ä»¶"""
    log_dir = Path(log_dir)
    
    if not log_dir.exists():
        print(f"æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {log_dir}")
        return
    
    print("ğŸ“Š æ—¥å¿—åˆ†ææŠ¥å‘Š")
    print("=" * 80)
    
    stats = {}
    
    # éå†æ—¥å¿—ç›®å½•
    for model_dir in log_dir.iterdir():
        if not model_dir.is_dir():
            continue
        
        # è¿‡æ»¤æ¨¡å‹ç±»å‹
        if model_type and model_type != model_dir.name:
            continue
        
        model_name = model_dir.name
        stats[model_name] = {
            'training': {'count': 0, 'size': 0},
            'evaluation': {'count': 0, 'size': 0},
            'prediction': {'count': 0, 'size': 0}
        }
        
        for script_dir in model_dir.iterdir():
            if not script_dir.is_dir():
                continue
            
            script_type = script_dir.name
            if script_type not in stats[model_name]:
                stats[model_name][script_type] = {'count': 0, 'size': 0}
            
            for log_file in script_dir.glob("*.log"):
                stats[model_name][script_type]['count'] += 1
                stats[model_name][script_type]['size'] += log_file.stat().st_size
    
    # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
    for model_name, model_stats in stats.items():
        print(f"\nğŸ“ {model_name}")
        print("-" * 40)
        
        total_count = 0
        total_size = 0
        
        for script_type, script_stats in model_stats.items():
            count = script_stats['count']
            size_mb = script_stats['size'] / (1024 * 1024)
            
            if count > 0:
                print(f"  {script_type:12}: {count:3d} ä¸ªæ–‡ä»¶, {size_mb:6.2f} MB")
                total_count += count
                total_size += script_stats['size']
        
        total_size_mb = total_size / (1024 * 1024)
        print(f"  {'æ€»è®¡':12}: {total_count:3d} ä¸ªæ–‡ä»¶, {total_size_mb:6.2f} MB")

def show_structure():
    """æ˜¾ç¤ºæ—¥å¿—ç›®å½•ç»“æ„"""
    print("ğŸ“ æ—¥å¿—ç›®å½•ç»“æ„è¯´æ˜")
    print("=" * 80)
    print(create_log_structure_info())

def main():
    parser = argparse.ArgumentParser(description='æ—¥å¿—ç®¡ç†å·¥å…·')
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # åˆ—å‡ºæ—¥å¿—
    list_parser = subparsers.add_parser('list', help='åˆ—å‡ºæ—¥å¿—æ–‡ä»¶')
    list_parser.add_argument('--model', type=str, help='è¿‡æ»¤æ¨¡å‹ç±»å‹')
    list_parser.add_argument('--script', type=str, help='è¿‡æ»¤è„šæœ¬ç±»å‹')
    list_parser.add_argument('--log-dir', type=str, default='outputs/logs', help='æ—¥å¿—ç›®å½•')
    
    # æ¸…ç†æ—¥å¿—
    clean_parser = subparsers.add_parser('clean', help='æ¸…ç†æ—§æ—¥å¿—æ–‡ä»¶')
    clean_parser.add_argument('--days', type=int, default=7, help='ä¿ç•™å¤©æ•°')
    clean_parser.add_argument('--no-dry-run', action='store_true', help='å®é™…æ‰§è¡Œåˆ é™¤')
    clean_parser.add_argument('--log-dir', type=str, default='outputs/logs', help='æ—¥å¿—ç›®å½•')
    
    # åˆ†ææ—¥å¿—
    analyze_parser = subparsers.add_parser('analyze', help='åˆ†ææ—¥å¿—æ–‡ä»¶')
    analyze_parser.add_argument('--model', type=str, help='è¿‡æ»¤æ¨¡å‹ç±»å‹')
    analyze_parser.add_argument('--log-dir', type=str, default='outputs/logs', help='æ—¥å¿—ç›®å½•')
    
    # æ˜¾ç¤ºç»“æ„
    subparsers.add_parser('structure', help='æ˜¾ç¤ºæ—¥å¿—ç›®å½•ç»“æ„')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    project_root = Path(__file__).parent.parent
    
    if args.command == 'list':
        log_dir = project_root / args.log_dir
        list_logs(log_dir, args.model, args.script)
    
    elif args.command == 'clean':
        log_dir = project_root / args.log_dir
        clean_logs(log_dir, args.days, not args.no_dry_run)
    
    elif args.command == 'analyze':
        log_dir = project_root / args.log_dir
        analyze_logs(log_dir, args.model)
    
    elif args.command == 'structure':
        show_structure()

if __name__ == "__main__":
    main()
