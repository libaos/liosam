#!/usr/bin/env python3
"""
å›ç¯æ£€æµ‹é¢„æµ‹å™¨
æ”¯æŒå•ä¸ªæŸ¥è¯¢å’Œæ‰¹é‡æŸ¥è¯¢çš„å›ç¯æ£€æµ‹
"""
import argparse
import torch
import numpy as np
from pathlib import Path
import json
import time
import sys
from typing import List, Dict, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from config import get_config
from models import SCRingCNN, SCStandardCNN, SCStandardCNNLite, SimpleCNN, SimpleCNNLite
from utils import ScanContext, PLYReader, setup_model_logger, get_timestamp

class LoopClosureDetector:
    """å›ç¯æ£€æµ‹å™¨"""
    
    def __init__(self, model_path: str, device: str = 'cpu'):
        """
        åˆå§‹åŒ–å›ç¯æ£€æµ‹å™¨
        
        å‚æ•°:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            device: è®¾å¤‡ç±»å‹
        """
        self.device = torch.device(device)
        self.model = None
        self.model_type = None
        self.config = None
        self.sc_generator = None
        
        # åŠ è½½æ¨¡å‹
        self._load_model(model_path)
        
        # åˆ›å»ºScanContextç”Ÿæˆå™¨
        self.sc_generator = ScanContext()
        
    def _load_model(self, model_path: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
        
        # è·å–é…ç½®
        self.config = checkpoint.get('config', {})
        self.model_type = checkpoint.get('model_type', 'SCRingCNN')
        
        # åˆ›å»ºæ¨¡å‹
        if self.model_type == 'SCRingCNN':
            self.model = SCRingCNN(
                num_rings=self.config.get('num_rings', 20),
                num_sectors=self.config.get('num_sectors', 60),
                descriptor_dim=self.config.get('descriptor_dim', 256)
            )
        elif self.model_type == 'SCStandardCNN':
            self.model = SCStandardCNN(
                num_rings=self.config.get('num_rings', 20),
                num_sectors=self.config.get('num_sectors', 60),
                descriptor_dim=self.config.get('descriptor_dim', 256),
                use_residual=self.config.get('use_residual', True)
            )
        elif self.model_type == 'SCStandardCNNLite':
            self.model = SCStandardCNNLite(
                num_rings=self.config.get('num_rings', 20),
                num_sectors=self.config.get('num_sectors', 60),
                descriptor_dim=self.config.get('descriptor_dim', 128)
            )
        elif self.model_type == 'SimpleCNN':
            self.model = SimpleCNN(
                num_rings=self.config.get('num_rings', 20),
                num_sectors=self.config.get('num_sectors', 60),
                descriptor_dim=self.config.get('descriptor_dim', 256)
            )
        elif self.model_type == 'SimpleCNNLite':
            self.model = SimpleCNNLite(
                num_rings=self.config.get('num_rings', 20),
                num_sectors=self.config.get('num_sectors', 60),
                descriptor_dim=self.config.get('descriptor_dim', 128)
            )
        else:
            raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {self.model_type}")
        
        # åŠ è½½æƒé‡
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_type}")
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
    
    def extract_descriptor(self, ply_file_path: str) -> Tuple[np.ndarray, np.ndarray]:
        """
        ä»PLYæ–‡ä»¶æå–æè¿°å­
        
        å‚æ•°:
            ply_file_path: PLYæ–‡ä»¶è·¯å¾„
            
        è¿”å›:
            descriptor: æè¿°å­å‘é‡
            scan_context: ScanContextç‰¹å¾å›¾
        """
        # è¯»å–ç‚¹äº‘
        points = PLYReader.read_ply_file(ply_file_path)
        
        # ç”ŸæˆScanContext
        scan_context = self.sc_generator.make_scan_context(points)
        
        # è½¬æ¢ä¸ºtensor
        sc_tensor = torch.from_numpy(scan_context).unsqueeze(0).unsqueeze(0).float()
        sc_tensor = sc_tensor.to(self.device)
        
        # æå–æè¿°å­
        with torch.no_grad():
            descriptor = self.model(sc_tensor)
            descriptor = descriptor.cpu().numpy().flatten()
        
        return descriptor, scan_context
    
    def calculate_similarity(self, desc1: np.ndarray, desc2: np.ndarray) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæè¿°å­ä¹‹é—´çš„ç›¸ä¼¼åº¦
        
        å‚æ•°:
            desc1: ç¬¬ä¸€ä¸ªæè¿°å­
            desc2: ç¬¬äºŒä¸ªæè¿°å­
            
        è¿”å›:
            similarity: ä½™å¼¦ç›¸ä¼¼åº¦
        """
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        dot_product = np.dot(desc1, desc2)
        norm1 = np.linalg.norm(desc1)
        norm2 = np.linalg.norm(desc2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        similarity = dot_product / (norm1 * norm2)
        return float(similarity)
    
    def detect_loop_closure(self, query_ply: str, database_plys: List[str], 
                          threshold: float = 0.8, top_k: int = 10) -> Dict:
        """
        æ£€æµ‹å›ç¯
        
        å‚æ•°:
            query_ply: æŸ¥è¯¢PLYæ–‡ä»¶è·¯å¾„
            database_plys: æ•°æ®åº“PLYæ–‡ä»¶è·¯å¾„åˆ—è¡¨
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            top_k: è¿”å›å‰kä¸ªç»“æœ
            
        è¿”å›:
            results: å›ç¯æ£€æµ‹ç»“æœ
        """
        start_time = time.time()
        
        # æå–æŸ¥è¯¢æè¿°å­
        print(f"ğŸ” å¤„ç†æŸ¥è¯¢æ–‡ä»¶: {Path(query_ply).name}")
        query_desc, query_sc = self.extract_descriptor(query_ply)
        
        results = []
        
        print(f"ğŸ“Š å¤„ç†æ•°æ®åº“æ–‡ä»¶: {len(database_plys)} ä¸ª")
        for i, db_ply in enumerate(database_plys):
            try:
                # æ˜¾ç¤ºè¿›åº¦
                if (i + 1) % 50 == 0 or i == len(database_plys) - 1:
                    print(f"  è¿›åº¦: {i + 1}/{len(database_plys)}")
                
                # æå–æ•°æ®åº“æè¿°å­
                db_desc, db_sc = self.extract_descriptor(db_ply)
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = self.calculate_similarity(query_desc, db_desc)
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºå›ç¯
                is_loop = similarity > threshold
                
                results.append({
                    'database_file': str(db_ply),
                    'database_name': Path(db_ply).name,
                    'similarity': similarity,
                    'is_loop': is_loop
                })
                
            except Exception as e:
                print(f"âš ï¸  å¤„ç†æ–‡ä»¶å¤±è´¥ {Path(db_ply).name}: {e}")
                continue
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        results.sort(key=lambda x: x['similarity'], reverse=True)
        
        # ç»Ÿè®¡ç»“æœ
        total_time = time.time() - start_time
        loop_count = sum(1 for r in results if r['is_loop'])
        
        # è¿”å›ç»“æœ
        detection_results = {
            'query_file': str(query_ply),
            'query_name': Path(query_ply).name,
            'model_type': self.model_type,
            'database_size': len(database_plys),
            'threshold': threshold,
            'processing_time': total_time,
            'loop_candidates': loop_count,
            'top_results': results[:top_k],
            'all_results': results,
            'statistics': {
                'max_similarity': max(r['similarity'] for r in results) if results else 0,
                'min_similarity': min(r['similarity'] for r in results) if results else 0,
                'avg_similarity': sum(r['similarity'] for r in results) / len(results) if results else 0,
                'processing_speed': len(database_plys) / total_time if total_time > 0 else 0
            }
        }
        
        return detection_results
    
    def batch_detect(self, query_dir: str, database_dir: str, 
                    threshold: float = 0.8, top_k: int = 5) -> Dict:
        """
        æ‰¹é‡å›ç¯æ£€æµ‹
        
        å‚æ•°:
            query_dir: æŸ¥è¯¢æ–‡ä»¶ç›®å½•
            database_dir: æ•°æ®åº“æ–‡ä»¶ç›®å½•
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            top_k: æ¯ä¸ªæŸ¥è¯¢è¿”å›å‰kä¸ªç»“æœ
            
        è¿”å›:
            batch_results: æ‰¹é‡æ£€æµ‹ç»“æœ
        """
        query_dir = Path(query_dir)
        database_dir = Path(database_dir)
        
        # è·å–æ–‡ä»¶åˆ—è¡¨
        query_plys = list(query_dir.glob("*.ply"))
        database_plys = list(database_dir.glob("*.ply"))
        
        print(f"ğŸ” æ‰¹é‡å›ç¯æ£€æµ‹")
        print(f"  æŸ¥è¯¢æ–‡ä»¶: {len(query_plys)} ä¸ª")
        print(f"  æ•°æ®åº“æ–‡ä»¶: {len(database_plys)} ä¸ª")
        
        batch_results = {
            'query_dir': str(query_dir),
            'database_dir': str(database_dir),
            'model_type': self.model_type,
            'threshold': threshold,
            'top_k': top_k,
            'query_count': len(query_plys),
            'database_count': len(database_plys),
            'results': []
        }
        
        start_time = time.time()
        
        for i, query_ply in enumerate(query_plys):
            print(f"\nğŸ“ å¤„ç†æŸ¥è¯¢ {i + 1}/{len(query_plys)}: {query_ply.name}")
            
            try:
                result = self.detect_loop_closure(
                    str(query_ply), 
                    [str(p) for p in database_plys], 
                    threshold, 
                    top_k
                )
                batch_results['results'].append(result)
                
            except Exception as e:
                print(f"âš ï¸  æŸ¥è¯¢å¤±è´¥ {query_ply.name}: {e}")
                continue
        
        batch_results['total_time'] = time.time() - start_time
        batch_results['avg_time_per_query'] = batch_results['total_time'] / len(query_plys) if query_plys else 0
        
        return batch_results

def main():
    parser = argparse.ArgumentParser(description='å›ç¯æ£€æµ‹é¢„æµ‹å™¨')
    parser.add_argument('--model', type=str, required=True,
                       help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--query', type=str, required=True,
                       help='æŸ¥è¯¢PLYæ–‡ä»¶è·¯å¾„æˆ–ç›®å½•')
    parser.add_argument('--database', type=str, required=True,
                       help='æ•°æ®åº“ç›®å½•è·¯å¾„')
    parser.add_argument('--threshold', type=float, default=0.8,
                       help='ç›¸ä¼¼åº¦é˜ˆå€¼')
    parser.add_argument('--top_k', type=int, default=10,
                       help='è¿”å›å‰kä¸ªç»“æœ')
    parser.add_argument('--output', type=str, default=None,
                       help='è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--device', type=str, default='cpu',
                       help='è®¾å¤‡ç±»å‹')
    parser.add_argument('--batch', action='store_true',
                       help='æ‰¹é‡æ¨¡å¼ï¼ˆæŸ¥è¯¢ä¸ºç›®å½•ï¼‰')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    timestamp = get_timestamp()
    project_root = Path(__file__).parent.parent.parent

    # ä»æ¨¡å‹æ–‡ä»¶æ¨æ–­æ¨¡å‹ç±»å‹
    model_type = 'general'
    try:
        if Path(args.model).exists():
            checkpoint = torch.load(args.model, map_location='cpu', weights_only=False)
            model_type_from_checkpoint = checkpoint.get('model_type', 'general')
            if model_type_from_checkpoint in ['SCRingCNN', 'SCStandardCNN', 'SimpleCNN', 'SimpleCNNLite']:
                model_type = model_type_from_checkpoint.lower().replace('cnn', '_cnn')
    except:
        pass

    logger, log_file = setup_model_logger(
        model_type=model_type,
        script_type='prediction',
        timestamp=timestamp,
        project_root=project_root
    )
    
    print("ğŸš€ å›ç¯æ£€æµ‹é¢„æµ‹å™¨")
    print("="*50)
    print(f"æ¨¡å‹: {args.model}")
    print(f"æŸ¥è¯¢: {args.query}")
    print(f"æ•°æ®åº“: {args.database}")
    print(f"é˜ˆå€¼: {args.threshold}")
    print(f"è®¾å¤‡: {args.device}")
    print(f"æ‰¹é‡æ¨¡å¼: {args.batch}")
    
    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    if not Path(args.model).exists():
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model}")
        return
    
    if not Path(args.query).exists():
        print(f"âŒ æŸ¥è¯¢æ–‡ä»¶/ç›®å½•ä¸å­˜åœ¨: {args.query}")
        return
    
    if not Path(args.database).exists():
        print(f"âŒ æ•°æ®åº“ç›®å½•ä¸å­˜åœ¨: {args.database}")
        return
    
    # åˆ›å»ºæ£€æµ‹å™¨
    print("\nğŸ“¥ åŠ è½½æ¨¡å‹...")
    detector = LoopClosureDetector(args.model, args.device)
    
    # æ‰§è¡Œæ£€æµ‹
    if args.batch:
        # æ‰¹é‡æ£€æµ‹
        print("\nğŸ”„ å¼€å§‹æ‰¹é‡å›ç¯æ£€æµ‹...")
        results = detector.batch_detect(
            args.query, args.database, args.threshold, args.top_k
        )
    else:
        # å•ä¸ªæ£€æµ‹
        database_plys = list(Path(args.database).glob("*.ply"))
        print(f"\nğŸ” å¼€å§‹å›ç¯æ£€æµ‹...")
        print(f"æ•°æ®åº“ä¸­æœ‰ {len(database_plys)} ä¸ªPLYæ–‡ä»¶")
        
        results = detector.detect_loop_closure(
            args.query, [str(p) for p in database_plys], args.threshold, args.top_k
        )
    
    # æ˜¾ç¤ºç»“æœ
    if args.batch:
        print(f"\nğŸ“Š æ‰¹é‡æ£€æµ‹å®Œæˆ")
        print(f"å¤„ç†æŸ¥è¯¢: {results['query_count']} ä¸ª")
        print(f"æ€»è€—æ—¶: {results['total_time']:.2f}s")
        print(f"å¹³å‡æ¯æŸ¥è¯¢: {results['avg_time_per_query']:.2f}s")
    else:
        print(f"\nğŸ“Š æ£€æµ‹å®Œæˆ")
        print(f"å¤„ç†æ—¶é—´: {results['processing_time']:.2f}s")
        print(f"å›ç¯å€™é€‰: {results['loop_candidates']} ä¸ª")
        print(f"å¤„ç†é€Ÿåº¦: {results['statistics']['processing_speed']:.2f} files/s")
        
        print(f"\nğŸ† Top {args.top_k} ç›¸ä¼¼ç»“æœ:")
        for i, result in enumerate(results['top_results']):
            status = "âœ… å›ç¯" if result['is_loop'] else "âŒ éå›ç¯"
            print(f"  {i+1}. {result['database_name']}: "
                  f"ç›¸ä¼¼åº¦={result['similarity']:.4f} {status}")
    
    # ä¿å­˜ç»“æœ
    if args.output:
        output_path = Path(args.output)
    else:
        results_dir = project_root / "outputs" / "results"
        results_dir.mkdir(parents=True, exist_ok=True)
        
        if args.batch:
            output_path = results_dir / f"batch_loop_closure_{timestamp}.json"
        else:
            query_name = Path(args.query).stem
            output_path = results_dir / f"loop_closure_{query_name}_{timestamp}.json"
    
    # æ·»åŠ æ—¶é—´æˆ³
    results['timestamp'] = timestamp
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nğŸ’¾ ç»“æœä¿å­˜è‡³: {output_path}")

if __name__ == "__main__":
    main()
