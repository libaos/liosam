#!/usr/bin/env python3
"""
åŸºäºç‚¹äº‘çš„ä½ç½®å®šä½è„šæœ¬
ç”¨äºç¡®å®šå½“å‰ä½ç½®åœ¨é¢„å»ºåœ°å›¾ä¸­çš„ç´¢å¼•ä½ç½®
"""
import argparse
import torch
import numpy as np
from pathlib import Path
import json
import time
import pickle
from typing import List, Dict, Tuple, Optional

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from config import get_config
from models import SCRingCNN, SCStandardCNN, SCStandardCNNLite, SimpleCNN, SimpleCNNLite
from utils import ScanContext, PLYReader, setup_logger, get_timestamp

class PositionLocalizer:
    """ä½ç½®å®šä½å™¨ - ç”¨äºåœ¨é¢„å»ºåœ°å›¾ä¸­å®šä½å½“å‰ä½ç½®"""
    
    def __init__(self, model_path: str, map_database_path: str, device: str = 'cpu'):
        """
        åˆå§‹åŒ–ä½ç½®å®šä½å™¨
        
        å‚æ•°:
            model_path (str): è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„
            map_database_path (str): åœ°å›¾æ•°æ®åº“è·¯å¾„ï¼ˆåŒ…å«æ‰€æœ‰å‚è€ƒä½ç½®çš„PLYæ–‡ä»¶ï¼‰
            device (str): è®¾å¤‡ç±»å‹
        """
        self.device = torch.device(device)
        self.model = None
        self.config = None
        self.sc_generator = None
        self.map_database = {}  # å­˜å‚¨åœ°å›¾æ•°æ®åº“ {ä½ç½®ç´¢å¼•: æè¿°å­}
        self.map_files = []     # å­˜å‚¨æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        
        # åŠ è½½æ¨¡å‹
        self._load_model(model_path)
        
        # åˆ›å»ºScanContextç”Ÿæˆå™¨
        self.sc_generator = ScanContext()
        
        # åŠ è½½åœ°å›¾æ•°æ®åº“
        self._load_map_database(map_database_path)
        
    def _load_model(self, model_path: str):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        checkpoint = torch.load(model_path, map_location=self.device)
        
        # è·å–é…ç½®
        self.config = checkpoint.get('config', {})
        model_type = self.config.get('MODEL_TYPE', 'sc_ring_cnn')
        
        # åˆ›å»ºæ¨¡å‹
        if model_type == "simple_cnn":
            self.model = SimpleCNN(
                num_rings=self.config.get('INPUT_HEIGHT', 20),
                num_sectors=self.config.get('INPUT_WIDTH', 60),
                descriptor_dim=self.config.get('DESCRIPTOR_DIM', 256)
            )
        elif model_type == "simple_cnn_lite":
            self.model = SimpleCNNLite(
                num_rings=self.config.get('INPUT_HEIGHT', 20),
                num_sectors=self.config.get('INPUT_WIDTH', 60),
                descriptor_dim=self.config.get('DESCRIPTOR_DIM', 128)
            )
        elif model_type == "sc_standard_cnn":
            self.model = SCStandardCNN(
                num_rings=self.config.get('INPUT_HEIGHT', 20),
                num_sectors=self.config.get('INPUT_WIDTH', 60),
                descriptor_dim=self.config.get('DESCRIPTOR_DIM', 256)
            )
        elif model_type == "sc_standard_cnn_lite":
            self.model = SCStandardCNNLite(
                num_rings=self.config.get('INPUT_HEIGHT', 20),
                num_sectors=self.config.get('INPUT_WIDTH', 60),
                descriptor_dim=self.config.get('DESCRIPTOR_DIM', 128)
            )
        else:  # é»˜è®¤ä½¿ç”¨SCRingCNN
            self.model = SCRingCNN(
                num_rings=self.config.get('INPUT_HEIGHT', 20),
                num_sectors=self.config.get('INPUT_WIDTH', 60),
                descriptor_dim=self.config.get('DESCRIPTOR_DIM', 256)
            )
        
        # åŠ è½½æƒé‡
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_type}")
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
    
    def _load_map_database(self, map_database_path: str):
        """åŠ è½½åœ°å›¾æ•°æ®åº“"""
        map_path = Path(map_database_path)
        
        if not map_path.exists():
            raise FileNotFoundError(f"åœ°å›¾æ•°æ®åº“è·¯å¾„ä¸å­˜åœ¨: {map_database_path}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é¢„è®¡ç®—çš„æè¿°å­æ–‡ä»¶
        descriptor_cache_path = map_path / "descriptors_cache.pkl"
        
        if descriptor_cache_path.exists():
            print("ğŸ”„ åŠ è½½é¢„è®¡ç®—çš„æè¿°å­ç¼“å­˜...")
            with open(descriptor_cache_path, 'rb') as f:
                cache_data = pickle.load(f)
                self.map_database = cache_data['descriptors']
                self.map_files = cache_data['files']
            print(f"âœ… åŠ è½½äº† {len(self.map_database)} ä¸ªä½ç½®çš„æè¿°å­")
        else:
            print("ğŸ”„ é¦–æ¬¡è¿è¡Œï¼Œè®¡ç®—åœ°å›¾æ•°æ®åº“æè¿°å­...")
            self._compute_map_descriptors(map_path)
            
            # ä¿å­˜ç¼“å­˜
            cache_data = {
                'descriptors': self.map_database,
                'files': self.map_files
            }
            with open(descriptor_cache_path, 'wb') as f:
                pickle.dump(cache_data, f)
            print(f"ğŸ’¾ æè¿°å­ç¼“å­˜å·²ä¿å­˜åˆ°: {descriptor_cache_path}")
    
    def _compute_map_descriptors(self, map_path: Path):
        """è®¡ç®—åœ°å›¾æ•°æ®åº“ä¸­æ‰€æœ‰ä½ç½®çš„æè¿°å­"""
        ply_files = sorted(list(map_path.glob("*.ply")))
        
        if len(ply_files) == 0:
            raise ValueError(f"åœ°å›¾æ•°æ®åº“ä¸­æ²¡æœ‰PLYæ–‡ä»¶: {map_path}")
        
        print(f"ğŸ“ æ‰¾åˆ° {len(ply_files)} ä¸ªåœ°å›¾ä½ç½®æ–‡ä»¶")
        
        for i, ply_file in enumerate(ply_files):
            try:
                # æå–æè¿°å­
                descriptor, _ = self._extract_descriptor_from_ply(str(ply_file))
                
                # å­˜å‚¨åˆ°æ•°æ®åº“
                self.map_database[i] = descriptor
                self.map_files.append(str(ply_file))
                
                if (i + 1) % 50 == 0:
                    print(f"  å¤„ç†è¿›åº¦: {i + 1}/{len(ply_files)}")
                    
            except Exception as e:
                print(f"âš ï¸  å¤„ç†æ–‡ä»¶å¤±è´¥ {ply_file.name}: {e}")
                continue
        
        print(f"âœ… åœ°å›¾æ•°æ®åº“æ„å»ºå®Œæˆï¼Œå…± {len(self.map_database)} ä¸ªæœ‰æ•ˆä½ç½®")
    
    def _extract_descriptor_from_ply(self, ply_file_path: str) -> Tuple[np.ndarray, np.ndarray]:
        """ä»PLYæ–‡ä»¶æå–æè¿°å­"""
        # è¯»å–ç‚¹äº‘
        points = PLYReader.read_ply_file(ply_file_path)
        
        # ç”ŸæˆScanContext
        scan_context = self.sc_generator.make_scan_context(points)
        
        # è½¬æ¢ä¸ºtensor
        sc_tensor = torch.from_numpy(scan_context).unsqueeze(0).unsqueeze(0).float()
        sc_tensor = sc_tensor.to(self.device)
        
        # æå–æè¿°å­
        with torch.no_grad():
            descriptor = self.model(sc_tensor)
            descriptor = descriptor.cpu().numpy().flatten()
        
        return descriptor, scan_context
    
    def _calculate_similarity(self, desc1: np.ndarray, desc2: np.ndarray) -> float:
        """è®¡ç®—ä¸¤ä¸ªæè¿°å­ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        dot_product = np.dot(desc1, desc2)
        norm1 = np.linalg.norm(desc1)
        norm2 = np.linalg.norm(desc2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        similarity = dot_product / (norm1 * norm2)
        return similarity
    
    def localize_from_ply(self, query_ply_path: str, top_k: int = 5) -> Dict:
        """
        ä»PLYæ–‡ä»¶è¿›è¡Œä½ç½®å®šä½
        
        å‚æ•°:
            query_ply_path (str): æŸ¥è¯¢PLYæ–‡ä»¶è·¯å¾„
            top_k (int): è¿”å›å‰kä¸ªæœ€ç›¸ä¼¼çš„ä½ç½®
            
        è¿”å›:
            å®šä½ç»“æœå­—å…¸
        """
        # æå–æŸ¥è¯¢æè¿°å­
        query_desc, query_sc = self._extract_descriptor_from_ply(query_ply_path)
        
        return self._localize_from_descriptor(query_desc, top_k, query_ply_path)
    
    def localize_from_points(self, points: np.ndarray, top_k: int = 5) -> Dict:
        """
        ä»ç‚¹äº‘æ•°æ®è¿›è¡Œä½ç½®å®šä½
        
        å‚æ•°:
            points (np.ndarray): ç‚¹äº‘æ•°æ® (N, 3)
            top_k (int): è¿”å›å‰kä¸ªæœ€ç›¸ä¼¼çš„ä½ç½®
            
        è¿”å›:
            å®šä½ç»“æœå­—å…¸
        """
        # ç”ŸæˆScanContext
        scan_context = self.sc_generator.make_scan_context(points)
        
        # è½¬æ¢ä¸ºtensor
        sc_tensor = torch.from_numpy(scan_context).unsqueeze(0).unsqueeze(0).float()
        sc_tensor = sc_tensor.to(self.device)
        
        # æå–æè¿°å­
        with torch.no_grad():
            query_desc = self.model(sc_tensor)
            query_desc = query_desc.cpu().numpy().flatten()
        
        return self._localize_from_descriptor(query_desc, top_k, "ç‚¹äº‘æ•°æ®")
    
    def _localize_from_descriptor(self, query_desc: np.ndarray, top_k: int, source: str) -> Dict:
        """ä»æè¿°å­è¿›è¡Œä½ç½®å®šä½"""
        similarities = []
        
        # è®¡ç®—ä¸æ‰€æœ‰åœ°å›¾ä½ç½®çš„ç›¸ä¼¼åº¦
        for position_idx, map_desc in self.map_database.items():
            similarity = self._calculate_similarity(query_desc, map_desc)
            similarities.append({
                'position_index': position_idx,
                'similarity': similarity,
                'map_file': Path(self.map_files[position_idx]).name
            })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        
        # è·å–æœ€ä½³åŒ¹é…
        best_match = similarities[0]
        
        # æ„å»ºç»“æœ
        result = {
            'query_source': source,
            'best_position_index': best_match['position_index'],
            'best_similarity': best_match['similarity'],
            'best_map_file': best_match['map_file'],
            'confidence': 'high' if best_match['similarity'] > 0.8 else 
                         'medium' if best_match['similarity'] > 0.6 else 'low',
            'top_k_candidates': similarities[:top_k],
            'total_map_positions': len(self.map_database)
        }
        
        return result
    
    def get_position_info(self, position_index: int) -> Dict:
        """è·å–æŒ‡å®šä½ç½®çš„ä¿¡æ¯"""
        if position_index not in self.map_database:
            return None

        return {
            'position_index': position_index,
            'map_file': Path(self.map_files[position_index]).name,
            'map_file_path': self.map_files[position_index],
            'has_descriptor': True
        }

def main():
    parser = argparse.ArgumentParser(description='åŸºäºç‚¹äº‘çš„ä½ç½®å®šä½')
    parser.add_argument('--model', type=str, required=True,
                       help='è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--map_database', type=str, required=True,
                       help='åœ°å›¾æ•°æ®åº“ç›®å½•è·¯å¾„ï¼ˆåŒ…å«æ‰€æœ‰å‚è€ƒä½ç½®çš„PLYæ–‡ä»¶ï¼‰')
    parser.add_argument('--query', type=str, required=True,
                       help='æŸ¥è¯¢PLYæ–‡ä»¶è·¯å¾„ï¼ˆå½“å‰ä½ç½®çš„ç‚¹äº‘ï¼‰')
    parser.add_argument('--top_k', type=int, default=5,
                       help='è¿”å›å‰kä¸ªæœ€ç›¸ä¼¼çš„ä½ç½®')
    parser.add_argument('--output', type=str, default=None,
                       help='è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--device', type=str, default='cpu',
                       help='è®¾å¤‡ç±»å‹ (cpu, 0, 1, ...)')

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—
    timestamp = get_timestamp()
    logger = setup_logger('localize', f"localize_{timestamp}.log")

    logger.info("ğŸš€ å¼€å§‹ä½ç½®å®šä½")
    logger.info(f"ğŸ“ æ¨¡å‹: {args.model}")
    logger.info(f"ğŸ—ºï¸  åœ°å›¾æ•°æ®åº“: {args.map_database}")
    logger.info(f"ğŸ” æŸ¥è¯¢æ–‡ä»¶: {args.query}")
    logger.info(f"ğŸ“Š è¿”å›å‰ {args.top_k} ä¸ªå€™é€‰ä½ç½®")

    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    if not Path(args.model).exists():
        logger.error(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model}")
        return

    if not Path(args.query).exists():
        logger.error(f"âŒ æŸ¥è¯¢æ–‡ä»¶ä¸å­˜åœ¨: {args.query}")
        return

    if not Path(args.map_database).exists():
        logger.error(f"âŒ åœ°å›¾æ•°æ®åº“ç›®å½•ä¸å­˜åœ¨: {args.map_database}")
        return

    try:
        # åˆ›å»ºä½ç½®å®šä½å™¨
        logger.info("ğŸ”„ åˆå§‹åŒ–ä½ç½®å®šä½å™¨...")
        localizer = PositionLocalizer(args.model, args.map_database, args.device)

        # æ‰§è¡Œä½ç½®å®šä½
        logger.info("ğŸ” æ‰§è¡Œä½ç½®å®šä½...")
        start_time = time.time()
        result = localizer.localize_from_ply(args.query, args.top_k)
        end_time = time.time()

        # æ˜¾ç¤ºç»“æœ
        logger.info("âœ… ä½ç½®å®šä½å®Œæˆï¼")
        logger.info(f"â±ï¸  å¤„ç†æ—¶é—´: {end_time - start_time:.2f}ç§’")
        logger.info(f"ğŸ¯ æœ€ä½³åŒ¹é…ä½ç½®: {result['best_position_index']}")
        logger.info(f"ğŸ“„ å¯¹åº”æ–‡ä»¶: {result['best_map_file']}")
        logger.info(f"ğŸ”— ç›¸ä¼¼åº¦: {result['best_similarity']:.4f}")
        logger.info(f"ğŸšï¸  ç½®ä¿¡åº¦: {result['confidence']}")

        logger.info(f"\nğŸ“‹ å‰ {args.top_k} ä¸ªå€™é€‰ä½ç½®:")
        for i, candidate in enumerate(result['top_k_candidates']):
            logger.info(f"  {i+1}. ä½ç½® {candidate['position_index']}: "
                       f"ç›¸ä¼¼åº¦={candidate['similarity']:.4f}, "
                       f"æ–‡ä»¶={candidate['map_file']}")

        # ä¿å­˜ç»“æœ
        output_data = {
            'localization_result': result,
            'processing_time': end_time - start_time,
            'timestamp': timestamp,
            'parameters': {
                'model_path': args.model,
                'map_database_path': args.map_database,
                'query_file': args.query,
                'top_k': args.top_k,
                'device': args.device
            }
        }

        if args.output:
            output_path = Path(args.output)
        else:
            output_path = Path(f"localization_result_{timestamp}.json")

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False, default=str)

        logger.info(f"ğŸ’¾ ç»“æœä¿å­˜è‡³: {output_path}")

        # è¾“å‡ºå…³é”®ä¿¡æ¯åˆ°æ§åˆ¶å°
        print(f"\nğŸ¯ å®šä½ç»“æœ:")
        print(f"   å½“å‰ä½ç½®ç´¢å¼•: {result['best_position_index']}")
        print(f"   ç½®ä¿¡åº¦: {result['confidence']}")
        print(f"   ç›¸ä¼¼åº¦: {result['best_similarity']:.4f}")
        print(f"   å¯¹åº”åœ°å›¾æ–‡ä»¶: {result['best_map_file']}")

    except Exception as e:
        logger.error(f"âŒ ä½ç½®å®šä½å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()
