#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
çœŸå®åŸºçº¿æ¨¡å‹è¯„ä¼°è„šæœ¬
è¯„ä¼°åˆšåˆšè®­ç»ƒå®Œæˆçš„åŸºçº¿æ¨¡å‹çš„å®é™…æ€§èƒ½
"""

import torch
import torch.nn as nn
import numpy as np
import os
import json
import time
from datetime import datetime
from pathlib import Path
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from tqdm import tqdm

# å¯¼å…¥æ¨¡å—
from models.sc_ring_cnn import SCRingCNN
from utils.scan_context import ScanContext
import open3d as o3d

def load_trajectory_segments():
    """åŠ è½½è½¨è¿¹åˆ†æ®µé…ç½®"""
    segments = []
    for i in range(1, 21):  # 20ä¸ªåˆ†æ®µ
        start_seq = 2 + (i-1) * 88
        end_seq = start_seq + 87
        segments.append({
            'id': i,
            'start': start_seq,
            'end': end_seq,
            'name': f'æ®µ{i}'
        })
    return segments

def load_point_cloud_data():
    """åŠ è½½ç‚¹äº‘æ•°æ®"""
    data_dir = Path("data/2025-07-03-16-28-57plyæå–æ–‡ä»¶3")
    
    # è·å–æ‰€æœ‰PLYæ–‡ä»¶
    ply_files = sorted(list(data_dir.glob("*.ply")))
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(ply_files)} ä¸ªPLYæ–‡ä»¶")
    
    # æå–åºåˆ—å·
    sequence_numbers = []
    valid_files = []
    
    for ply_file in ply_files:
        try:
            # ä»æ–‡ä»¶åæå–åºåˆ—å·
            filename = ply_file.stem  # cloud_00002
            seq_num = int(filename.split('_')[-1])
            sequence_numbers.append(seq_num)
            valid_files.append(ply_file)
        except:
            continue
    
    print(f"âœ… æˆåŠŸæå– {len(sequence_numbers)} ä¸ªåºåˆ—å·")
    
    return valid_files, sequence_numbers

def assign_segment_labels(sequence_numbers, segments):
    """åˆ†é…åˆ†æ®µæ ‡ç­¾"""
    labels = []
    
    for seq_num in sequence_numbers:
        # æ‰¾åˆ°å¯¹åº”çš„åˆ†æ®µ
        segment_id = None
        for segment in segments:
            if segment['start'] <= seq_num <= segment['end']:
                segment_id = segment['id']
                break
        
        if segment_id is None:
            # å¦‚æœä¸åœ¨ä»»ä½•åˆ†æ®µå†…ï¼Œåˆ†é…åˆ°æœ€è¿‘çš„åˆ†æ®µ
            distances = [abs(seq_num - (seg['start'] + seg['end']) / 2) for seg in segments]
            segment_id = segments[np.argmin(distances)]['id']
        
        labels.append(segment_id)
    
    return labels

def generate_scan_context_manual(points, num_rings=20, num_sectors=60, max_range=50.0):
    """æ‰‹åŠ¨å®ç°ScanContextç”Ÿæˆ"""
    # åˆå§‹åŒ–ScanContextçŸ©é˜µ
    sc = np.zeros((num_rings, num_sectors))
    
    if len(points) == 0:
        return sc
    
    # è®¡ç®—æåæ ‡
    x = points[:, 0]
    y = points[:, 1]
    z = points[:, 2]
    
    # è®¡ç®—è·ç¦»å’Œè§’åº¦
    distances = np.sqrt(x**2 + y**2)
    angles = np.arctan2(y, x)
    
    # è¿‡æ»¤è¶…å‡ºèŒƒå›´çš„ç‚¹
    valid_mask = distances <= max_range
    distances = distances[valid_mask]
    angles = angles[valid_mask]
    heights = z[valid_mask]
    
    if len(distances) == 0:
        return sc
    
    # è®¡ç®—ç¯å’Œæ‰‡åŒºç´¢å¼•
    ring_indices = np.floor(distances / max_range * num_rings).astype(int)
    ring_indices = np.clip(ring_indices, 0, num_rings - 1)
    
    # è§’åº¦å½’ä¸€åŒ–åˆ°[0, 2Ï€]
    angles = (angles + np.pi) % (2 * np.pi)
    sector_indices = np.floor(angles / (2 * np.pi) * num_sectors).astype(int)
    sector_indices = np.clip(sector_indices, 0, num_sectors - 1)
    
    # å¡«å……ScanContext - ä½¿ç”¨æœ€å¤§é«˜åº¦
    for i in range(len(ring_indices)):
        ring_idx = ring_indices[i]
        sector_idx = sector_indices[i]
        sc[ring_idx, sector_idx] = max(sc[ring_idx, sector_idx], heights[i])
    
    return sc

def extract_scan_context_features(ply_files):
    """æå–ScanContextç‰¹å¾"""
    scan_contexts = []
    
    print("ğŸ”„ å¼€å§‹æå–ScanContextç‰¹å¾...")
    
    for i, ply_file in enumerate(tqdm(ply_files, desc="æå–ç‰¹å¾")):
        try:
            # åŠ è½½ç‚¹äº‘
            pcd = o3d.io.read_point_cloud(str(ply_file))
            points = np.asarray(pcd.points)
            
            if len(points) == 0:
                print(f"âš ï¸  æ–‡ä»¶ {ply_file} ä¸ºç©º")
                sc = np.zeros((20, 60))
                scan_contexts.append(sc)
                continue
            
            # æ‰‹åŠ¨å®ç°ScanContextæå–
            sc = generate_scan_context_manual(points, num_rings=20, num_sectors=60, max_range=50.0)
            scan_contexts.append(sc)
            
        except Exception as e:
            print(f"âš ï¸  å¤„ç†æ–‡ä»¶ {ply_file} æ—¶å‡ºé”™: {e}")
            # åˆ›å»ºé›¶å¡«å……çš„ScanContext
            sc = np.zeros((20, 60))
            scan_contexts.append(sc)
    
    return np.array(scan_contexts)

def evaluate_baseline_model():
    """è¯„ä¼°åŸºçº¿æ¨¡å‹"""
    print("ğŸš€ å¼€å§‹è¯„ä¼°çœŸå®åŸºçº¿æ¨¡å‹")
    print("=" * 60)
    
    # è®¾å¤‡è®¾ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    model_path = Path("experiments/baseline_training_20250723_135737/best_baseline_model.pth")
    if not model_path.exists():
        print("âŒ æ‰¾ä¸åˆ°è®­ç»ƒå¥½çš„åŸºçº¿æ¨¡å‹")
        return None
    
    print(f"ğŸ“ åŠ è½½æ¨¡å‹: {model_path}")
    
    # åˆ›å»ºæ¨¡å‹
    model = SCRingCNN(descriptor_dim=512).to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # 1. åŠ è½½æ•°æ®
    print("ğŸ“‹ åŠ è½½è½¨è¿¹åˆ†æ®µé…ç½®...")
    segments = load_trajectory_segments()
    
    print("ğŸ“ åŠ è½½ç‚¹äº‘æ•°æ®...")
    ply_files, sequence_numbers = load_point_cloud_data()
    
    print("ğŸ·ï¸ åˆ†é…æ®µæ ‡ç­¾...")
    labels = assign_segment_labels(sequence_numbers, segments)
    
    # 2. æå–ç‰¹å¾
    print("ğŸ”„ æå–ScanContextç‰¹å¾...")
    scan_contexts = extract_scan_context_features(ply_files)
    
    print(f"âœ… æå–äº† {scan_contexts.shape} çš„ScanContextç‰¹å¾")
    
    # 3. ç”Ÿæˆæè¿°å­
    print("ğŸ§  ç”Ÿæˆæè¿°å­...")
    descriptors = []
    
    with torch.no_grad():
        for i, sc in enumerate(tqdm(scan_contexts, desc="ç”Ÿæˆæè¿°å­")):
            # è½¬æ¢ä¸ºå¼ é‡
            sc_tensor = torch.FloatTensor(sc).unsqueeze(0).unsqueeze(0).to(device)
            
            # å‰å‘ä¼ æ’­
            descriptor = model(sc_tensor)
            descriptors.append(descriptor.cpu().numpy().flatten())
    
    descriptors = np.array(descriptors)
    print(f"âœ… ç”Ÿæˆäº† {descriptors.shape} çš„æè¿°å­")
    
    # 4. è®¡ç®—è·ç¦»çŸ©é˜µ
    print("ğŸ“Š è®¡ç®—è·ç¦»çŸ©é˜µ...")
    num_samples = len(descriptors)
    distance_matrix = np.zeros((num_samples, num_samples))
    
    for i in tqdm(range(num_samples), desc="è®¡ç®—è·ç¦»"):
        for j in range(num_samples):
            # ä½¿ç”¨æ¬§å‡ é‡Œå¾—è·ç¦»
            distance_matrix[i, j] = np.linalg.norm(descriptors[i] - descriptors[j])
    
    # 5. åœ°ç‚¹è¯†åˆ«è¯„ä¼°
    print("ğŸ¯ è¿›è¡Œåœ°ç‚¹è¯†åˆ«è¯„ä¼°...")
    
    # åˆ›å»ºæµ‹è¯•é›†ï¼ˆä½¿ç”¨100ä¸ªæ ·æœ¬ï¼‰
    test_indices = np.random.choice(num_samples, min(100, num_samples), replace=False)
    
    top1_correct = 0
    top5_correct = 0
    top10_correct = 0
    
    for query_idx in tqdm(test_indices, desc="åœ°ç‚¹è¯†åˆ«æµ‹è¯•"):
        query_label = labels[query_idx]
        
        # è·å–è·ç¦»ï¼ˆæ’é™¤è‡ªå·±ï¼‰
        distances = distance_matrix[query_idx].copy()
        distances[query_idx] = float('inf')  # æ’é™¤è‡ªå·±
        
        # æ‰¾åˆ°æœ€è¿‘çš„é‚»å±…
        nearest_indices = np.argsort(distances)
        
        # Top-1å‡†ç¡®ç‡
        if labels[nearest_indices[0]] == query_label:
            top1_correct += 1
        
        # Top-5å‡†ç¡®ç‡
        top5_labels = [labels[idx] for idx in nearest_indices[:5]]
        if query_label in top5_labels:
            top5_correct += 1
        
        # Top-10å‡†ç¡®ç‡
        top10_labels = [labels[idx] for idx in nearest_indices[:10]]
        if query_label in top10_labels:
            top10_correct += 1
    
    # è®¡ç®—å‡†ç¡®ç‡
    num_test = len(test_indices)
    top1_accuracy = top1_correct / num_test
    top5_accuracy = top5_correct / num_test
    top10_accuracy = top10_correct / num_test
    
    # 6. åˆ†ç±»è¯„ä¼°
    print("ğŸ·ï¸ è¿›è¡Œåˆ†ç±»è¯„ä¼°...")
    
    # ä½¿ç”¨æœ€è¿‘é‚»åˆ†ç±»
    predicted_labels = []
    true_labels = []
    
    for query_idx in test_indices:
        query_label = labels[query_idx]
        true_labels.append(query_label)
        
        # è·å–è·ç¦»ï¼ˆæ’é™¤è‡ªå·±ï¼‰
        distances = distance_matrix[query_idx].copy()
        distances[query_idx] = float('inf')
        
        # æ‰¾åˆ°æœ€è¿‘çš„é‚»å±…
        nearest_idx = np.argmin(distances)
        predicted_labels.append(labels[nearest_idx])
    
    # è®¡ç®—åˆ†ç±»æŒ‡æ ‡
    classification_accuracy = accuracy_score(true_labels, predicted_labels)
    precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=0)
    recall = recall_score(true_labels, predicted_labels, average='weighted', zero_division=0)
    f1 = f1_score(true_labels, predicted_labels, average='weighted', zero_division=0)
    
    # 7. æè¿°å­è´¨é‡è¯„ä¼°
    print("ğŸ”¬ è¯„ä¼°æè¿°å­è´¨é‡...")
    
    # è®¡ç®—ç±»å†…å’Œç±»é—´è·ç¦»
    intra_class_distances = []
    inter_class_distances = []
    
    unique_labels = list(set(labels))
    
    for label in unique_labels:
        label_indices = [i for i, l in enumerate(labels) if l == label]
        
        # ç±»å†…è·ç¦»
        for i in range(len(label_indices)):
            for j in range(i+1, len(label_indices)):
                idx1, idx2 = label_indices[i], label_indices[j]
                intra_class_distances.append(distance_matrix[idx1, idx2])
        
        # ç±»é—´è·ç¦»
        other_indices = [i for i, l in enumerate(labels) if l != label]
        for idx1 in label_indices[:5]:  # é™åˆ¶è®¡ç®—é‡
            for idx2 in other_indices[:10]:
                inter_class_distances.append(distance_matrix[idx1, idx2])
    
    avg_intra_distance = np.mean(intra_class_distances) if intra_class_distances else 0
    avg_inter_distance = np.mean(inter_class_distances) if inter_class_distances else 0
    separation_ratio = avg_inter_distance / avg_intra_distance if avg_intra_distance > 0 else 0
    
    # 8. ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results = {
        "model_info": {
            "model_path": str(model_path),
            "model_type": "SCRingCNN_Baseline",
            "descriptor_dim": 512,
            "evaluation_time": timestamp
        },
        "data_info": {
            "num_samples": num_samples,
            "num_test_samples": num_test,
            "num_segments": len(unique_labels),
            "scan_context_shape": list(scan_contexts.shape)
        },
        "place_recognition": {
            "top_1": float(top1_accuracy),
            "top_5": float(top5_accuracy),
            "top_10": float(top10_accuracy)
        },
        "classification": {
            "accuracy": float(classification_accuracy),
            "precision": float(precision),
            "recall": float(recall),
            "f1": float(f1)
        },
        "descriptor_quality": {
            "intra_class_distance": float(avg_intra_distance),
            "inter_class_distance": float(avg_inter_distance),
            "separation_ratio": float(separation_ratio)
        },
        "training_info": {
            "final_loss": 0.0266,
            "epochs": 80,
            "best_epoch": "80"
        }
    }
    
    # ä¿å­˜ç»“æœ
    results_file = f"evaluation_results/baseline_evaluation_{timestamp}.json"
    os.makedirs("evaluation_results", exist_ok=True)
    
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    # æ‰“å°ç»“æœ
    print("=" * 60)
    print("ğŸ‰ åŸºçº¿æ¨¡å‹è¯„ä¼°å®Œæˆ!")
    print("=" * 60)
    print(f"ğŸ“Š åœ°ç‚¹è¯†åˆ«æ€§èƒ½:")
    print(f"   Top-1å‡†ç¡®ç‡: {top1_accuracy:.1%} ({top1_correct}/{num_test})")
    print(f"   Top-5å‡†ç¡®ç‡: {top5_accuracy:.1%} ({top5_correct}/{num_test})")
    print(f"   Top-10å‡†ç¡®ç‡: {top10_accuracy:.1%} ({top10_correct}/{num_test})")
    print()
    print(f"ğŸ·ï¸ åˆ†ç±»æ€§èƒ½:")
    print(f"   åˆ†ç±»å‡†ç¡®ç‡: {classification_accuracy:.1%}")
    print(f"   ç²¾ç¡®ç‡: {precision:.1%}")
    print(f"   å¬å›ç‡: {recall:.1%}")
    print(f"   F1åˆ†æ•°: {f1:.3f}")
    print()
    print(f"ğŸ”¬ æè¿°å­è´¨é‡:")
    print(f"   ç±»å†…è·ç¦»: {avg_intra_distance:.4f}")
    print(f"   ç±»é—´è·ç¦»: {avg_inter_distance:.4f}")
    print(f"   åˆ†ç¦»åº¦: {separation_ratio:.2f}")
    print()
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {results_file}")
    print("=" * 60)
    
    return results

if __name__ == "__main__":
    try:
        results = evaluate_baseline_model()
        
        if results:
            print("\nğŸ¯ çœŸå®åŸºçº¿æ€§èƒ½å·²ç¡®è®¤!")
            print("ç°åœ¨å¯ä»¥è®­ç»ƒCBAMæ¨¡å‹å¹¶è¿›è¡Œå¯¹æ¯”äº†")
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
