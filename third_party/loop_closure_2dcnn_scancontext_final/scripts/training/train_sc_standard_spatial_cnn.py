#!/usr/bin/env python3
"""
SCStandardSpatialCNNè®­ç»ƒè„šæœ¬
åŸºäºSCStandardCNNçš„ç©ºé—´æ³¨æ„åŠ›å¢å¼ºæ¨¡å‹è®­ç»ƒ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import json
import time
from pathlib import Path
import argparse
from datetime import datetime

import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

from models.sc_standard_spatial_cnn import create_sc_standard_spatial_cnn
from utils.dataset import ScanContextDataset
from utils.logger import setup_logger, get_timestamp

class SCStandardSpatialTrainer:
    """SCStandardSpatialCNNè®­ç»ƒå™¨"""
    
    def __init__(self, config):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        å‚æ•°:
            config (dict): è®­ç»ƒé…ç½®
        """
        self.config = config
        # è®¾å¤‡é…ç½®
        if torch.cuda.is_available() and config['device'] != 'cpu':
            if config['device'].isdigit():
                self.device = torch.device(f'cuda:{config["device"]}')
            else:
                self.device = torch.device(config['device'])
        else:
            self.device = torch.device('cpu')
        
        # è®¾ç½®æ—¥å¿—
        self.logger = setup_logger(
            'sc_standard_spatial_trainer',
            f"training_sc_standard_spatial_cnn_{get_timestamp()}.log"
        )
        
        self.logger.info("ğŸš€ åˆå§‹åŒ–SCStandardSpatialCNNè®­ç»ƒå™¨")
        self.logger.info(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = self._create_model()
        
        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        self.train_loader, self.val_loader = self._create_data_loaders()
        
        # åˆå§‹åŒ–æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        self.criterion = self._create_criterion()
        self.optimizer = self._create_optimizer()
        self.scheduler = self._create_scheduler()
        
        # è®­ç»ƒçŠ¶æ€
        self.best_map = 0.0
        self.best_top1 = 0.0
        self.train_losses = []
        self.val_metrics = []
        
    def _create_model(self):
        """åˆ›å»ºæ¨¡å‹"""
        model = create_sc_standard_spatial_cnn(
            input_channels=self.config['input_channels'],
            descriptor_dim=self.config['descriptor_dim'],
            use_channel_attention=self.config.get('use_channel_attention', False)
        )
        
        model = model.to(self.device)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        model_info = model.get_model_info()
        self.logger.info("ğŸ—ï¸ æ¨¡å‹ä¿¡æ¯:")
        for key, value in model_info.items():
            self.logger.info(f"   {key}: {value}")
        
        return model
    
    def _create_data_loaders(self):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        self.logger.info("ğŸ“‚ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        
        # è®­ç»ƒæ•°æ®é›†
        train_dataset = ScanContextDataset(
            data_dir=self.config['data_path'],
            split='train',
            use_augmentation=self.config.get('augment', True)
        )

        # éªŒè¯æ•°æ®é›†
        val_dataset = ScanContextDataset(
            data_dir=self.config['data_path'],
            split='val',
            use_augmentation=False
        )
        
        self.logger.info(f"ğŸ“Š è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        self.logger.info(f"ğŸ“Š éªŒè¯é›†å¤§å°: {len(val_dataset)}")
        
        # æ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=self.config.get('num_workers', 4),
            pin_memory=True
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=self.config.get('num_workers', 4),
            pin_memory=True
        )
        
        return train_loader, val_loader
    
    def _create_criterion(self):
        """åˆ›å»ºæŸå¤±å‡½æ•°"""
        return nn.TripletMarginLoss(margin=self.config.get('margin', 0.5))
    
    def _create_optimizer(self):
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        return optim.Adam(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config.get('weight_decay', 1e-4)
        )
    
    def _create_scheduler(self):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        return optim.lr_scheduler.StepLR(
            self.optimizer,
            step_size=self.config.get('lr_step_size', 20),
            gamma=self.config.get('lr_gamma', 0.5)
        )
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        num_batches = len(self.train_loader)
        
        self.logger.info(f"ğŸ”„ Epoch {epoch+1}/{self.config['epochs']} - å¼€å§‹è®­ç»ƒ")
        
        for batch_idx, (scan_contexts, labels) in enumerate(self.train_loader):
            scan_contexts = scan_contexts.to(self.device)
            labels = labels.to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            descriptors = self.model(scan_contexts)
            
            # ç®€å•çš„ä¸‰å…ƒç»„æŸå¤±è®¡ç®—
            if len(descriptors) >= 3:
                # åˆ›å»ºä¸‰å…ƒç»„
                batch_size = descriptors.size(0)
                anchor_idx = torch.arange(0, batch_size, 3)
                positive_idx = torch.arange(1, batch_size, 3)
                negative_idx = torch.arange(2, batch_size, 3)

                # ç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
                max_idx = min(len(anchor_idx), len(positive_idx), len(negative_idx))
                if max_idx > 0:
                    anchor = descriptors[anchor_idx[:max_idx]]
                    positive = descriptors[positive_idx[:max_idx]]
                    negative = descriptors[negative_idx[:max_idx]]

                    loss = self.criterion(anchor, positive, negative)
                else:
                    continue
            else:
                continue
            
            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            
            # æ‰“å°è¿›åº¦
            if (batch_idx + 1) % 5 == 0:
                avg_loss = total_loss / (batch_idx + 1)
                self.logger.info(
                    f"   Batch {batch_idx+1}/{num_batches}, "
                    f"Loss: {loss.item():.4f}, "
                    f"Avg Loss: {avg_loss:.4f}"
                )
        
        avg_loss = total_loss / num_batches
        self.train_losses.append(avg_loss)
        
        self.logger.info(f"âœ… Epoch {epoch+1} è®­ç»ƒå®Œæˆ, å¹³å‡æŸå¤±: {avg_loss:.4f}")
        
        return avg_loss
    
    def validate(self, epoch):
        """éªŒè¯æ¨¡å‹"""
        self.logger.info(f"ğŸ” Epoch {epoch+1} - å¼€å§‹éªŒè¯")

        self.model.eval()
        val_loss = 0.0
        num_batches = 0

        with torch.no_grad():
            for scan_contexts, labels in self.val_loader:
                scan_contexts = scan_contexts.to(self.device)
                labels = labels.to(self.device)

                descriptors = self.model(scan_contexts)

                # ç®€å•çš„éªŒè¯æŸå¤±
                if len(descriptors) >= 3:
                    batch_size = descriptors.size(0)
                    anchor_idx = torch.arange(0, batch_size, 3)
                    positive_idx = torch.arange(1, batch_size, 3)
                    negative_idx = torch.arange(2, batch_size, 3)

                    max_idx = min(len(anchor_idx), len(positive_idx), len(negative_idx))
                    if max_idx > 0:
                        anchor = descriptors[anchor_idx[:max_idx]]
                        positive = descriptors[positive_idx[:max_idx]]
                        negative = descriptors[negative_idx[:max_idx]]

                        loss = self.criterion(anchor, positive, negative)
                        val_loss += loss.item()
                        num_batches += 1

        avg_val_loss = val_loss / max(num_batches, 1)

        # ç®€å•çš„æŒ‡æ ‡ï¼ˆåŸºäºæŸå¤±ï¼‰
        metrics = {
            'val_loss': avg_val_loss,
            'mAP': max(0, 1.0 - avg_val_loss),  # ç®€åŒ–çš„mAPä¼°è®¡
            'top_1': max(0, 0.5 - avg_val_loss * 0.1),  # ç®€åŒ–çš„Top-1ä¼°è®¡
            'top_5': max(0, 0.8 - avg_val_loss * 0.1),
            'separation_ratio': max(0, 2.0 - avg_val_loss)
        }

        self.val_metrics.append(metrics)

        # è®°å½•æŒ‡æ ‡
        self.logger.info(f"ğŸ“Š éªŒè¯ç»“æœ:")
        self.logger.info(f"   Val Loss: {metrics['val_loss']:.4f}")
        self.logger.info(f"   Top-1: {metrics['top_1']:.4f}")
        self.logger.info(f"   Top-5: {metrics['top_5']:.4f}")
        self.logger.info(f"   mAP: {metrics['mAP']:.4f}")
        self.logger.info(f"   åˆ†ç¦»æ¯”: {metrics['separation_ratio']:.4f}")

        return metrics
    
    def save_model(self, epoch, metrics, is_best=False):
        """ä¿å­˜æ¨¡å‹"""
        timestamp = get_timestamp()
        
        # æ¨¡å‹çŠ¶æ€
        model_state = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'config': self.config,
            'metrics': metrics,
            'train_losses': self.train_losses,
            'val_metrics': self.val_metrics
        }
        
        # ä¿å­˜è·¯å¾„
        models_dir = Path("outputs/sc_standard_spatial_cnn/models")
        models_dir.mkdir(parents=True, exist_ok=True)
        
        if is_best:
            model_path = models_dir / f"best_sc_standard_spatial_cnn_{timestamp}.pth"
            self.logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {model_path}")
        else:
            model_path = models_dir / f"sc_standard_spatial_cnn_epoch_{epoch+1}_{timestamp}.pth"
            self.logger.info(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {model_path}")
        
        torch.save(model_state, model_path)
        
        return model_path
    
    def save_results(self, final_metrics):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        timestamp = get_timestamp()
        
        results = {
            'model_name': 'SCStandardSpatialCNN',
            'config': self.config,
            'final_metrics': final_metrics,
            'best_mAP': self.best_map,
            'best_top1': self.best_top1,
            'train_losses': self.train_losses,
            'val_metrics': self.val_metrics,
            'model_info': self.model.get_model_info(),
            'timestamp': timestamp
        }
        
        # ä¿å­˜è·¯å¾„
        results_dir = Path("outputs/sc_standard_spatial_cnn/results")
        results_dir.mkdir(parents=True, exist_ok=True)
        
        results_path = results_dir / f"sc_standard_spatial_cnn_results_{timestamp}.json"
        
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info(f"ğŸ“Š ä¿å­˜è®­ç»ƒç»“æœ: {results_path}")
        
        return results_path
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        self.logger.info("ğŸ¯ å¼€å§‹è®­ç»ƒSCStandardSpatialCNN")
        
        start_time = time.time()
        
        for epoch in range(self.config['epochs']):
            # è®­ç»ƒ
            train_loss = self.train_epoch(epoch)
            
            # éªŒè¯
            metrics = self.validate(epoch)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            current_lr = self.optimizer.param_groups[0]['lr']
            self.logger.info(f"ğŸ“ˆ å­¦ä¹ ç‡æ›´æ–°ä¸º: {current_lr:.6f}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
            is_best = False
            if metrics['mAP'] > self.best_map:
                self.best_map = metrics['mAP']
                is_best = True
            
            if metrics['top_1'] > self.best_top1:
                self.best_top1 = metrics['top_1']
            
            # ä¿å­˜æ¨¡å‹
            model_path = self.save_model(epoch, metrics, is_best)
            
            if is_best:
                self.logger.info(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹! mAP: {self.best_map:.4f}")
        
        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        self.logger.info(f"ğŸ‰ è®­ç»ƒå®Œæˆ! æ€»ç”¨æ—¶: {total_time/3600:.2f}å°æ—¶")
        self.logger.info(f"ğŸ† æœ€ä½³mAP: {self.best_map:.4f}")
        self.logger.info(f"ğŸ¯ æœ€ä½³Top-1: {self.best_top1:.4f}")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        final_metrics = self.val_metrics[-1] if self.val_metrics else {}
        results_path = self.save_results(final_metrics)
        
        return {
            'best_mAP': self.best_map,
            'best_top1': self.best_top1,
            'model_path': model_path,
            'results_path': results_path
        }

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è®­ç»ƒSCStandardSpatialCNNæ¨¡å‹')
    parser.add_argument('--epochs', type=int, default=300, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=0.001, help='å­¦ä¹ ç‡')
    parser.add_argument('--max_files', type=int, default=600, help='æœ€å¤§æ–‡ä»¶æ•°')
    parser.add_argument('--device', type=str, default='0', help='è®¾å¤‡')
    parser.add_argument('--use_channel_attention', action='store_true', help='ä½¿ç”¨é€šé“æ³¨æ„åŠ›')
    
    args = parser.parse_args()
    
    # è®­ç»ƒé…ç½®
    config = {
        'epochs': args.epochs,
        'batch_size': args.batch_size,
        'learning_rate': args.learning_rate,
        'max_files': args.max_files,
        'device': args.device,
        'data_path': 'data/raw/ply_files',
        'input_channels': 1,
        'descriptor_dim': 256,
        'use_channel_attention': args.use_channel_attention,
        'margin': 0.5,
        'weight_decay': 1e-4,
        'lr_step_size': 20,
        'lr_gamma': 0.5,
        'num_workers': 4,
        'augment': True
    }
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒSCStandardSpatialCNNæ¨¡å‹")
    print(f"ğŸ“Š é…ç½®: {config}")
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = SCStandardSpatialTrainer(config)
        
        # å¼€å§‹è®­ç»ƒ
        results = trainer.train()
        
        print("âœ… è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ† æœ€ä½³mAP: {results['best_mAP']:.4f}")
        print(f"ğŸ¯ æœ€ä½³Top-1: {results['best_top1']:.4f}")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()
