#!/usr/bin/env python3
"""
SCStandardCNNä¸“é—¨è®­ç»ƒè„šæœ¬
æ ‡å‡†å·ç§¯æ¨¡å‹è®­ç»ƒï¼Œç”¨äºä¸ç¯å½¢å·ç§¯å¯¹æ¯”
"""
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from pathlib import Path
import json
from tqdm import tqdm
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from config import get_config
from models import SCStandardCNN
from utils import SimpleLoopClosureDataset, setup_model_logger, get_timestamp, calculate_metrics

class TripletLoss(nn.Module):
    """ä¸‰å…ƒç»„æŸå¤±"""
    def __init__(self, margin=1.0):
        super().__init__()
        self.margin = margin
        
    def forward(self, anchor, positive, negative):
        pos_dist = torch.norm(anchor - positive, p=2, dim=1)
        neg_dist = torch.norm(anchor - negative, p=2, dim=1)
        loss = torch.clamp(pos_dist - neg_dist + self.margin, min=0.0)
        return loss.mean()

def create_triplets(features, labels):
    """åˆ›å»ºä¸‰å…ƒç»„"""
    triplets = []
    labels_np = labels.cpu().numpy()
    
    for i in range(len(features)):
        anchor_label = labels_np[i]
        
        # æ‰¾æ­£æ ·æœ¬ï¼ˆåŒç±»åˆ«ï¼Œä½†ä¸æ˜¯è‡ªå·±ï¼‰
        positive_indices = [j for j in range(len(features)) 
                          if labels_np[j] == anchor_label and j != i]
        if not positive_indices:
            continue
            
        # æ‰¾è´Ÿæ ·æœ¬ï¼ˆä¸åŒç±»åˆ«ï¼‰
        negative_indices = [j for j in range(len(features)) 
                          if labels_np[j] != anchor_label]
        if not negative_indices:
            continue
        
        # éšæœºé€‰æ‹©æ­£è´Ÿæ ·æœ¬
        import random
        pos_idx = random.choice(positive_indices)
        neg_idx = random.choice(negative_indices)
        
        triplets.append((i, pos_idx, neg_idx))
    
    return triplets

def train_epoch(model, dataloader, criterion, optimizer, device, logger):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    num_batches = 0
    
    progress_bar = tqdm(dataloader, desc="Training SCStandardCNN")
    
    for batch_idx, (data, labels) in enumerate(progress_bar):
        data, labels = data.to(device), labels.to(device)
        
        # å‰å‘ä¼ æ’­
        features = model(data)
        
        # åˆ›å»ºä¸‰å…ƒç»„
        triplets = create_triplets(features, labels)
        
        if not triplets:
            continue
        
        # è®¡ç®—ä¸‰å…ƒç»„æŸå¤±
        total_triplet_loss = 0
        for anchor_idx, pos_idx, neg_idx in triplets:
            anchor = features[anchor_idx:anchor_idx+1]
            positive = features[pos_idx:pos_idx+1]
            negative = features[neg_idx:neg_idx+1]
            
            triplet_loss = criterion(anchor, positive, negative)
            total_triplet_loss += triplet_loss
        
        if len(triplets) > 0:
            loss = total_triplet_loss / len(triplets)
        else:
            continue
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        num_batches += 1
        
        progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})
    
    avg_loss = total_loss / num_batches if num_batches > 0 else 0
    logger.info(f"SCStandardCNN Training Loss: {avg_loss:.4f}")
    return avg_loss

def evaluate(model, dataloader, device, logger):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    all_features = []
    all_labels = []
    
    with torch.no_grad():
        for data, labels in tqdm(dataloader, desc="Evaluating SCStandardCNN"):
            data = data.to(device)
            features = model(data)
            
            all_features.append(features.cpu())
            all_labels.append(labels)
    
    # åˆå¹¶æ‰€æœ‰ç‰¹å¾å’Œæ ‡ç­¾
    all_features = torch.cat(all_features, dim=0)
    all_labels = torch.cat(all_labels, dim=0)
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = calculate_metrics(all_features, all_labels)
    
    # è®°å½•ç»“æœ
    logger.info("SCStandardCNN Evaluation Results:")
    for key, value in metrics.items():
        if isinstance(value, float):
            logger.info(f"  {key}: {value:.4f}")
        else:
            logger.info(f"  {key}: {value}")
    
    return metrics

def main():
    parser = argparse.ArgumentParser(description='SCStandardCNNä¸“é—¨è®­ç»ƒè„šæœ¬')
    parser.add_argument('--epochs', type=int, default=20,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=16,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=0.001,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--descriptor_dim', type=int, default=256,
                       help='æè¿°å­ç»´åº¦')
    parser.add_argument('--margin', type=float, default=1.0,
                       help='ä¸‰å…ƒç»„æŸå¤±è¾¹ç•Œ')
    parser.add_argument('--data_dir', type=str, default=None,
                       help='æ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--max_files', type=int, default=100,
                       help='æœ€å¤§æ–‡ä»¶æ•°é‡')
    parser.add_argument('--device', type=str, default='0',
                       help='è®¾å¤‡ç±»å‹ (cpu, 0, 1, 2, ... æˆ– cuda:0, cuda:1, ...)')
    parser.add_argument('--use_residual', action='store_true',
                       help='æ˜¯å¦ä½¿ç”¨æ®‹å·®è¿æ¥')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    timestamp = get_timestamp()
    project_root = Path(__file__).parent.parent.parent

    logger, log_file = setup_model_logger(
        model_type='sc_standard_cnn',
        script_type='training',
        timestamp=timestamp,
        project_root=project_root
    )
    
    logger.info("ğŸš€ å¼€å§‹SCStandardCNNä¸“é—¨è®­ç»ƒ")
    logger.info(f"è®­ç»ƒè½®æ•°: {args.epochs}")
    logger.info(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    logger.info(f"å­¦ä¹ ç‡: {args.learning_rate}")
    logger.info(f"æè¿°å­ç»´åº¦: {args.descriptor_dim}")
    logger.info(f"ä½¿ç”¨æ®‹å·®è¿æ¥: {args.use_residual}")
    # è®¾å¤‡å¤„ç†
    if args.device == 'cpu':
        device = torch.device('cpu')
        logger.info(f"ä½¿ç”¨è®¾å¤‡: CPU")
    elif args.device.isdigit():
        # æ•°å­—å½¢å¼ï¼Œå¦‚ '0', '1', '2'
        gpu_id = int(args.device)
        if torch.cuda.is_available() and gpu_id < torch.cuda.device_count():
            device = torch.device(f'cuda:{gpu_id}')
            logger.info(f"ä½¿ç”¨è®¾å¤‡: GPU {gpu_id} ({torch.cuda.get_device_name(gpu_id)})")
        else:
            device = torch.device('cpu')
            logger.warning(f"GPU {gpu_id} ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")
    elif args.device.startswith('cuda:'):
        # cuda:0, cuda:1 å½¢å¼
        if torch.cuda.is_available():
            device = torch.device(args.device)
            gpu_id = int(args.device.split(':')[1])
            logger.info(f"ä½¿ç”¨è®¾å¤‡: {args.device} ({torch.cuda.get_device_name(gpu_id)})")
        else:
            device = torch.device('cpu')
            logger.warning(f"CUDA ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")
    else:
        device = torch.device('cpu')
        logger.warning(f"æœªçŸ¥è®¾å¤‡ç±»å‹ '{args.device}'ï¼Œä½¿ç”¨ CPU")

    # æ•°æ®ç›®å½•
    if args.data_dir:
        data_dir = Path(args.data_dir)
    else:
        data_dir = project_root / "data" / "raw" / "ply_files"
    
    if not data_dir.exists():
        logger.error(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return
    
    # åˆ›å»ºæ•°æ®é›†
    logger.info("åŠ è½½æ•°æ®é›†...")
    dataset = SimpleLoopClosureDataset(
        data_dir=data_dir,
        cache_dir=project_root / "data" / "cache",
        max_files=args.max_files
    )
    
    # åˆ†å‰²æ•°æ®é›†
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=True
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=args.batch_size, 
        shuffle=False
    )
    
    logger.info(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    logger.info(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")
    
    # åˆ›å»ºSCStandardCNNæ¨¡å‹
    model = SCStandardCNN(
        num_rings=20,
        num_sectors=60,
        descriptor_dim=args.descriptor_dim,
        use_residual=args.use_residual
    )
    
    model.to(device)
    
    logger.info(f"SCStandardCNNæ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # åˆ›å»ºæŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = TripletLoss(margin=args.margin)
    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)
    
    # è®­ç»ƒå¾ªç¯
    best_top1 = 0
    results = {
        'model_type': 'SCStandardCNN',
        'experiment_name': f'sc_standard_cnn_{timestamp}',
        'config': vars(args),
        'epochs': [],
        'best_metrics': None
    }
    
    for epoch in range(args.epochs):
        logger.info(f"\nEpoch {epoch + 1}/{args.epochs}")
        
        # è®­ç»ƒ
        train_loss = train_epoch(model, train_loader, criterion, optimizer, device, logger)
        
        # è¯„ä¼°
        if (epoch + 1) % 2 == 0 or epoch == args.epochs - 1:
            metrics = evaluate(model, val_loader, device, logger)
            
            # ä¿å­˜ç»“æœ
            epoch_result = {
                'epoch': epoch + 1,
                'train_loss': train_loss,
                'metrics': metrics
            }
            results['epochs'].append(epoch_result)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if metrics['top_1'] > best_top1:
                best_top1 = metrics['top_1']
                results['best_metrics'] = metrics
                
                model_dir = project_root / "outputs" / "models"
                model_dir.mkdir(parents=True, exist_ok=True)
                
                model_path = model_dir / f"best_sc_standard_cnn_{timestamp}.pth"
                torch.save({
                    'model_state_dict': model.state_dict(),
                    'config': vars(args),
                    'metrics': metrics,
                    'epoch': epoch + 1,
                    'model_type': 'SCStandardCNN'
                }, model_path)
                
                logger.info(f"ä¿å­˜æœ€ä½³SCStandardCNNæ¨¡å‹: {model_path}")
    
    # ä¿å­˜è®­ç»ƒç»“æœ
    results_dir = project_root / "outputs" / "results"
    results_dir.mkdir(parents=True, exist_ok=True)
    
    results_path = results_dir / f"sc_standard_cnn_results_{timestamp}.json"
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    logger.info(f"SCStandardCNNè®­ç»ƒå®Œæˆï¼ç»“æœä¿å­˜è‡³: {results_path}")
    logger.info(f"æœ€ä½³Top-1å‡†ç¡®ç‡: {best_top1:.4f}")

if __name__ == "__main__":
    main()
