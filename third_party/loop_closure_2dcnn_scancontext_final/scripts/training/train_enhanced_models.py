#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆSC Standard CNNè®­ç»ƒè„šæœ¬
æµ‹è¯•å¤šç§æ³¨æ„åŠ›æœºåˆ¶ç»„åˆçš„æ€§èƒ½
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import argparse
import json
import time
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

from models.sc_standard_enhanced_cnn import create_enhanced_model
from utils.spatial_dataset import SpatialScanContextDataset
from utils.simple_contrastive_loss import AdaptiveTripletLoss
from utils.evaluation_metrics import compute_retrieval_metrics
from utils.logger import setup_logger

class EnhancedModelTrainer:
    """å¢å¼ºç‰ˆæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device(config['device'])
        
        # è®¾ç½®æ—¥å¿—
        self.logger = setup_logger(
            f"enhanced_trainer_{config['model_type']}", 
            f"training_enhanced_{config['model_type']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        )
        
        self.logger.info(f"ğŸš€ åˆå§‹åŒ–å¢å¼ºç‰ˆ{config['model_type']}è®­ç»ƒå™¨")
        self.logger.info(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºæ¨¡å‹
        self.model = self._create_model()
        self.model.to(self.device)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.train_loader, self.val_loader = self._create_data_loaders()
        
        # åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        self.optimizer = self._create_optimizer()
        self.criterion = self._create_criterion()
        self.scheduler = self._create_scheduler()
        
        # è®­ç»ƒçŠ¶æ€
        self.best_top1 = 0.0
        self.best_map = 0.0
        self.train_losses = []
        self.val_metrics = []
    
    def _create_model(self):
        """åˆ›å»ºå¢å¼ºç‰ˆæ¨¡å‹"""
        model = create_enhanced_model(
            model_type=self.config['model_type'],
            input_channels=self.config['input_channels'],
            descriptor_dim=self.config['descriptor_dim'],
            reduction=self.config.get('reduction', 16),
            dropout_rate=self.config.get('dropout_rate', 0.3)
        )
        
        # è®°å½•æ¨¡å‹ä¿¡æ¯
        model_info = model.get_model_info()
        self.logger.info(f"ğŸ—ï¸ {self.config['model_type']}æ¨¡å‹ä¿¡æ¯:")
        for key, value in model_info.items():
            self.logger.info(f"   {key}: {value}")
        
        return model
    
    def _create_data_loaders(self):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        self.logger.info("ğŸ“‚ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        
        # è®­ç»ƒæ•°æ®é›†
        train_dataset = SpatialScanContextDataset(
            data_dir=self.config['data_path'],
            split='train',
            max_files=self.config.get('max_files', None),
            use_augmentation=self.config.get('augment', True)
        )
        
        # éªŒè¯æ•°æ®é›†
        val_dataset = SpatialScanContextDataset(
            data_dir=self.config['data_path'],
            split='val',
            max_files=self.config.get('max_files', None),
            use_augmentation=False
        )
        
        self.logger.info(f"ğŸ“Š è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        self.logger.info(f"ğŸ“Š éªŒè¯é›†å¤§å°: {len(val_dataset)}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=self.config.get('num_workers', 4),
            pin_memory=True
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=self.config.get('num_workers', 4),
            pin_memory=True
        )
        
        return train_loader, val_loader
    
    def _create_optimizer(self):
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        return optim.Adam(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config.get('weight_decay', 1e-4)
        )
    
    def _create_criterion(self):
        """åˆ›å»ºæŸå¤±å‡½æ•°"""
        return AdaptiveTripletLoss(
            margin=self.config.get('margin', 0.5),
            adaptive_margin=True
        )
    
    def _create_scheduler(self):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        return optim.lr_scheduler.MultiStepLR(
            self.optimizer,
            milestones=[self.config['epochs']//3, 2*self.config['epochs']//3],
            gamma=0.5
        )
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        num_batches = len(self.train_loader)
        
        self.logger.info(f"ğŸ”„ Epoch {epoch+1}/{self.config['epochs']} - å¼€å§‹è®­ç»ƒ")
        
        for batch_idx, (data, labels) in enumerate(self.train_loader):
            data, labels = data.to(self.device), labels.to(self.device)
            
            self.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            embeddings = self.model(data)
            loss = self.criterion(embeddings, labels)
            
            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            
            # è®°å½•è¿›åº¦
            if (batch_idx + 1) % 10 == 0:
                avg_loss = total_loss / (batch_idx + 1)
                self.logger.info(f"   Batch {batch_idx+1}/{num_batches}, Loss: {loss.item():.4f}, Avg Loss: {avg_loss:.4f}")
        
        avg_loss = total_loss / num_batches
        self.train_losses.append(avg_loss)
        
        self.logger.info(f"âœ… Epoch {epoch+1} è®­ç»ƒå®Œæˆ, å¹³å‡æŸå¤±: {avg_loss:.4f}")
        
        return avg_loss
    
    def validate(self, epoch):
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        
        self.logger.info(f"ğŸ” Epoch {epoch+1} - å¼€å§‹éªŒè¯")
        self.logger.info("ğŸ” å¼€å§‹æå–ç‰¹å¾...")
        
        all_embeddings = []
        all_labels = []
        
        with torch.no_grad():
            for data, labels in self.val_loader:
                data = data.to(self.device)
                embeddings = self.model(data)
                
                all_embeddings.append(embeddings.cpu())
                all_labels.append(labels.cpu())
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾å’Œæ ‡ç­¾
        all_embeddings = torch.cat(all_embeddings, dim=0)
        all_labels = torch.cat(all_labels, dim=0)
        
        self.logger.info(f"âœ… ç‰¹å¾æå–å®Œæˆï¼Œå…± {len(all_embeddings)} ä¸ªæ ·æœ¬")
        
        # è®¡ç®—æ£€ç´¢æŒ‡æ ‡
        metrics = compute_retrieval_metrics(all_embeddings, all_labels)
        
        # è®°å½•æŒ‡æ ‡
        for key, value in metrics.items():
            if isinstance(value, float):
                self.logger.info(f"   {key}: {value:.4f}")
            else:
                self.logger.info(f"   {key}: {value}")
        
        # ä¿å­˜éªŒè¯æŒ‡æ ‡
        self.val_metrics.append({
            'epoch': epoch,
            'metrics': metrics
        })
        
        # æ›´æ–°æœ€ä½³æŒ‡æ ‡
        if metrics['top_1'] > self.best_top1:
            self.best_top1 = metrics['top_1']
            self._save_best_model(epoch, 'top1')
        
        if metrics['mAP'] > self.best_map:
            self.best_map = metrics['mAP']
            self._save_best_model(epoch, 'map')
        
        self.logger.info("ğŸ“Š éªŒè¯ç»“æœ:")
        self.logger.info(f"   Top-1: {metrics['top_1']:.4f}")
        self.logger.info(f"   Top-5: {metrics['top_5']:.4f}")
        self.logger.info(f"   mAP: {metrics['mAP']:.4f}")
        
        return metrics
    
    def _save_best_model(self, epoch, metric_type):
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        os.makedirs(f"outputs/enhanced_{self.config['model_type']}/models", exist_ok=True)
        
        model_path = f"outputs/enhanced_{self.config['model_type']}/models/best_{metric_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pth"
        
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_top1': self.best_top1,
            'best_map': self.best_map,
            'config': self.config
        }, model_path)
        
        self.logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³{metric_type}æ¨¡å‹: {model_path}")
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        self.logger.info(f"ğŸ¯ å¼€å§‹{self.config['model_type']}è®­ç»ƒ - {self.config['epochs']}è½®")
        
        start_time = time.time()
        
        for epoch in range(self.config['epochs']):
            # è®­ç»ƒ
            train_loss = self.train_epoch(epoch)
            
            # éªŒè¯
            if (epoch + 1) % self.config.get('val_interval', 20) == 0:
                val_metrics = self.validate(epoch)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            
            # è®°å½•å­¦ä¹ ç‡
            current_lr = self.optimizer.param_groups[0]['lr']
            self.logger.info(f"ğŸ“ˆ Epoch {epoch+1} å­¦ä¹ ç‡: {current_lr:.6f}")
        
        # æœ€ç»ˆéªŒè¯
        final_metrics = self.validate(self.config['epochs'] - 1)
        
        # è®¡ç®—æ€»è®­ç»ƒæ—¶é—´
        total_time = time.time() - start_time
        total_hours = total_time / 3600
        
        self.logger.info(f"ğŸ‰ {self.config['model_type']}è®­ç»ƒå®Œæˆ! æ€»ç”¨æ—¶: {total_hours:.2f}å°æ—¶")
        self.logger.info(f"ğŸ† æœ€ä½³mAP: {self.best_map:.4f}")
        self.logger.info(f"ğŸ¯ æœ€ä½³Top-1: {self.best_top1:.4f}")
        
        # ä¿å­˜è®­ç»ƒç»“æœ
        self._save_results(total_hours)
        
        return {
            'best_top1': self.best_top1,
            'best_map': self.best_map,
            'total_time': total_hours,
            'final_metrics': final_metrics
        }
    
    def _save_results(self, total_time):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        os.makedirs(f"outputs/enhanced_{self.config['model_type']}/results", exist_ok=True)
        
        results = {
            'model_type': self.config['model_type'],
            'config': self.config,
            'best_map': self.best_map,
            'best_top1': self.best_top1,
            'train_losses': self.train_losses,
            'val_metrics': self.val_metrics,
            'model_info': self.model.get_model_info(),
            'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S'),
            'total_time_hours': total_time
        }
        
        results_path = f"outputs/enhanced_{self.config['model_type']}/results/training_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ğŸ“Š ä¿å­˜è®­ç»ƒç»“æœ: {results_path}")

def main():
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆSC Standard CNNè®­ç»ƒ')
    parser.add_argument('--model_type', type=str, default='cbam', 
                       choices=['cbam', 'eca', 'se', 'simam', 'dual', 'triple', 'all'],
                       help='æ³¨æ„åŠ›æœºåˆ¶ç±»å‹')
    parser.add_argument('--epochs', type=int, default=100, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=0.001, help='å­¦ä¹ ç‡')
    parser.add_argument('--device', type=str, default='cpu', help='è®¾å¤‡')
    parser.add_argument('--data_path', type=str, default='data/raw/ply_files', help='æ•°æ®è·¯å¾„')
    parser.add_argument('--max_files', type=int, default=None, help='æœ€å¤§æ–‡ä»¶æ•°')
    
    args = parser.parse_args()
    
    config = {
        'model_type': args.model_type,
        'epochs': args.epochs,
        'batch_size': args.batch_size,
        'learning_rate': args.learning_rate,
        'device': args.device,
        'data_path': args.data_path,
        'max_files': args.max_files,
        'input_channels': 1,
        'descriptor_dim': 256,
        'reduction': 16,
        'dropout_rate': 0.3,
        'margin': 0.5,
        'weight_decay': 1e-4,
        'num_workers': 4,
        'augment': True,
        'val_interval': 20
    }
    
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒå¢å¼ºç‰ˆ{args.model_type}æ¨¡å‹")
    print(f"ğŸ“Š é…ç½®: {config}")
    
    trainer = EnhancedModelTrainer(config)
    results = trainer.train()
    
    print(f"âœ… è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ† æœ€ä½³mAP: {results['best_map']:.4f}")
    print(f"ğŸ¯ æœ€ä½³Top-1: {results['best_top1']:.4f}")
    print(f"â±ï¸ æ€»ç”¨æ—¶: {results['total_time']:.2f}å°æ—¶")

if __name__ == "__main__":
    main()
