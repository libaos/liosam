#!/usr/bin/env python3
"""
SCStandardSpatialCNN 300è½®é•¿æœŸè®­ç»ƒè„šæœ¬
åŒæ—¶è®­ç»ƒä¸¤ä¸ªç‰ˆæœ¬ï¼šä»…ç©ºé—´æ³¨æ„åŠ› vs ç©ºé—´+é€šé“æ³¨æ„åŠ›
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import json
import time
from pathlib import Path
import argparse
from datetime import datetime

import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

from models.sc_standard_spatial_cnn import create_sc_standard_spatial_cnn
from utils.spatial_dataset import SpatialScanContextDataset
from utils.simple_contrastive_loss import AdaptiveTripletLoss, SimpleContrastiveLoss
from utils.evaluation_metrics import evaluate_model
from utils import setup_logger, get_timestamp

class SpatialCNNTrainer:
    """ç©ºé—´æ³¨æ„åŠ›CNNè®­ç»ƒå™¨"""
    
    def __init__(self, config, model_name):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        å‚æ•°:
            config (dict): è®­ç»ƒé…ç½®
            model_name (str): æ¨¡å‹åç§°æ ‡è¯†
        """
        self.config = config
        self.model_name = model_name
        
        # è®¾å¤‡é…ç½®
        if torch.cuda.is_available() and config['device'] != 'cpu':
            if config['device'].isdigit():
                self.device = torch.device(f'cuda:{config["device"]}')
            else:
                self.device = torch.device(config['device'])
        else:
            self.device = torch.device('cpu')
        
        # è®¾ç½®æ—¥å¿—
        self.logger = setup_logger(
            f'spatial_trainer_{model_name}',
            f"training_{model_name}_{get_timestamp()}.log"
        )
        
        self.logger.info(f"ğŸš€ åˆå§‹åŒ–{model_name}è®­ç»ƒå™¨")
        self.logger.info(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = self._create_model()
        
        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        self.train_loader, self.val_loader = self._create_data_loaders()
        
        # åˆå§‹åŒ–æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        self.criterion = self._create_criterion()
        self.optimizer = self._create_optimizer()
        self.scheduler = self._create_scheduler()
        
        # è®­ç»ƒçŠ¶æ€
        self.best_map = 0.0
        self.best_top1 = 0.0
        self.train_losses = []
        self.val_metrics = []
        
        # æ£€æŸ¥ç‚¹ä¿å­˜é—´éš”
        self.save_interval = config.get('save_interval', 50)  # æ¯50è½®ä¿å­˜ä¸€æ¬¡
        
    def _create_model(self):
        """åˆ›å»ºæ¨¡å‹"""
        use_channel_attention = 'channel' in self.model_name.lower()
        
        model = create_sc_standard_spatial_cnn(
            input_channels=self.config['input_channels'],
            descriptor_dim=self.config['descriptor_dim'],
            use_channel_attention=use_channel_attention
        )
        
        model = model.to(self.device)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        model_info = model.get_model_info()
        self.logger.info(f"ğŸ—ï¸ {self.model_name}æ¨¡å‹ä¿¡æ¯:")
        for key, value in model_info.items():
            self.logger.info(f"   {key}: {value}")
        
        return model
    
    def _create_data_loaders(self):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        self.logger.info("ğŸ“‚ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        
        # è®­ç»ƒæ•°æ®é›†
        train_dataset = SpatialScanContextDataset(
            data_dir=self.config['data_path'],
            split='train',
            max_files=self.config.get('max_files', None),
            use_augmentation=self.config.get('augment', True)
        )

        # éªŒè¯æ•°æ®é›†
        val_dataset = SpatialScanContextDataset(
            data_dir=self.config['data_path'],
            split='val',
            max_files=self.config.get('max_files', None),
            use_augmentation=False
        )
        
        self.logger.info(f"ğŸ“Š è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        self.logger.info(f"ğŸ“Š éªŒè¯é›†å¤§å°: {len(val_dataset)}")
        
        # æ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=self.config.get('num_workers', 4),
            pin_memory=True
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=self.config.get('num_workers', 4),
            pin_memory=True
        )
        
        return train_loader, val_loader
    
    def _create_criterion(self):
        """åˆ›å»ºæŸå¤±å‡½æ•°"""
        return AdaptiveTripletLoss(
            margin=self.config.get('margin', 0.5),
            adaptive_margin=True
        )
    
    def _create_optimizer(self):
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        return optim.Adam(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config.get('weight_decay', 1e-4)
        )
    
    def _create_scheduler(self):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ - é€‚åˆ300è½®è®­ç»ƒ"""
        return optim.lr_scheduler.MultiStepLR(
            self.optimizer,
            milestones=[100, 200, 250],  # åœ¨100, 200, 250è½®æ—¶é™ä½å­¦ä¹ ç‡
            gamma=0.5
        )
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        num_batches = len(self.train_loader)
        
        if epoch % 10 == 0:  # æ¯10è½®è¯¦ç»†è®°å½•
            self.logger.info(f"ğŸ”„ Epoch {epoch+1}/{self.config['epochs']} - å¼€å§‹è®­ç»ƒ")
        
        for batch_idx, (scan_contexts, labels) in enumerate(self.train_loader):
            scan_contexts = scan_contexts.to(self.device)
            labels = labels.to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            descriptors = self.model(scan_contexts)
            
            # è®¡ç®—æŸå¤±
            loss = self.criterion(descriptors, labels)
            
            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            
            # å‡å°‘æ‰“å°é¢‘ç‡ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
            if epoch % 10 == 0 and (batch_idx + 1) % 10 == 0:
                avg_loss = total_loss / (batch_idx + 1)
                self.logger.info(
                    f"   Batch {batch_idx+1}/{num_batches}, "
                    f"Loss: {loss.item():.4f}, "
                    f"Avg Loss: {avg_loss:.4f}"
                )
        
        avg_loss = total_loss / num_batches
        self.train_losses.append(avg_loss)
        
        if epoch % 10 == 0:
            self.logger.info(f"âœ… Epoch {epoch+1} è®­ç»ƒå®Œæˆ, å¹³å‡æŸå¤±: {avg_loss:.4f}")
        
        return avg_loss
    
    def validate(self, epoch):
        """éªŒè¯æ¨¡å‹ - å‡å°‘éªŒè¯é¢‘ç‡"""
        if epoch % 20 != 0 and epoch != self.config['epochs'] - 1:  # æ¯20è½®éªŒè¯ä¸€æ¬¡
            return None
            
        self.logger.info(f"ğŸ” Epoch {epoch+1} - å¼€å§‹éªŒè¯")
        
        # è¯„ä¼°æ¨¡å‹
        metrics = evaluate_model(
            self.model,
            self.val_loader,
            self.device,
            top_k_list=[1, 3, 5, 10],
            logger=self.logger
        )
        
        self.val_metrics.append({'epoch': epoch, 'metrics': metrics})
        
        # è®°å½•æŒ‡æ ‡
        self.logger.info(f"ğŸ“Š éªŒè¯ç»“æœ:")
        self.logger.info(f"   Top-1: {metrics['top_1']:.4f}")
        self.logger.info(f"   Top-5: {metrics['top_5']:.4f}")
        self.logger.info(f"   mAP: {metrics['mAP']:.4f}")
        
        return metrics
    
    def save_model(self, epoch, metrics=None, is_best=False, is_checkpoint=False):
        """ä¿å­˜æ¨¡å‹"""
        timestamp = get_timestamp()
        
        # æ¨¡å‹çŠ¶æ€
        model_state = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'config': self.config,
            'model_name': self.model_name,
            'metrics': metrics,
            'best_mAP': self.best_map,
            'best_top1': self.best_top1,
            'train_losses': self.train_losses,
            'val_metrics': self.val_metrics
        }
        
        # ä¿å­˜è·¯å¾„
        models_dir = Path("outputs/sc_standard_spatial_cnn/models")
        models_dir.mkdir(parents=True, exist_ok=True)
        
        if is_best:
            model_path = models_dir / f"best_{self.model_name}_{timestamp}.pth"
            self.logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {model_path}")
        elif is_checkpoint:
            model_path = models_dir / f"{self.model_name}_checkpoint_epoch_{epoch+1}_{timestamp}.pth"
            self.logger.info(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {model_path}")
        else:
            model_path = models_dir / f"{self.model_name}_final_{timestamp}.pth"
            self.logger.info(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹: {model_path}")
        
        torch.save(model_state, model_path)
        
        return model_path
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        self.logger.info(f"ğŸ¯ å¼€å§‹{self.model_name}é•¿æœŸè®­ç»ƒ - {self.config['epochs']}è½®")
        
        start_time = time.time()
        
        for epoch in range(self.config['epochs']):
            # è®­ç»ƒ
            train_loss = self.train_epoch(epoch)
            
            # éªŒè¯ (æ¯20è½®)
            metrics = self.validate(epoch)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            
            if epoch % 50 == 0:  # æ¯50è½®è®°å½•å­¦ä¹ ç‡
                current_lr = self.optimizer.param_groups[0]['lr']
                self.logger.info(f"ğŸ“ˆ Epoch {epoch+1} å­¦ä¹ ç‡: {current_lr:.6f}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
            is_best = False
            if metrics and metrics['mAP'] > self.best_map:
                self.best_map = metrics['mAP']
                is_best = True
                self.save_model(epoch, metrics, is_best=True)
                self.logger.info(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹! mAP: {self.best_map:.4f}")
            
            if metrics and metrics['top_1'] > self.best_top1:
                self.best_top1 = metrics['top_1']
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.save_interval == 0:
                self.save_model(epoch, metrics, is_checkpoint=True)
        
        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        self.logger.info(f"ğŸ‰ {self.model_name}è®­ç»ƒå®Œæˆ! æ€»ç”¨æ—¶: {total_time/3600:.2f}å°æ—¶")
        self.logger.info(f"ğŸ† æœ€ä½³mAP: {self.best_map:.4f}")
        self.logger.info(f"ğŸ¯ æœ€ä½³Top-1: {self.best_top1:.4f}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = self.save_model(self.config['epochs']-1, metrics)
        
        # ä¿å­˜è®­ç»ƒç»“æœ
        self.save_results()
        
        return {
            'model_name': self.model_name,
            'best_mAP': self.best_map,
            'best_top1': self.best_top1,
            'final_model_path': final_model_path,
            'total_time_hours': total_time/3600
        }
    
    def save_results(self):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        timestamp = get_timestamp()
        
        results = {
            'model_name': self.model_name,
            'config': self.config,
            'best_mAP': self.best_map,
            'best_top1': self.best_top1,
            'train_losses': self.train_losses,
            'val_metrics': self.val_metrics,
            'model_info': self.model.get_model_info(),
            'timestamp': timestamp,
            'total_epochs': self.config['epochs']
        }
        
        # ä¿å­˜è·¯å¾„
        results_dir = Path("outputs/sc_standard_spatial_cnn/results")
        results_dir.mkdir(parents=True, exist_ok=True)
        
        results_path = results_dir / f"{self.model_name}_300epochs_results_{timestamp}.json"
        
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info(f"ğŸ“Š ä¿å­˜è®­ç»ƒç»“æœ: {results_path}")

def main():
    """ä¸»å‡½æ•° - è®­ç»ƒæŒ‡å®šç‰ˆæœ¬"""
    parser = argparse.ArgumentParser(description='SCStandardSpatialCNN 300è½®é•¿æœŸè®­ç»ƒ')
    parser.add_argument('--epochs', type=int, default=300, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=0.001, help='å­¦ä¹ ç‡')
    parser.add_argument('--max_files', type=int, default=600, help='æœ€å¤§æ–‡ä»¶æ•°')
    parser.add_argument('--device', type=str, default='cpu', help='è®¾å¤‡')
    parser.add_argument('--model_type', choices=['spatial_only', 'spatial_channel'], 
                       default='spatial_channel', help='æ¨¡å‹ç±»å‹')
    
    args = parser.parse_args()
    
    # åŸºç¡€é…ç½®
    config = {
        'epochs': args.epochs,
        'batch_size': args.batch_size,
        'learning_rate': args.learning_rate,
        'max_files': args.max_files,
        'device': args.device,
        'data_path': 'data/raw/ply_files',
        'input_channels': 1,
        'descriptor_dim': 256,
        'margin': 0.5,
        'weight_decay': 1e-4,
        'num_workers': 4,
        'augment': True,
        'save_interval': 50
    }
    
    model_name = f"sc_standard_{args.model_type}"
    
    print(f"ğŸš€ å¼€å§‹SCStandardSpatialCNN 300è½®è®­ç»ƒ")
    print(f"ğŸ“Š é…ç½®: {config}")
    print(f"ğŸ¯ æ¨¡å‹ç±»å‹: {model_name}")
    
    try:
        trainer = SpatialCNNTrainer(config, model_name)
        result = trainer.train()
        
        print("âœ… è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ† æœ€ä½³mAP: {result['best_mAP']:.4f}")
        print(f"ğŸ¯ æœ€ä½³Top-1: {result['best_top1']:.4f}")
        print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {result['total_time_hours']:.2f}å°æ—¶")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()
