#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åŸºäºåœºæ™¯å†…å®¹å˜åŒ–çš„è½¨è¿¹åˆ†æ®µæ£€æµ‹å™¨
"""

import numpy as np
import matplotlib.pyplot as plt
from utils.scan_context import ScanContext
from utils.ply_reader import PLYReader
import glob
from pathlib import Path
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from scipy.spatial.distance import cosine, euclidean
from scipy.signal import find_peaks
import pickle

class SceneChangeDetector:
    """åœºæ™¯å˜åŒ–æ£€æµ‹å™¨"""
    
    def __init__(self, similarity_threshold=0.8, min_segment_length=20):
        self.similarity_threshold = similarity_threshold
        self.min_segment_length = min_segment_length
        self.sc_generator = ScanContext()
        
    def compute_scene_features(self, data_dir):
        """è®¡ç®—æ‰€æœ‰å¸§çš„åœºæ™¯ç‰¹å¾"""
        print("ğŸ” è®¡ç®—åœºæ™¯ç‰¹å¾...")
        
        ply_files = sorted(glob.glob(f"{data_dir}/*.ply"))
        print(f"æ‰¾åˆ° {len(ply_files)} ä¸ªplyæ–‡ä»¶")
        
        features = []
        valid_indices = []
        
        for i, ply_file in enumerate(ply_files):
            if i % 100 == 0:
                print(f"  å¤„ç† {i+1}/{len(ply_files)}")
            
            try:
                points = PLYReader.read_ply_file(ply_file)
                if points is not None and len(points) > 100:
                    points = points[:, :3]
                    
                    # è®¡ç®—å¤šç§åœºæ™¯ç‰¹å¾
                    scene_features = self.extract_scene_features(points)
                    if scene_features is not None:
                        features.append(scene_features)
                        valid_indices.append(i)
                        
            except Exception as e:
                print(f"å¤„ç†å¤±è´¥ {ply_file}: {e}")
                continue
        
        features = np.array(features)
        print(f"æˆåŠŸæå– {len(features)} ä¸ªåœºæ™¯ç‰¹å¾")
        
        return features, valid_indices
    
    def extract_scene_features(self, points):
        """æå–å¤šç»´åœºæ™¯ç‰¹å¾"""
        try:
            # 1. ScanContextç‰¹å¾
            sc = self.sc_generator.generate_scan_context(points)
            if sc is None:
                return None
            
            # 2. ç‚¹äº‘ç»Ÿè®¡ç‰¹å¾
            stats_features = self.compute_point_cloud_stats(points)
            
            # 3. ç©ºé—´åˆ†å¸ƒç‰¹å¾
            spatial_features = self.compute_spatial_features(points)
            
            # 4. ScanContextç»Ÿè®¡ç‰¹å¾
            sc_stats = self.compute_scancontext_stats(sc)
            
            # åˆå¹¶æ‰€æœ‰ç‰¹å¾
            all_features = np.concatenate([
                sc.flatten(),           # ScanContextåŸå§‹ç‰¹å¾ (1200ç»´)
                stats_features,         # ç»Ÿè®¡ç‰¹å¾ (10ç»´)
                spatial_features,       # ç©ºé—´ç‰¹å¾ (15ç»´)
                sc_stats               # ScanContextç»Ÿè®¡ç‰¹å¾ (8ç»´)
            ])
            
            return all_features
            
        except Exception as e:
            print(f"ç‰¹å¾æå–å¤±è´¥: {e}")
            return None
    
    def compute_point_cloud_stats(self, points):
        """è®¡ç®—ç‚¹äº‘ç»Ÿè®¡ç‰¹å¾"""
        # è·ç¦»ç»Ÿè®¡
        distances = np.linalg.norm(points[:, :2], axis=1)  # åˆ°åŸç‚¹è·ç¦»
        heights = points[:, 2]  # é«˜åº¦
        
        features = [
            np.mean(distances),      # å¹³å‡è·ç¦»
            np.std(distances),       # è·ç¦»æ ‡å‡†å·®
            np.min(distances),       # æœ€å°è·ç¦»
            np.max(distances),       # æœ€å¤§è·ç¦»
            np.mean(heights),        # å¹³å‡é«˜åº¦
            np.std(heights),         # é«˜åº¦æ ‡å‡†å·®
            np.min(heights),         # æœ€å°é«˜åº¦
            np.max(heights),         # æœ€å¤§é«˜åº¦
            len(points),             # ç‚¹æ•°
            np.mean(np.abs(heights)) # å¹³å‡ç»å¯¹é«˜åº¦
        ]
        
        return np.array(features)
    
    def compute_spatial_features(self, points):
        """è®¡ç®—ç©ºé—´åˆ†å¸ƒç‰¹å¾"""
        # è§’åº¦åˆ†å¸ƒ
        angles = np.arctan2(points[:, 1], points[:, 0])
        angle_hist, _ = np.histogram(angles, bins=8, range=(-np.pi, np.pi))
        angle_hist = angle_hist / len(points)  # å½’ä¸€åŒ–
        
        # è·ç¦»åˆ†å¸ƒ
        distances = np.linalg.norm(points[:, :2], axis=1)
        dist_hist, _ = np.histogram(distances, bins=5, range=(0, 50))
        dist_hist = dist_hist / len(points)  # å½’ä¸€åŒ–
        
        # å¯†åº¦ç‰¹å¾
        density_near = np.sum(distances < 10) / len(points)  # è¿‘è·ç¦»ç‚¹å¯†åº¦
        density_far = np.sum(distances > 30) / len(points)   # è¿œè·ç¦»ç‚¹å¯†åº¦
        
        features = np.concatenate([
            angle_hist,      # è§’åº¦åˆ†å¸ƒ (8ç»´)
            dist_hist,       # è·ç¦»åˆ†å¸ƒ (5ç»´)
            [density_near, density_far]  # å¯†åº¦ç‰¹å¾ (2ç»´)
        ])
        
        return features
    
    def compute_scancontext_stats(self, sc):
        """è®¡ç®—ScanContextç»Ÿè®¡ç‰¹å¾"""
        features = [
            np.mean(sc),                    # å¹³å‡å€¼
            np.std(sc),                     # æ ‡å‡†å·®
            np.max(sc),                     # æœ€å¤§å€¼
            np.min(sc),                     # æœ€å°å€¼
            np.count_nonzero(sc) / sc.size, # éé›¶æ¯”ä¾‹
            np.mean(np.max(sc, axis=0)),    # æ¯åˆ—æœ€å¤§å€¼çš„å¹³å‡
            np.mean(np.max(sc, axis=1)),    # æ¯è¡Œæœ€å¤§å€¼çš„å¹³å‡
            np.sum(sc > 0.5) / sc.size      # é«˜å€¼æ¯”ä¾‹
        ]
        
        return np.array(features)
    
    def detect_scene_changes(self, features):
        """æ£€æµ‹åœºæ™¯å˜åŒ–ç‚¹"""
        print("ğŸ” æ£€æµ‹åœºæ™¯å˜åŒ–...")
        
        # 1. è®¡ç®—ç›¸é‚»å¸§ä¹‹é—´çš„ç›¸ä¼¼åº¦
        similarities = []
        for i in range(1, len(features)):
            # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
            sim = 1 - cosine(features[i-1], features[i])
            similarities.append(sim)
        
        similarities = np.array(similarities)
        
        # 2. æ£€æµ‹ç›¸ä¼¼åº¦çš„æ˜¾è‘—ä¸‹é™ç‚¹
        # ä½¿ç”¨æ»‘åŠ¨çª—å£å¹³æ»‘
        window_size = 5
        smoothed_sim = np.convolve(similarities, np.ones(window_size)/window_size, mode='same')
        
        # è®¡ç®—ç›¸ä¼¼åº¦çš„è´Ÿæ¢¯åº¦ï¼ˆä¸‹é™ç¨‹åº¦ï¼‰
        gradient = -np.gradient(smoothed_sim)
        
        # æ‰¾åˆ°æ¢¯åº¦å³°å€¼ï¼ˆç›¸ä¼¼åº¦æ˜¾è‘—ä¸‹é™çš„ç‚¹ï¼‰
        peaks, properties = find_peaks(gradient, 
                                     height=np.std(gradient),  # é«˜åº¦é˜ˆå€¼
                                     distance=self.min_segment_length)  # æœ€å°é—´è·
        
        change_points = peaks + 1  # +1å› ä¸ºsimilaritiesæ¯”featureså°‘1ä¸ª
        
        print(f"æ£€æµ‹åˆ° {len(change_points)} ä¸ªåœºæ™¯å˜åŒ–ç‚¹")
        
        return similarities, change_points, gradient
    
    def create_segments(self, change_points, total_frames):
        """æ ¹æ®å˜åŒ–ç‚¹åˆ›å»ºåˆ†æ®µ"""
        segments = []
        
        # æ·»åŠ èµ·å§‹ç‚¹
        segment_starts = [0] + list(change_points) + [total_frames]
        
        for i in range(len(segment_starts) - 1):
            start = segment_starts[i]
            end = segment_starts[i + 1]
            segments.append((start, end))
        
        print(f"åˆ›å»ºäº† {len(segments)} ä¸ªåˆ†æ®µ:")
        for i, (start, end) in enumerate(segments):
            print(f"  æ®µ {i}: å¸§ {start:4d} - {end:4d} (é•¿åº¦: {end-start:3d})")
        
        return segments
    
    def cluster_scenes(self, features, n_clusters=None):
        """ä½¿ç”¨èšç±»æ–¹æ³•åˆ†æåœºæ™¯ç±»å‹"""
        print("ğŸ” åœºæ™¯èšç±»åˆ†æ...")
        
        if n_clusters is None:
            # è‡ªåŠ¨ç¡®å®šæœ€ä½³èšç±»æ•°
            silhouette_scores = []
            K_range = range(2, min(21, len(features)//10))
            
            for k in K_range:
                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                labels = kmeans.fit_predict(features)
                score = silhouette_score(features, labels)
                silhouette_scores.append(score)
            
            best_k = K_range[np.argmax(silhouette_scores)]
            print(f"æœ€ä½³èšç±»æ•°: {best_k} (è½®å»“ç³»æ•°: {max(silhouette_scores):.3f})")
        else:
            best_k = n_clusters
        
        # æ‰§è¡Œèšç±»
        kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(features)
        
        # åˆ†æèšç±»ç»“æœ
        print("èšç±»ç»“æœ:")
        for i in range(best_k):
            cluster_frames = np.where(cluster_labels == i)[0]
            print(f"  èšç±» {i}: {len(cluster_frames)} å¸§")
        
        return cluster_labels, kmeans
    
    def visualize_analysis(self, similarities, change_points, gradient, cluster_labels=None):
        """å¯è§†åŒ–åˆ†æç»“æœ"""
        fig, axes = plt.subplots(3, 1, figsize=(15, 10))
        
        # 1. ç›¸ä¼¼åº¦æ›²çº¿
        axes[0].plot(similarities, 'b-', alpha=0.7, label='å¸§é—´ç›¸ä¼¼åº¦')
        axes[0].axhline(y=self.similarity_threshold, color='r', linestyle='--', label='é˜ˆå€¼')
        for cp in change_points:
            if cp < len(similarities):
                axes[0].axvline(x=cp, color='red', alpha=0.7)
        axes[0].set_ylabel('ç›¸ä¼¼åº¦')
        axes[0].set_title('å¸§é—´ç›¸ä¼¼åº¦å˜åŒ–')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # 2. æ¢¯åº¦æ›²çº¿
        axes[1].plot(gradient, 'g-', alpha=0.7, label='ç›¸ä¼¼åº¦æ¢¯åº¦')
        axes[1].plot(change_points, gradient[change_points], 'ro', markersize=8, label='å˜åŒ–ç‚¹')
        axes[1].set_ylabel('æ¢¯åº¦')
        axes[1].set_title('ç›¸ä¼¼åº¦æ¢¯åº¦ï¼ˆåœºæ™¯å˜åŒ–æ£€æµ‹ï¼‰')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        # 3. èšç±»ç»“æœ
        if cluster_labels is not None:
            axes[2].plot(cluster_labels, 'o-', markersize=3, alpha=0.7)
            axes[2].set_ylabel('èšç±»æ ‡ç­¾')
            axes[2].set_title('åœºæ™¯èšç±»ç»“æœ')
            axes[2].grid(True, alpha=0.3)
        
        axes[2].set_xlabel('å¸§ç´¢å¼•')
        
        plt.tight_layout()
        plt.savefig('scene_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()  # å…³é—­å›¾å½¢ï¼Œä¸æ˜¾ç¤º
        
        print("ğŸ“Š åˆ†æå›¾è¡¨å·²ä¿å­˜ä¸º scene_analysis.png")
    
    def save_results(self, features, similarities, change_points, segments, cluster_labels, filename='scene_analysis_results.pkl'):
        """ä¿å­˜åˆ†æç»“æœ"""
        results = {
            'features': features,
            'similarities': similarities,
            'change_points': change_points,
            'segments': segments,
            'cluster_labels': cluster_labels,
            'similarity_threshold': self.similarity_threshold,
            'min_segment_length': self.min_segment_length
        }
        
        with open(filename, 'wb') as f:
            pickle.dump(results, f)
        
        print(f"ğŸ“ åˆ†æç»“æœå·²ä¿å­˜åˆ° {filename}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ åŸºäºåœºæ™¯å†…å®¹å˜åŒ–çš„è½¨è¿¹åˆ†æ®µåˆ†æ")
    print("="*60)
    
    # æ•°æ®è·¯å¾„
    data_dir = "/mysda/shared_dir/2025.7.3/2025-07-03-16-28-57.ply"
    
    # åˆ›å»ºæ£€æµ‹å™¨
    detector = SceneChangeDetector(similarity_threshold=0.8, min_segment_length=20)
    
    # 1. è®¡ç®—åœºæ™¯ç‰¹å¾
    features, valid_indices = detector.compute_scene_features(data_dir)
    
    if len(features) == 0:
        print("âŒ æœªèƒ½æå–åˆ°æœ‰æ•ˆç‰¹å¾")
        return
    
    print(f"âœ… ç‰¹å¾ç»´åº¦: {features.shape}")
    
    # 2. æ£€æµ‹åœºæ™¯å˜åŒ–
    similarities, change_points, gradient = detector.detect_scene_changes(features)
    
    # 3. åˆ›å»ºåˆ†æ®µ
    segments = detector.create_segments(change_points, len(features))
    
    # 4. èšç±»åˆ†æ
    cluster_labels, kmeans = detector.cluster_scenes(features)
    
    # 5. å¯è§†åŒ–ç»“æœ
    detector.visualize_analysis(similarities, change_points, gradient, cluster_labels)
    
    # 6. ä¿å­˜ç»“æœ
    detector.save_results(features, similarities, change_points, segments, cluster_labels)
    
    # 7. åˆ†ææ€»ç»“
    print("\nğŸ“Š åˆ†ææ€»ç»“:")
    print(f"æ€»å¸§æ•°: {len(features)}")
    print(f"æ£€æµ‹åˆ°çš„å˜åŒ–ç‚¹: {len(change_points)}")
    print(f"åˆ†æ®µæ•°é‡: {len(segments)}")
    print(f"èšç±»æ•°é‡: {len(np.unique(cluster_labels))}")
    print(f"å¹³å‡åˆ†æ®µé•¿åº¦: {len(features) / len(segments):.1f} å¸§")
    
    # åˆ†æ®µé•¿åº¦åˆ†å¸ƒ
    segment_lengths = [end - start for start, end in segments]
    print(f"åˆ†æ®µé•¿åº¦èŒƒå›´: {min(segment_lengths)} - {max(segment_lengths)} å¸§")
    
    return features, similarities, change_points, segments, cluster_labels

if __name__ == '__main__':
    main()
