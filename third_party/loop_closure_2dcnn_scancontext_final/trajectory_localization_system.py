#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åŸºäºScanContext + CNNçš„è½¨è¿¹å®šä½ç³»ç»Ÿ
ç›®æ ‡ï¼šè¯†åˆ«æœºå™¨äººåœ¨è½¨è¿¹ä¸­çš„å…·ä½“ä½ç½®
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from models.cnn_2d_models import Simple2DCNN, Enhanced2DCNN, ResNet2D
from utils.scan_context import ScanContext
from utils.ply_reader import PLYReader
import glob
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import pickle
import time

class TrajectoryLocalizationSystem:
    """ä¼˜åŒ–çš„è½¨è¿¹å®šä½ç³»ç»Ÿ"""

    def __init__(self, num_locations=20, model_type='simple2dcnn', adaptive_segments=True):
        self.num_locations = num_locations
        self.model_type = model_type
        self.adaptive_segments = adaptive_segments
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.sc_generator = ScanContext()

        print(f"ğŸ¯ ä¼˜åŒ–çš„è½¨è¿¹å®šä½ç³»ç»Ÿ")
        print(f"è®¾å¤‡: {self.device}")
        print(f"ç›®æ ‡ä½ç½®æ•°: {num_locations}")
        print(f"æ¨¡å‹ç±»å‹: {model_type}")
        print(f"è‡ªé€‚åº”åˆ†æ®µ: {adaptive_segments}")
        print(f"ç›®æ ‡: åŸºäºScanContextç‰¹å¾è¿›è¡Œç²¾ç¡®è½¨è¿¹å®šä½")

        # åˆå§‹åŒ–æ¨¡å‹
        if model_type == 'simple2dcnn':
            self.model = Simple2DCNN(num_classes=num_locations)
        elif model_type == 'enhanced2dcnn':
            self.model = Enhanced2DCNN(num_classes=num_locations)
        elif model_type == 'resnet2d':
            self.model = ResNet2D(num_classes=num_locations)
        else:
            raise ValueError(f"æœªçŸ¥æ¨¡å‹ç±»å‹: {model_type}")

        self.model = self.model.to(self.device)

        # ä½ç½®ä¿¡æ¯å­˜å‚¨
        self.location_database = {}  # å­˜å‚¨æ¯ä¸ªä½ç½®çš„ä»£è¡¨æ€§ScanContext
        self.location_features = []  # æ‰€æœ‰ä½ç½®çš„ç‰¹å¾
        self.location_labels = []    # å¯¹åº”çš„ä½ç½®æ ‡ç­¾

        # ä¼˜åŒ–å‚æ•°
        self.confidence_threshold = 0.7  # ç½®ä¿¡åº¦é˜ˆå€¼
        self.temporal_smoothing = True   # æ—¶åºå¹³æ»‘
        self.location_history = []       # ä½ç½®å†å²
        self.confidence_history = []     # ç½®ä¿¡åº¦å†å²
        
    def create_adaptive_location_database(self, data_dir, save_path='location_database.pkl'):
        """åˆ›å»ºè‡ªé€‚åº”ä½ç½®æ•°æ®åº“ï¼ˆåŸºäºåœºæ™¯å˜åŒ–ï¼‰"""
        print(f"ğŸ“ åˆ›å»ºè‡ªé€‚åº”è½¨è¿¹ä½ç½®æ•°æ®åº“...")

        if self.adaptive_segments:
            # ä½¿ç”¨åœºæ™¯å˜åŒ–æ£€æµ‹æ¥ç¡®å®šåˆ†æ®µ
            from scene_change_detector import SceneChangeDetector
            detector = SceneChangeDetector(similarity_threshold=0.75, min_segment_length=15)

            # è®¡ç®—åœºæ™¯ç‰¹å¾
            features, valid_indices = detector.compute_scene_features(data_dir)
            if len(features) == 0:
                print("âŒ æ— æ³•æå–åœºæ™¯ç‰¹å¾ï¼Œå›é€€åˆ°å‡åŒ€åˆ†æ®µ")
                return self.create_uniform_location_database(data_dir, save_path)

            # æ£€æµ‹åœºæ™¯å˜åŒ–
            similarities, change_points, gradient = detector.detect_scene_changes(features)
            segments = detector.create_segments(change_points, len(features))

            print(f"ğŸ¯ åŸºäºåœºæ™¯å˜åŒ–æ£€æµ‹åˆ° {len(segments)} ä¸ªè‡ªç„¶åˆ†æ®µ")

            # å¦‚æœåˆ†æ®µæ•°é‡ä¸ç›®æ ‡ä¸ç¬¦ï¼Œè°ƒæ•´
            if len(segments) != self.num_locations:
                print(f"âš ï¸  åˆ†æ®µæ•°é‡({len(segments)})ä¸ç›®æ ‡({self.num_locations})ä¸ç¬¦ï¼Œè¿›è¡Œè°ƒæ•´")
                segments = self.adjust_segments(segments, self.num_locations)

            return self.create_database_from_segments(data_dir, segments, save_path)
        else:
            return self.create_uniform_location_database(data_dir, save_path)

    def create_uniform_location_database(self, data_dir, save_path='location_database.pkl'):
        """åˆ›å»ºå‡åŒ€åˆ†æ®µçš„ä½ç½®æ•°æ®åº“"""
        print(f"ğŸ“ åˆ›å»ºå‡åŒ€åˆ†æ®µä½ç½®æ•°æ®åº“...")

        # è·å–æ‰€æœ‰plyæ–‡ä»¶
        ply_files = sorted(glob.glob(f"{data_dir}/*.ply"))
        print(f"æ‰¾åˆ° {len(ply_files)} ä¸ªplyæ–‡ä»¶")

        if len(ply_files) == 0:
            print("âŒ æœªæ‰¾åˆ°plyæ–‡ä»¶")
            return False

        # è®¡ç®—æ¯ä¸ªä½ç½®æ®µçš„æ–‡ä»¶èŒƒå›´
        files_per_location = len(ply_files) // self.num_locations
        print(f"æ¯ä¸ªä½ç½®æ®µåŒ…å«çº¦ {files_per_location} ä¸ªæ–‡ä»¶")

        location_data = {}
        all_features = []
        all_labels = []

        for location_id in range(self.num_locations):
            print(f"  å¤„ç†ä½ç½® {location_id+1}/{self.num_locations}")

            # ç¡®å®šè¿™ä¸ªä½ç½®çš„æ–‡ä»¶èŒƒå›´
            start_idx = location_id * files_per_location
            if location_id == self.num_locations - 1:
                end_idx = len(ply_files)  # æœ€åä¸€ä¸ªä½ç½®åŒ…å«å‰©ä½™æ‰€æœ‰æ–‡ä»¶
            else:
                end_idx = (location_id + 1) * files_per_location

            location_files = ply_files[start_idx:end_idx]
            location_features = []

            # å¤„ç†è¿™ä¸ªä½ç½®çš„æ‰€æœ‰æ–‡ä»¶
            for ply_file in location_files:
                try:
                    points = PLYReader.read_ply_file(ply_file)
                    if points is not None and len(points) > 100:
                        points = points[:, :3]
                        sc = self.sc_generator.generate_scan_context(points)

                        if sc is not None:
                            location_features.append(sc)
                            all_features.append(sc)
                            all_labels.append(location_id)

                except Exception as e:
                    print(f"    å¤„ç†å¤±è´¥ {ply_file}: {e}")
                    continue

            if len(location_features) > 0:
                # è®¡ç®—è¿™ä¸ªä½ç½®çš„ä»£è¡¨æ€§ç‰¹å¾ï¼ˆå¹³å‡å€¼ï¼‰
                representative_sc = np.mean(location_features, axis=0)
                location_data[location_id] = {
                    'representative_sc': representative_sc,
                    'sample_count': len(location_features),
                    'file_range': (start_idx, end_idx),
                    'all_features': location_features
                }
                print(f"    ä½ç½® {location_id}: {len(location_features)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
            else:
                print(f"    âš ï¸  ä½ç½® {location_id}: æ— æœ‰æ•ˆæ ·æœ¬")

        self.location_database = location_data
        self.location_features = np.array(all_features)
        self.location_labels = np.array(all_labels)

        # ä¿å­˜ä½ç½®æ•°æ®åº“
        with open(save_path, 'wb') as f:
            pickle.dump({
                'location_database': self.location_database,
                'location_features': self.location_features,
                'location_labels': self.location_labels,
                'num_locations': self.num_locations,
                'adaptive_segments': False
            }, f)

        print(f"âœ… ä½ç½®æ•°æ®åº“å·²ä¿å­˜åˆ°: {save_path}")
        print(f"æ€»æ ·æœ¬æ•°: {len(all_features)}")
        print(f"ä½ç½®åˆ†å¸ƒ: {np.bincount(all_labels)}")

        return True

    def create_location_database(self, data_dir, save_path='location_database.pkl'):
        """åˆ›å»ºä½ç½®æ•°æ®åº“ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰"""
        return self.create_adaptive_location_database(data_dir, save_path)
    
    def load_location_database(self, load_path='location_database.pkl'):
        """åŠ è½½ä½ç½®æ•°æ®åº“"""
        if not Path(load_path).exists():
            print(f"âŒ ä½ç½®æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {load_path}")
            return False
        
        with open(load_path, 'rb') as f:
            data = pickle.load(f)
        
        self.location_database = data['location_database']
        self.location_features = data['location_features']
        self.location_labels = data['location_labels']
        self.num_locations = data['num_locations']
        
        print(f"âœ… å·²åŠ è½½ä½ç½®æ•°æ®åº“")
        print(f"ä½ç½®æ•°é‡: {self.num_locations}")
        print(f"æ€»æ ·æœ¬æ•°: {len(self.location_features)}")
        
        return True
    
    def train_localization_model(self, epochs=50, batch_size=32):
        """è®­ç»ƒå®šä½æ¨¡å‹"""
        print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒè½¨è¿¹å®šä½æ¨¡å‹...")
        
        if len(self.location_features) == 0:
            print("âŒ æ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œè¯·å…ˆåˆ›å»ºä½ç½®æ•°æ®åº“")
            return False
        
        # æ•°æ®åˆ’åˆ† - å¤„ç†æ ·æœ¬ä¸è¶³çš„ç±»åˆ«
        print("ğŸ” æ£€æŸ¥æ•°æ®åˆ†å¸ƒ...")
        unique_labels, counts = np.unique(self.location_labels, return_counts=True)
        min_samples = np.min(counts)

        if min_samples < 3:
            print(f"âš ï¸  æ£€æµ‹åˆ°æ ·æœ¬ä¸è¶³çš„ç±»åˆ« (æœ€å°‘{min_samples}ä¸ªæ ·æœ¬)")
            print("ä½¿ç”¨éšæœºåˆ’åˆ†è€Œä¸æ˜¯åˆ†å±‚åˆ’åˆ†")

            # ä½¿ç”¨éšæœºåˆ’åˆ†
            X_train, X_temp, y_train, y_temp = train_test_split(
                self.location_features, self.location_labels,
                test_size=0.4, random_state=42
            )

            X_val, X_test, y_val, y_test = train_test_split(
                X_temp, y_temp, test_size=0.5, random_state=42
            )
        else:
            print("âœ… æ‰€æœ‰ç±»åˆ«æ ·æœ¬å……è¶³ï¼Œä½¿ç”¨åˆ†å±‚åˆ’åˆ†")
            # ä½¿ç”¨åˆ†å±‚åˆ’åˆ†
            X_train, X_temp, y_train, y_temp = train_test_split(
                self.location_features, self.location_labels,
                test_size=0.4, random_state=42, stratify=self.location_labels
            )

            X_val, X_test, y_val, y_test = train_test_split(
                X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
            )
        
        print(f"è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
        print(f"éªŒè¯é›†: {len(X_val)} æ ·æœ¬")
        print(f"æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        X_train_tensor = torch.FloatTensor(X_train).unsqueeze(1).to(self.device)
        X_val_tensor = torch.FloatTensor(X_val).unsqueeze(1).to(self.device)
        X_test_tensor = torch.FloatTensor(X_test).unsqueeze(1).to(self.device)
        
        y_train_tensor = torch.LongTensor(y_train).to(self.device)
        y_val_tensor = torch.LongTensor(y_val).to(self.device)
        y_test_tensor = torch.LongTensor(y_test).to(self.device)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        # è®­ç»ƒè®¾ç½®
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)
        
        # è®­ç»ƒå¾ªç¯
        best_val_acc = 0
        best_model_state = None
        patience = 10
        patience_counter = 0
        
        train_losses = []
        val_accuracies = []
        
        for epoch in range(epochs):
            # è®­ç»ƒ
            self.model.train()
            total_loss = 0
            
            for batch_idx, (data, target) in enumerate(train_loader):
                optimizer.zero_grad()
                output = self.model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            
            avg_train_loss = total_loss / len(train_loader)
            
            # éªŒè¯
            self.model.eval()
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for data, target in val_loader:
                    output = self.model(data)
                    _, predicted = torch.max(output.data, 1)
                    val_total += target.size(0)
                    val_correct += (predicted == target).sum().item()
            
            val_acc = 100. * val_correct / val_total
            
            train_losses.append(avg_train_loss)
            val_accuracies.append(val_acc)
            
            print(f'Epoch {epoch+1:2d}/{epochs:2d} | '
                  f'è®­ç»ƒæŸå¤±: {avg_train_loss:.4f} | '
                  f'éªŒè¯å‡†ç¡®ç‡: {val_acc:.1f}%')
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_model_state = self.model.state_dict().copy()
                patience_counter = 0
                print(f'ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.1f}%')
            else:
                patience_counter += 1
            
            # æ—©åœ
            if patience_counter >= patience:
                print(f'â¹ï¸  æ—©åœè§¦å‘')
                break
            
            scheduler.step()
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if best_model_state is not None:
            self.model.load_state_dict(best_model_state)
        
        # æµ‹è¯•
        self.model.eval()
        test_predictions = []
        test_targets = []
        
        with torch.no_grad():
            for data, target in test_loader:
                output = self.model(data)
                _, predicted = torch.max(output.data, 1)
                test_predictions.extend(predicted.cpu().numpy())
                test_targets.extend(target.cpu().numpy())
        
        test_acc = accuracy_score(test_targets, test_predictions) * 100
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.1f}%")
        print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.1f}%")
        
        # è¯¦ç»†åˆ†æ
        print(f"\nğŸ“Š è¯¦ç»†å®šä½æ€§èƒ½åˆ†æ:")
        
        # è®¡ç®—æ¯ä¸ªä½ç½®çš„å‡†ç¡®ç‡
        location_accuracies = {}
        for location_id in range(self.num_locations):
            location_mask = np.array(test_targets) == location_id
            if np.sum(location_mask) > 0:
                location_predictions = np.array(test_predictions)[location_mask]
                location_targets = np.array(test_targets)[location_mask]
                location_acc = accuracy_score(location_targets, location_predictions) * 100
                location_accuracies[location_id] = location_acc
                sample_count = np.sum(location_mask)
                print(f"  ä½ç½® {location_id:2d}: {location_acc:6.1f}% ({sample_count:2d} æ ·æœ¬)")
        
        avg_location_acc = np.mean(list(location_accuracies.values()))
        print(f"\nå¹³å‡ä½ç½®å‡†ç¡®ç‡: {avg_location_acc:.1f}%")

        # --- BEGIN MULTI-METRIC CALCULATION ---
        print("\nğŸ“Š è¯¦ç»†å¤šç»´åº¦æŒ‡æ ‡åˆ†æ (ç‹¬ç«‹æµ‹è¯•é›†):")
        errors = np.abs(np.array(test_predictions) - np.array(test_targets))
        acc_err1 = np.mean(errors <= 1) * 100
        acc_err2 = np.mean(errors <= 2) * 100
        
        try:
            report = classification_report(test_targets, test_predictions, output_dict=True, zero_division=0)
            precision = report['weighted avg']['precision']
            recall = report['weighted avg']['recall']
            f1 = report['weighted avg']['f1-score']
        except Exception:
            # Fallback for simpler sklearn versions
            from sklearn.metrics import precision_score, recall_score, f1_score
            precision = precision_score(test_targets, test_predictions, average='weighted', zero_division=0)
            recall = recall_score(test_targets, test_predictions, average='weighted', zero_division=0)
            f1 = f1_score(test_targets, test_predictions, average='weighted', zero_division=0)

        print(f"   - è¯¯å·®â‰¤1 å‡†ç¡®ç‡          : {acc_err1:.1f}%")
        print(f"   - è¯¯å·®â‰¤2 å‡†ç¡®ç‡          : {acc_err2:.1f}%")
        print(f"   - åŠ æƒç²¾ç¡®ç‡ (Precision) : {precision:.3f}")
        print(f"   - åŠ æƒå¬å›ç‡ (Recall)    : {recall:.3f}")
        print(f"   - åŠ æƒF1åˆ†æ•° (F1-Score)   : {f1:.3f}")
        # --- END MULTI-METRIC CALCULATION ---
        
        # ä¿å­˜æ¨¡å‹
        model_path = f"models/saved/trajectory_localizer_{self.model_type}_acc{test_acc:.1f}.pth"
        Path(model_path).parent.mkdir(parents=True, exist_ok=True)
        
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_type': self.model_type,
            'num_locations': self.num_locations,
            'test_accuracy': test_acc,
            'best_val_accuracy': best_val_acc,
            'location_accuracies': location_accuracies,
            'train_losses': train_losses,
            'val_accuracies': val_accuracies
        }, model_path)
        
        print(f"âœ… å®šä½æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
        
        return test_acc
    
    def localize_position(self, scan_context):
        """ä¼˜åŒ–çš„ä½ç½®å®šä½ï¼ˆåŒ…å«æ—¶åºå¹³æ»‘å’Œç½®ä¿¡åº¦è¿‡æ»¤ï¼‰"""
        if scan_context is None:
            return None, 0.0

        try:
            self.model.eval()
            sc_tensor = torch.FloatTensor(scan_context).unsqueeze(0).unsqueeze(0).to(self.device)

            with torch.no_grad():
                output = self.model(sc_tensor)
                probabilities = torch.softmax(output, dim=1)
                confidence, predicted_location = torch.max(probabilities, 1)

                predicted_location = predicted_location.item()
                confidence = confidence.item()

                # ç½®ä¿¡åº¦è¿‡æ»¤
                if confidence < self.confidence_threshold:
                    # ä½ç½®ä¿¡åº¦æ—¶ï¼Œå°è¯•ä½¿ç”¨å†å²ä¿¡æ¯
                    if len(self.location_history) > 0:
                        # ä½¿ç”¨æœ€è¿‘çš„é«˜ç½®ä¿¡åº¦ä½ç½®
                        recent_high_conf = [i for i, c in enumerate(self.confidence_history[-5:]) if c >= self.confidence_threshold]
                        if recent_high_conf:
                            last_reliable_idx = recent_high_conf[-1]
                            predicted_location = self.location_history[-(5-last_reliable_idx)]
                            confidence = 0.5  # æ ‡è®°ä¸ºä¸­ç­‰ç½®ä¿¡åº¦

                # æ—¶åºå¹³æ»‘
                if self.temporal_smoothing and len(self.location_history) > 0:
                    predicted_location = self.apply_temporal_smoothing(predicted_location, confidence)

                # æ›´æ–°å†å²
                self.location_history.append(predicted_location)
                self.confidence_history.append(confidence)

                # ä¿æŒå†å²é•¿åº¦
                if len(self.location_history) > 10:
                    self.location_history.pop(0)
                    self.confidence_history.pop(0)

                return predicted_location, confidence

        except Exception as e:
            print(f"å®šä½å¤±è´¥: {e}")
            return None, 0.0

    def apply_temporal_smoothing(self, current_prediction, current_confidence):
        """åº”ç”¨æ—¶åºå¹³æ»‘"""
        if len(self.location_history) < 2:
            return current_prediction

        # è·å–æœ€è¿‘çš„ä½ç½®
        recent_locations = self.location_history[-3:]
        recent_confidences = self.confidence_history[-3:]

        # å¦‚æœå½“å‰é¢„æµ‹ä¸æœ€è¿‘ä½ç½®å·®å¼‚å¾ˆå¤§ï¼Œä¸”ç½®ä¿¡åº¦ä¸é«˜ï¼Œåˆ™è¿›è¡Œå¹³æ»‘
        last_location = self.location_history[-1]
        location_diff = abs(current_prediction - last_location)

        if location_diff > 3 and current_confidence < 0.9:
            # è®¡ç®—åŠ æƒå¹³å‡
            weights = np.array(recent_confidences + [current_confidence])
            locations = np.array(recent_locations + [current_prediction])

            weighted_location = np.average(locations, weights=weights)
            smoothed_location = int(round(weighted_location))

            # ç¡®ä¿åœ¨æœ‰æ•ˆèŒƒå›´å†…
            smoothed_location = max(0, min(self.num_locations - 1, smoothed_location))

            return smoothed_location

        return current_prediction
    
    def load_trained_model(self, model_path):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        if not Path(model_path).exists():
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return False
        
        try:
            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.num_locations = checkpoint['num_locations']
            
            print(f"âœ… å·²åŠ è½½è®­ç»ƒå¥½çš„å®šä½æ¨¡å‹")
            print(f"æ¨¡å‹å‡†ç¡®ç‡: {checkpoint.get('test_accuracy', 'N/A'):.1f}%")
            
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return False

    def adjust_segments(self, segments, target_num):
        """è°ƒæ•´åˆ†æ®µæ•°é‡ä»¥åŒ¹é…ç›®æ ‡"""
        if len(segments) == target_num:
            return segments

        if len(segments) < target_num:
            # åˆ†æ®µå¤ªå°‘ï¼Œéœ€è¦ç»†åˆ†
            return self.split_segments(segments, target_num)
        else:
            # åˆ†æ®µå¤ªå¤šï¼Œéœ€è¦åˆå¹¶
            return self.merge_segments(segments, target_num)

    def split_segments(self, segments, target_num):
        """ç»†åˆ†åˆ†æ®µ"""
        print(f"ğŸ”„ ç»†åˆ† {len(segments)} ä¸ªåˆ†æ®µåˆ° {target_num} ä¸ª")

        # æ‰¾åˆ°æœ€é•¿çš„åˆ†æ®µè¿›è¡Œç»†åˆ†
        new_segments = list(segments)

        while len(new_segments) < target_num:
            # æ‰¾åˆ°æœ€é•¿çš„åˆ†æ®µ
            lengths = [(end - start, i) for i, (start, end) in enumerate(new_segments)]
            max_length, max_idx = max(lengths)

            if max_length <= 2:  # å¦‚æœæœ€é•¿åˆ†æ®µä¹Ÿå¾ˆçŸ­ï¼Œåœæ­¢ç»†åˆ†
                break

            # ç»†åˆ†æœ€é•¿çš„åˆ†æ®µ
            start, end = new_segments[max_idx]
            mid = (start + end) // 2
            new_segments[max_idx] = (start, mid)
            new_segments.insert(max_idx + 1, (mid, end))

        return new_segments

    def merge_segments(self, segments, target_num):
        """åˆå¹¶åˆ†æ®µ"""
        print(f"ğŸ”„ åˆå¹¶ {len(segments)} ä¸ªåˆ†æ®µåˆ° {target_num} ä¸ª")

        new_segments = list(segments)

        while len(new_segments) > target_num:
            # æ‰¾åˆ°æœ€çŸ­çš„ç›¸é‚»åˆ†æ®µå¯¹è¿›è¡Œåˆå¹¶
            min_combined_length = float('inf')
            merge_idx = 0

            for i in range(len(new_segments) - 1):
                start1, end1 = new_segments[i]
                start2, end2 = new_segments[i + 1]
                combined_length = (end1 - start1) + (end2 - start2)

                if combined_length < min_combined_length:
                    min_combined_length = combined_length
                    merge_idx = i

            # åˆå¹¶é€‰ä¸­çš„åˆ†æ®µ
            start1, end1 = new_segments[merge_idx]
            start2, end2 = new_segments[merge_idx + 1]
            new_segments[merge_idx] = (start1, end2)
            new_segments.pop(merge_idx + 1)

        return new_segments

    def create_database_from_segments(self, data_dir, segments, save_path):
        """æ ¹æ®åˆ†æ®µåˆ›å»ºä½ç½®æ•°æ®åº“"""
        print(f"ğŸ“ æ ¹æ® {len(segments)} ä¸ªåˆ†æ®µåˆ›å»ºä½ç½®æ•°æ®åº“...")

        ply_files = sorted(glob.glob(f"{data_dir}/*.ply"))
        if len(ply_files) == 0:
            print("âŒ æœªæ‰¾åˆ°plyæ–‡ä»¶")
            return False

        location_data = {}
        all_features = []
        all_labels = []

        for location_id, (start_idx, end_idx) in enumerate(segments):
            print(f"  å¤„ç†ä½ç½® {location_id+1}/{len(segments)} (å¸§ {start_idx}-{end_idx})")

            # ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
            start_idx = max(0, min(start_idx, len(ply_files) - 1))
            end_idx = max(start_idx + 1, min(end_idx, len(ply_files)))

            location_files = ply_files[start_idx:end_idx]
            location_features = []

            # å¤„ç†è¿™ä¸ªä½ç½®çš„æ‰€æœ‰æ–‡ä»¶
            for ply_file in location_files:
                try:
                    points = PLYReader.read_ply_file(ply_file)
                    if points is not None and len(points) > 100:
                        points = points[:, :3]
                        sc = self.sc_generator.generate_scan_context(points)

                        if sc is not None:
                            location_features.append(sc)
                            all_features.append(sc)
                            all_labels.append(location_id)

                except Exception as e:
                    print(f"    å¤„ç†å¤±è´¥ {ply_file}: {e}")
                    continue

            if len(location_features) > 0:
                # è®¡ç®—è¿™ä¸ªä½ç½®çš„ä»£è¡¨æ€§ç‰¹å¾
                representative_sc = np.mean(location_features, axis=0)
                location_data[location_id] = {
                    'representative_sc': representative_sc,
                    'sample_count': len(location_features),
                    'file_range': (start_idx, end_idx),
                    'all_features': location_features
                }
                print(f"    ä½ç½® {location_id}: {len(location_features)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
            else:
                print(f"    âš ï¸  ä½ç½® {location_id}: æ— æœ‰æ•ˆæ ·æœ¬")

        self.location_database = location_data
        self.location_features = np.array(all_features)
        self.location_labels = np.array(all_labels)

        # æ›´æ–°ä½ç½®æ•°é‡
        self.num_locations = len(segments)

        # ä¿å­˜ä½ç½®æ•°æ®åº“
        with open(save_path, 'wb') as f:
            pickle.dump({
                'location_database': self.location_database,
                'location_features': self.location_features,
                'location_labels': self.location_labels,
                'num_locations': self.num_locations,
                'adaptive_segments': True,
                'segments': segments
            }, f)

        print(f"âœ… è‡ªé€‚åº”ä½ç½®æ•°æ®åº“å·²ä¿å­˜åˆ°: {save_path}")
        print(f"æ€»æ ·æœ¬æ•°: {len(all_features)}")
        print(f"å®é™…ä½ç½®æ•°: {self.num_locations}")

        return True

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ğŸ¯ ä¼˜åŒ–çš„åŸºäºScanContext + CNNçš„è½¨è¿¹å®šä½ç³»ç»Ÿ")
    print("="*60)

    # æ•°æ®è·¯å¾„
    data_dir = "/mysda/w/w/RandLA-Net-pytorch/å›ç¯æ£€æµ‹/ply_files"

    # åˆ›å»ºä¼˜åŒ–çš„å®šä½ç³»ç»Ÿ
    localizer = TrajectoryLocalizationSystem(
        num_locations=8,
        model_type='resnet2d',
        adaptive_segments=False  # å¼ºåˆ¶å‡åŒ€åˆ†æ®µ
    )

    # 1. åˆ›å»ºè‡ªé€‚åº”ä½ç½®æ•°æ®åº“
    print("\næ­¥éª¤1: åˆ›å»ºè‡ªé€‚åº”ä½ç½®æ•°æ®åº“")
    success = localizer.create_location_database(data_dir)

    if not success:
        print("âŒ ä½ç½®æ•°æ®åº“åˆ›å»ºå¤±è´¥")
        return

    # 2. è®­ç»ƒå®šä½æ¨¡å‹
    print("\næ­¥éª¤2: è®­ç»ƒä¼˜åŒ–çš„å®šä½æ¨¡å‹")
    test_acc = localizer.train_localization_model(epochs=50)

    print(f"\nğŸ‰ ä¼˜åŒ–çš„è½¨è¿¹å®šä½ç³»ç»Ÿè®­ç»ƒå®Œæˆ!")
    print(f"å®šä½å‡†ç¡®ç‡: {test_acc:.1f}%")
    print(f"ç³»ç»Ÿå¯ä»¥è¯†åˆ«è½¨è¿¹ä¸­çš„ {localizer.num_locations} ä¸ªä¸åŒä½ç½®")
    print(f"âœ¨ ä¼˜åŒ–ç‰¹æ€§:")
    print(f"  - è‡ªé€‚åº”åœºæ™¯åˆ†æ®µ")
    print(f"  - æ—¶åºå¹³æ»‘å®šä½")
    print(f"  - ç½®ä¿¡åº¦è¿‡æ»¤")
    print(f"  - å†å²ä¿¡æ¯åˆ©ç”¨")
    print(f"ä¸‹æ¬¡æœºå™¨äººæ¥åˆ°ç›¸åŒåŒºåŸŸæ—¶ï¼Œå¯ä»¥æ›´å‡†ç¡®ã€æ›´ç¨³å®šåœ°å®šä½å…¶ä½ç½®ï¼")

if __name__ == '__main__':
    main()
